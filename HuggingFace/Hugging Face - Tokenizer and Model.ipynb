{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face - Tokenizer and Model\n",
    "\n",
    "For both _tokenizer_ and _model_, there are generic classes: `AutoTokenizer` and `AutoModelForSequenceClassification`. These can be substituted by specific classes, such as  `BertTokenizer` and `BertModel`.\n",
    "\n",
    "- See [AutoTokenizer official documentation](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "- See [AutoModelForSequenceClassification official documentation](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "res = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "\n",
    "print(res)\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "res = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What a tokenizer does?\n",
    "\n",
    "A tokenizer in natural language processing, such as the one used in the Hugging Face Transformers library, is responsible for converting text into a format that can be understood and processed by a model. Here's a detailed breakdown of what a tokenizer does:\n",
    "\n",
    "1. Text to Tokens: The tokenizer breaks down the input text into smaller units called tokens. These tokens can be words, subwords, or even characters, depending on the tokenizer's design. This process is important for handling complex languages where words can have multiple forms, or for managing large vocabularies efficiently.\n",
    "1. Tokenization Methods: Different models use different tokenization methods. For instance, BERT uses WordPiece tokenization, which splits words into subword units, allowing the model to handle unknown words more effectively.\n",
    "1. Generating Token IDs: Each token is mapped to a unique integer called a token ID. These IDs are used by the model to identify tokens.\n",
    "1. Attention Mask: Along with token IDs, the tokenizer generates attention masks. An attention mask is a sequence of 1s and 0s, where 1 indicates that a particular token should be paid attention to, and 0 means it should be ignored. This is useful for handling varying lengths of input sequences.\n",
    "1. Additional Tokens: The tokenizer may add special tokens that serve specific purposes. For example, '[CLS]' and '[SEP]' in BERT mark the beginning of a sequence and separation of segments, respectively. '[PAD]' tokens are used for padding shorter sequences to a uniform length.\n",
    "1. Pre-Trained Models: Tokenizers in Hugging Face are often linked to pre-trained models. When using a specific model, you typically use its associated tokenizer to ensure compatibility between the tokenization and the modelâ€™s training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print 1: {'input_ids': [101, 2478, 10938, 2121, 6125, 2003, 3722, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Print 2: ['using', 'transform', '##er', 'networks', 'is', 'simple']\n",
      "Print 3: [2478, 10938, 2121, 6125, 2003, 3722]\n",
      "Print 4: using transformer networks is simple\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Using Transformer networks is simple\"\n",
    "res = tokenizer(sequence)\n",
    "print(\"Print 1:\", res) #attention mask will be 1. If 0, the model will ignore that number\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(\"Print 2:\",tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Print 3:\",ids)\n",
    "\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(\"Print 4:\",decoded_string)\n",
    "#101 means \"begin of sentence\"\n",
    "#102 means \"end of sentece\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
