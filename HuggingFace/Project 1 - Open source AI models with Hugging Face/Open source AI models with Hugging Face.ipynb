{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Open Source AI Models with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face has a large ecosystem that consists of a Git platform and multiple open source libraries such as [transformers](https://github.com/huggingface/transformers), [diffusers](https://github.com/huggingface/diffusers), [datasets](https://github.com/huggingface/datasets), [accelerate](https://github.com/huggingface/accelerate), and much [more](https://github.com/huggingface). This project is intended to help you get a broad understanding of Hugging Face ecosystem and its components with a focus on `transformers`. You will learn about\n",
    "\n",
    "- How to navigate the Hugging Face Hub\n",
    "- Design principles of the transformers library\n",
    "- How to use the datasets library to load and create datasets on the Hub\n",
    "- How to load and use pre-trained open source models to create custom NLP and Computer Vision pipelines\n",
    "\n",
    "While this project is meant as an introduction, you can always refer to the Hugging Face [documentation pages](https://huggingface.co/docs) and the discussion [forum](https://discuss.huggingface.co/) to learn more about each library and integrated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN obtained successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if HF_TOKEN != None:\n",
    "    print(\"HF_TOKEN obtained successfully!\")\n",
    "else:\n",
    "    print(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we need the `torch`, `transformers` and `sentencepiece` Python packages in order to load and use pre-trained models. We will also need the `huggingface_hub` package to programmatically login to the Hugging Face Hub and manage repositories, and the `datasets` package to download datasets from the Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sentencepiece` package is required by `transformers` to perform inference with some of the pre-trained open source models on Hugging Face Hub and does not need to be explicitly imported. Import the remaining packages as follows.\n",
    "\n",
    "- Run the provided `!pip` code to install necessary packages and restart your kernel.\n",
    "- Import `torch` \n",
    "- Import `huggingface_hub` using the alias `hf_hub`.\n",
    "- Import `datasets` \n",
    "- Import `transformers` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16782,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1695629850604,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install datasets==2.13\n!pip install huggingface_hub==0.16.4\n!pip install pyarrow>=8.0.0",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub==0.16.4 in c:\\users\\josem\\anaconda3\\lib\\site-packages (0.16.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface_hub==0.16.4) (3.0.12)\n",
      "Requirement already satisfied: fsspec in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface_hub==0.16.4) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface_hub==0.16.4) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface_hub==0.16.4) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface_hub==0.16.4) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface_hub==0.16.4) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface_hub==0.16.4) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\josem\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub==0.16.4) (0.4.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from requests->huggingface_hub==0.16.4) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from requests->huggingface_hub==0.16.4) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from requests->huggingface_hub==0.16.4) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from requests->huggingface_hub==0.16.4) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\josem\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\josem\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets==2.13\n",
    "!pip install huggingface_hub==0.16.4\n",
    "# !pip install pyarrow>=8.0.0\n",
    "# !pip install --upgrade pip --user\n",
    "# !pip install --upgrade transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1185,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695629858058,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import os\nimport os\n\n# Import torch\nimport torch\n\n# Import huggingface_hub using the alias hf_hub\nimport huggingface_hub as hf_hub\n\n# Import datasets \nimport datasets\n\n# Import transformers\nimport transformers"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'huggingface_hub.constants' has no attribute 'HF_HUB_CACHE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Import transformers\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     logging,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     50\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\__init__.py:61\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     25\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     ContextManagers,\n\u001b[0;32m     33\u001b[0m     ExplicitEnum,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     60\u001b[0m )\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     63\u001b[0m     DISABLE_TELEMETRY,\n\u001b[0;32m     64\u001b[0m     HF_MODULES_CACHE,\n\u001b[0;32m     65\u001b[0m     HUGGINGFACE_CO_PREFIX,\n\u001b[0;32m     66\u001b[0m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001b[0;32m     67\u001b[0m     PYTORCH_PRETRAINED_BERT_CACHE,\n\u001b[0;32m     68\u001b[0m     PYTORCH_TRANSFORMERS_CACHE,\n\u001b[0;32m     69\u001b[0m     S3_BUCKET_PREFIX,\n\u001b[0;32m     70\u001b[0m     TRANSFORMERS_CACHE,\n\u001b[0;32m     71\u001b[0m     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n\u001b[0;32m     72\u001b[0m     EntryNotFoundError,\n\u001b[0;32m     73\u001b[0m     PushInProgress,\n\u001b[0;32m     74\u001b[0m     PushToHubMixin,\n\u001b[0;32m     75\u001b[0m     RepositoryNotFoundError,\n\u001b[0;32m     76\u001b[0m     RevisionNotFoundError,\n\u001b[0;32m     77\u001b[0m     cached_file,\n\u001b[0;32m     78\u001b[0m     default_cache_path,\n\u001b[0;32m     79\u001b[0m     define_sagemaker_information,\n\u001b[0;32m     80\u001b[0m     download_url,\n\u001b[0;32m     81\u001b[0m     extract_commit_hash,\n\u001b[0;32m     82\u001b[0m     get_cached_models,\n\u001b[0;32m     83\u001b[0m     get_file_from_repo,\n\u001b[0;32m     84\u001b[0m     has_file,\n\u001b[0;32m     85\u001b[0m     http_user_agent,\n\u001b[0;32m     86\u001b[0m     is_offline_mode,\n\u001b[0;32m     87\u001b[0m     is_remote_url,\n\u001b[0;32m     88\u001b[0m     move_cache,\n\u001b[0;32m     89\u001b[0m     send_example_telemetry,\n\u001b[0;32m     90\u001b[0m     try_to_load_from_cache,\n\u001b[0;32m     91\u001b[0m )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     93\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m     94\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     torch_required,\n\u001b[0;32m    198\u001b[0m )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    200\u001b[0m     ADAPTER_CONFIG_NAME,\n\u001b[0;32m    201\u001b[0m     ADAPTER_SAFE_WEIGHTS_NAME,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    204\u001b[0m     find_adapter_config_file,\n\u001b[0;32m    205\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py:94\u001b[0m\n\u001b[0;32m     84\u001b[0m old_default_cache_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(torch_cache_home, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Determine default cache directory. Lots of legacy environment variables to ensure backward compatibility.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# The best way to set the cache path is with the environment variable HF_HOME. For more details, checkout this\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# documentation page: https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# TODO: clean this for v5?\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m PYTORCH_PRETRAINED_BERT_CACHE \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_PRETRAINED_BERT_CACHE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_HUB_CACHE\u001b[49m)\n\u001b[0;32m     95\u001b[0m PYTORCH_TRANSFORMERS_CACHE \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_TRANSFORMERS_CACHE\u001b[39m\u001b[38;5;124m\"\u001b[39m, PYTORCH_PRETRAINED_BERT_CACHE)\n\u001b[0;32m     96\u001b[0m TRANSFORMERS_CACHE \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRANSFORMERS_CACHE\u001b[39m\u001b[38;5;124m\"\u001b[39m, PYTORCH_TRANSFORMERS_CACHE)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'huggingface_hub.constants' has no attribute 'HF_HUB_CACHE'"
     ]
    }
   ],
   "source": [
    "# Import os\n",
    "import os\n",
    "\n",
    "# Import torch\n",
    "import torch\n",
    "\n",
    "# Import huggingface_hub using the alias hf_hub\n",
    "import huggingface_hub as hf_hub\n",
    "\n",
    "# Import datasets \n",
    "import datasets\n",
    "\n",
    "# Import transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Download Pre-trained Models from the HF Hub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hugging Face Hub as a Git Platform**\n",
    "\n",
    "The Hugging Face website (also known as the Hub) is essentially a Git platform designed to store pretrained models and datasets as Git repositories. Similar to GitHub, it allows users to explore, create, clone, push repositories and so much more. Each pretrained checkpoint has its own repository and in most cases a descriptive README with code snippets to load and run the model. See the [bert-base-cased](https://huggingface.co/bert-base-cased) model repository as an example.\n",
    "\n",
    "**How to Use Pretrained Models**\n",
    "\n",
    "While the Hub is a great place to explore different tasks and pretrained models, we need the `transformers` or `diffusers` libraries in order to load and make predictions with pre-trained models. These two libraries reimplement the code of the state-of-the-art ML research such that vastly different models can be downloaded, loaded into memory and used in a unified way with a few lines of code.\n",
    "\n",
    "In this task, you will learn how to use the `Auto` classes of `transformers` and the `from_pretrained` method to download and load any model on the Hugging Face Hub. For a full list of supported models, refer to the GitHub [README](https://github.com/huggingface/transformers/tree/main#model-architectures).\n",
    "\n",
    "**What is the Auto Class?**\n",
    "\n",
    "`Auto` classes of the `transformers` are simply tools to load models and their data preprocessors in a unified way. Remember, the library reimplements each model such that they each have their own class (`BertModel`, `RobertaModel`, `T5Model`, etc.) with mostly uniform input and output data format across all models. transformers have the following `Auto` class types to load models and their data preprocessors:\n",
    "\n",
    "- `AutoModel` \n",
    "- `AutoModelForTASK>` (more on this below)\n",
    "- `AutoTokenizer` \n",
    "- `AutoFeatureExtractor`\n",
    "- `AutoImageProcessor`\n",
    "- `AutoProcessor`\n",
    "\n",
    "**Loading Models into Memory with from_pretrained**\n",
    "\n",
    "For the first task, you will download the pretrained \"cardiffnlp/twitter-roberta-base-emoji\" model and load the model and its data preprocessor into memory with the `from_pretrained(<REPO_NAME_OR_PATH>)` method. The \"cardiffnlp/twitter-roberta-base-emoji\" is a text classification model that is trained to predict the emoji class ID of a given tweet.\n",
    "\n",
    "[cardiffnlp/twitter-roberta-base-emoji](https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji) is a valid Git repository on the Hub and the `from_pretrained()` method downloads and uses the tokenizer specific [files](https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/tree/main) from the model repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `AutoTokenizer` and `AutoModel` classes to download a pre-trained language model from the Hub and load its data preprocessor.\n",
    "\n",
    "- Import `AutoTokenizer` and `AutoModel` classes\n",
    "- Call the `from_pretrained()` method for both classes using the target repository name as input \n",
    "- Identify the explicit class name of the pretrained model using the model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 635,
    "id": "bA5ajAmk7XH6",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1695629872158,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import the AutoTokenizer and AutoModel classes from transformers\nfrom transformers import AutoTokenizer, AutoModel\n\n# Load the pre-trained tokenizer of the \"cardiffnlp/twitter-roberta-base-emoji\" model\ntokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")",
    "outputsMetadata": {
     "0": {
      "height": 97,
      "type": "stream"
     },
     "1": {
      "height": 537,
      "type": "stream"
     },
     "10": {
      "height": 237,
      "type": "stream"
     },
     "3": {
      "height": 257,
      "type": "stream"
     },
     "4": {
      "height": 557,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import the AutoTokenizer and AutoModel classes from transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the pre-trained tokenizer of the \"cardiffnlp/twitter-roberta-base-emoji\" model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What from_pretrained() does**\n",
    "\n",
    "The `from_pretrained()` method (see [docs](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained)) first searches for a model repository with the same name on the Hugging Face Hub but it also accepts a local path or a URL with the expected folder structure. You can simply git clone the repository and load it from your local path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695572345202,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Print the tokenizer to see the data preprocessing configuration for this model \nprint(tokenizer)",
    "outputsMetadata": {
     "0": {
      "height": 117,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Print the tokenizer to see the data preprocessing configuration for this model \n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizers**\n",
    "\n",
    "NLP models can't process text inputs as it is and need to be converted to a fixed length mathematical format. The `Tokenizer` classes preprocess the text such that each word and punctuation is given a unique ID or a **token**, short sentences are padded and long sentences are truncated to create fixed-size input vectors. They also allow using additional tokens such as _bos, eos, unk, sep_, and more to specify start and end of sentences, and to assign token IDS to unknown words that are not in the tokenizer vocabulary.\n",
    "\n",
    "The output of the tokenizer tells us this pretrained model uses a tokenizer with a 50265 unique token IDs that applies padding or truncation to the end of each input text, and removes leading (leftside) extra whitespaces. You can always refer to the corresponding Tokenizer [docs](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaTokenizerFast) to learn more about each preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1000,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1695639166592,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load the pre-trained \"cardiffnlp/twitter-roberta-base-emoji\" model\nmodel = AutoModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")",
    "outputsMetadata": {
     "0": {
      "height": 297,
      "type": "stream"
     },
     "1": {
      "height": 297,
      "type": "stream"
     },
     "2": {
      "height": 297,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained \"cardiffnlp/twitter-roberta-base-emoji\" model\n",
    "model = AutoModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on Model Loading**\n",
    "\n",
    "Take a look at the warning above. We were only able to load a chunk of the model parameters included in the checkpoint with the `AutoModel` class. \n",
    "\n",
    "This is because all `transformers` models are designed to have a single base class and multiple task-specific prediction classes built on top of it. The `AutoModel` class is designed to only load the base model parameters such as `RobertaModel` but not task-specific models such as `RobertaForSequenceClassification`.\n",
    "\n",
    "In order to identify the exact class name and task of your target checkpoint, you can simply refer to the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 100,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695629885605,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Get the model configuration \nconfig = model.config\n\n# Print model config's architectures attribute to see the model class name\nprint(model.config.architectures)",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get the model configuration \n",
    "config = model.config\n",
    "\n",
    "# Print model config's architectures attribute to see the model class name\n",
    "print(model.config.architectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out that the pretrained model we are trying to load is an instance of the `RobertaForSequenceClassification` class. \n",
    "\n",
    "**Important Tip**\n",
    "\n",
    "You don't have to download and print out the configuration to identify the model class name. You can simply navigate to the [config.json](https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/blob/main/config.json) file within the model repository to find it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Downstream Tasks and Task-Specific `AutoModel` Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations of AutoModel**\n",
    "\n",
    "In the previous task, we learned all about `Auto` classes and the limitations of the `AutoModel` class. We also learned that each `transformers` model consists of a base class and one or more task-specific classes. For example, RoBERTa has the base `RobertaModel` class and multiple prediction classes such as `RobertaForSequenceClassification`, `RobertaForQuestionAnswering`, etc. that leverage the base class. You can always refer to the corresponding model [documentation page](https://huggingface.co/docs/transformers/model_doc/roberta) to see its full list of tasks and classes. \n",
    "\n",
    "**Task-Specific AutoModel Classes**\n",
    "\n",
    "Luckily for us, `transformers` has multiple task-specific `AutoModelForTASK` classes to load pretrained models. For a full list of `Auto` classes, take a look at the [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto#auto-classes) documentation page.\n",
    "\n",
    "In this task, you will learn how to load a pretrained model with a task-specific `Auto` model class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [cardiffnlp/twitter-roberta-base-emoji](https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji) model we used in the previous task with the correct `AutoModelForTASK` class.\n",
    "\n",
    "- _cardiffnlp/twitter-roberta-base-emoji_ is an instance of the `RobertaForSequenceClassification` class, take a look at the list of [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto#auto-classes)\n",
    "- Identify and import the correct Auto class\n",
    "- Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1159,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695629894717,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load the correct AutoModel class for RobertaForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification\n\n# Load the pretrained \"cardiffnlp/twitter-roberta-base-emoji\" model using the correct auto model class\nmodel = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")",
    "outputsMetadata": {
     "0": {
      "height": 177,
      "type": "stream"
     },
     "1": {
      "height": 297,
      "type": "stream"
     },
     "2": {
      "height": 587,
      "type": "stream"
     },
     "3": {
      "height": 582,
      "type": "stream"
     },
     "5": {
      "height": 197,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load the correct AutoModel class for RobertaForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pretrained \"cardiffnlp/twitter-roberta-base-emoji\" model using the correct auto model class\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Explicit Model and Preprocesser Class Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned how to identify the data preprocessor and model class of a model repository on the Hub in the previous task. Note that you can also do this by navigating to the **Files and versions** tab on the repository [page](https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji) and taking a look at the [config.json](https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji/blob/main/config.json) file.\n",
    "\n",
    "Similar to models with multiple task-specific model classes, data preprocessor classes are named based on the model name and the modality. \n",
    "\n",
    "- `Tokenizer` classes for language models\n",
    "- `FeatureExtractor` classes for audio models\n",
    "- `ImageProcessor` classes for computer vision models\n",
    "- `Processor` classes for multi-modal models\n",
    "\n",
    "These data preprocessor classes transform raw input data such as text, image, etc. to the format expected by your target pre-trained model. For an vision model, the preprocessor performs steps such as resizing to a fixed size, normalization, cropping, etc. In this task, you will\n",
    "\n",
    "- Use the \"cardiffnlp/twitter-roberta-base-emoji\" model we loaded with `Auto` classes before\n",
    "- Explicitly import a model and its preprocessor class\n",
    "- Use explicit class names to load a pretrained model and its preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by explicitly importing the tokenizer and model class of \"cardiffnlp/twitter-roberta-base-emoji\", which we identified in our previous task.\n",
    "\n",
    "- Use the from_pretrained method to download and load the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1695629898609,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import the explicit tokenizer class of \"cardiffnlp/twitter-roberta-base-emoji\"\nfrom transformers import RobertaTokenizerFast\n\n# Import the explicit model class of \"cardiffnlp/twitter-roberta-base-emoji\"\nfrom transformers import RobertaForSequenceClassification",
    "outputsMetadata": {
     "0": {
      "height": 117,
      "type": "stream"
     },
     "6": {
      "height": 197,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import the explicit tokenizer class of \"cardiffnlp/twitter-roberta-base-emoji\"\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# Import the explicit model class of \"cardiffnlp/twitter-roberta-base-emoji\"\n",
    "from transformers import RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the explicit tokenizer and model classes the same way we use the Auto classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1203,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1695629901909,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load the tokenizer and model of \"cardiffnlp/twitter-roberta-base-emoji\"\ntokenizer = RobertaTokenizerFast.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer and model of \"cardiffnlp/twitter-roberta-base-emoji\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Creating an NLP Pipeline - Part 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You learned how to navigate the Hugging Face Hub repositories, identify the model and data preprocessor classes of your target repository and how to import and load a pre-trained model and its corresponding preprocessor.\n",
    "\n",
    "You will now put these skills to use and create powerful language translation pipeline with a few lines of code using the [_google/flan-t5-base_](https://huggingface.co/google/flan-t5-base) model. This multi-lingual translation model allows you to translate between multiple languages. For a full list of languages this model supports, simply refer to its model repository. \n",
    "\n",
    "**Text-to-Text Generation**\n",
    "\n",
    "In transformers, NLP models with text input and output fall under the `AutoModelForSeq2SeqLM` class. This class of pretrained models perform tasks such as translation, question answering and text completion. \n",
    "\n",
    "**Inputs for T5**\n",
    "\n",
    "The tokenizer of T5 preprocesses the raw text input to return its token ids and attention mask. Each token id represents a unique word, tag or punctuation in the model's vocabulary. The attention mask consists of 1s and 0s for each token to decide whether that token should be attended to or ignored by the transformer model.\n",
    "\n",
    "In this task, you will learn\n",
    "\n",
    "- What the tokenizer classes do under the hood\n",
    "- How to preprocess your raw input text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the `AutoTokenizer` and `AutoModelForSeq2SeqLM` classes to load the pre-trained ``\"google/flan-t5-base\"`` model and its tokenizer\n",
    "- Create a text input that includes\n",
    "    - A prompt to specify source and target languages for translation\n",
    "    - Text to be translated\n",
    "- Preprocess the text input to retrieve token ids and the attention mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2381,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1695639397729,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import AutoModelForSeq2SeqLM from transformers\nfrom transformers import AutoModelForSeq2SeqLM\n\n# Load the pre-trained google/flan-t5-base model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")",
    "outputsMetadata": {
     "0": {
      "height": 177,
      "type": "stream"
     },
     "1": {
      "height": 77,
      "type": "stream"
     },
     "6": {
      "height": 177,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import AutoModelForSeq2SeqLM from transformers\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pre-trained google/flan-t5-base model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to perform inference with the pretrained model. We will start by defining an input sentence where we start by which language we would like to translate from and to, and add the sentence to be translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_adjectives = ['Adventurous', 'Brave', 'Charming', 'Delightful', 'Energetic', 'Fearless', \n",
    " 'Graceful', 'Humorous', 'Imaginative', 'Joyful', 'Kind', 'Lively', 'Magical', \n",
    " 'Nostalgic', 'Optimistic', 'Peaceful', 'Quirky', 'Resourceful', 'Sincere', \n",
    " 'Thoughtful', 'Unique', 'Vibrant', 'Witty', 'Youthful', 'Zealous', 'Amazing', \n",
    " 'Beautiful', 'Creative', 'Dynamic', 'Elegant', 'Friendly', 'Generous', \n",
    " 'Helpful', 'Innovative', 'Jovial', 'Knowledgeable', 'Loyal', 'Merry', \n",
    " 'Natural', 'Observant', 'Playful', 'Quiet', 'Reliable', 'Strong', 'Talented', \n",
    " 'Understanding', 'Valiant', 'Warm', 'Xenodochial', 'Youthful', 'Zestful', \n",
    " 'Affable', 'Benevolent', 'Courteous', 'Determined', 'Empathetic', 'Fortunate', \n",
    " 'Genuine', 'Honorable', 'Insightful', 'Jubilant', 'Keen', 'Learned', 'Mindful', \n",
    " 'Nimble', 'Outgoing', 'Pleasant', 'Quick-witted', 'Respectful', 'Skillful', \n",
    " 'Trustworthy', 'Upbeat', 'Versatile', 'Wise', 'Xenial', 'Yearning', 'Zealous', \n",
    " 'Assertive', 'Blissful', 'Compassionate', 'Diligent', 'Exuberant', 'Faithful', \n",
    " 'Gallant', 'Hardworking', 'Intuitive', 'Jocular', 'Knack', 'Lucid', 'Motivated', \n",
    " 'Notable', 'Open-minded', 'Practical', 'Quaint', 'Resilient', 'Sophisticated', \n",
    " 'Thrifty', 'Unbiased', 'Vivacious', 'Whimsical', 'Xenophilic', 'Yielding', 'Zany']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 14,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1695639398308,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Specify source and target languages for translation\nsource_lang = \"English\"\ntarget_lang = \"German\"\ninput_text = f\"translate {source_lang} to {target_lang}: How old are you?\"\n\n# Use the tokenizer to preprocess input text, with return_tensors=\"pt\" to return PyTorch tensors \ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Print the key and values of the preprocessed inputs dictionary\nfor key, value in inputs.items():\n    print(key, value)",
    "outputsMetadata": {
     "0": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Specify source and target languages for translation\n",
    "source_lang = \"English\"\n",
    "target_lang = \"Spanish\"\n",
    "input_text = f\"translate {source_lang} to {target_lang}\"\n",
    "\n",
    "# Use the tokenizer to preprocess input text, with return_tensors=\"pt\" to return PyTorch tensors \n",
    "inputs = tokenizer(text=en_adjectives[:10],\n",
    "                   return_tensors=\"pt\",\n",
    "                   padding=True) #add padding if text is a list\n",
    "\n",
    "# Print the key and values of the preprocessed inputs dictionary\n",
    "for key, value in inputs.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Creating an NLP Pipeline - Part 2/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You preprocessed the input text and are ready learn how to perform inference. In this task, you will learn \n",
    "\n",
    "- How to use `torch.no_grad()` to perform inference fast and efficiently, and reduce memory usage\n",
    "- How to feed preprocessed inputs to the model\n",
    "- How to decode model output to make it human readable\n",
    "\n",
    "Note that `transformers` and `diffusers` libraries support batched inputs, meaning you can inputs multiple text inputs or images, etc. as a list. Regardless of the number of inputs, the preprocess and model classes output batched results where each entry corresponds to an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torch.no_grad()`\n",
    "\n",
    "- Pass preprocessed inputs to the loaded model to perform inference\n",
    "- Save the model output to a variable named outputs\n",
    "- Decode the model output tokens to human readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 329,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695640422414,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Use torch.no_grad() to prevent gradient accumulation - speeds up inference, reduces memory\nwith torch.no_grad():\n    # Use model.generate method to perform inference\n    # Use **inputs to pass the unpacked dictionary as multiple arguments to the generate method \n    outputs = model.generate(**inputs)\n\n# The output is a list of length batch_size - number of input texts - which is 1 in this case\n# Print the length of outputs list\nprint(len(outputs))",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Use torch.no_grad() to prevent gradient accumulation - speeds up inference, reduces memory\n",
    "with torch.no_grad():\n",
    "    # Use model.generate method to perform inference\n",
    "    # Use **inputs to pass the unpacked dictionary as multiple arguments to the generate method \n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# The output is a list of length batch_size - number of input texts - which is 1 in this case\n",
    "# Print the length of outputs list\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model output is a sequence of token IDs, where each token represents a unique word or punctuation in the vocabular. To decode it to a human readable format, we just need to map each token ID to its corresponding word. All `Tokenizer` classes of `transformers` provides a convenient `decode` method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695640486309,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Use the decode method of the tokenizer to convert the output to a human readable format \nprint(tokenizer.decode(outputs[0]))",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Use the decode method of the tokenizer to convert the output to a human readable format \n",
    "print(tokenizer.decode(outputs[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Datasets Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transformers` library is designed to make it easy to load and use pre-trained models, and to preprocess raw input data. However, it is up to you to find or create interesting datasets.\n",
    "\n",
    "We will now take a look at the `datasets` library and learn how to find, download and create datasets with a few lines of code. This library is part of the Hugging Face ecosystem, and similar to `transformers` and `diffusers`, is tightly integrated with the Hugging Face Hub. Each library is stored as a Git repository on the Hub and can be downloaded with a single line of code.\n",
    "\n",
    "In this task, you will learn\n",
    "\n",
    "- How to use the `load_dataset()` function to download and load a dataset from the Hub\n",
    "- How to only download a specific data split (training, validation, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the `load_dataset()` function from datasets\n",
    "- Load the `\"adirik/fashion_image_caption-100\"` dataset\n",
    "- Print the dataset to see what features it contains\n",
    "- Print a sample of the dataset\n",
    "- Download only the training split of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 207,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695643063160,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import load_dataset from datasets package\nfrom datasets import load_dataset\n\n# Use the load_dataset function to load the \"adirik/fashion_image_caption-100\" \ndataset = load_dataset(\"adirik/fashion_image_caption-100\")\n\n# Print the dataset object\nprint(dataset)",
    "outputsMetadata": {
     "1": {
      "height": 137,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import load_dataset from datasets package\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Use the load_dataset function to load the \"adirik/fashion_image_caption-100\" \n",
    "dataset = load_dataset(\"adirik/fashion_image_caption-100\")\n",
    "\n",
    "# Print the dataset object\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is a dictionary with train, and optionally val and test splits. Each split contains an iterable subset  of the data with one or more keys that represent the features of the dataset defined by the dataset owner. For example, the \"adirik/fashion_image_caption-100\" dataset contains image and text keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695643103516,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Print the fifth data sample in the dataset - an image and a text caption pair\nprint(dataset[\"train\"][5])",
    "outputsMetadata": {
     "0": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Print the fifth data sample in the dataset - an image and a text caption pair\n",
    "print(dataset[\"train\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 66,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695643130454,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Visualize the image of the first data sample\ndataset[\"train\"][5][\"image\"]"
   },
   "outputs": [],
   "source": [
    "# Visualize the image of the first data sample\n",
    "dataset[\"train\"][5][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also opt to download only a specific split of the dataset or pass in the `streaming=True` argument in order to download smaller subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 166,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695643796797,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Download only the training split of the dataset\ndataset = load_dataset(\"adirik/fashion_image_caption-100\", split=\"train\")\nprint(dataset)",
    "outputsMetadata": {
     "0": {
      "height": 97,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Download only the training split of the dataset\n",
    "dataset = load_dataset(\"adirik/fashion_image_caption-100\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: CV / Multi-Modal Pipeline - Part 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned to how download datasets easily with the datasets library and tricks such as streaming to download data samples one by one if we don't have sufficient disk space for the whole dataset.\n",
    "\n",
    "As demonstrated in the previous task, the image captions of the `\"adirik/fashion_image_caption-100\"` dataset are not very high quality. Could we create a better version of this dataset? Let's first setup an image captioning pipeline to see if we can generate better captions.\n",
    "\n",
    "In this task, you will learn\n",
    "\n",
    "- How to create an multi-modal / image captioning pipeline with [BLIP](https://huggingface.co/docs/transformers/main/en/model_doc/blip)\n",
    "- How to use `datasets` library datasets as input to `transformers` models\n",
    "- How to postprocess / decode BLIP output to retrieve human-readable captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the `Salesforce/blip-image-captioning-base` repository on the Hugging Face Hub and determine the model and preprocessor classes.\n",
    "\n",
    "- Import the correct preprocessor and model classes from transformers\n",
    "- Using the `from_pretrained()` method\n",
    "    - Create a preprocessor \n",
    "    - Download and load the pre-trained model\n",
    "- Perform inference with a single image sample\n",
    "- Decode model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2301,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1695646185295,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import BlipProcessor and BlipForConditionalGeneration from transformers\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\n# Load preprocessor of \"Salesforce/blip-image-captioning-base\"\npreprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n# Load pre-trained \"Salesforce/blip-image-captioning-base\" model\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   },
   "outputs": [],
   "source": [
    "# Import BlipProcessor and BlipForConditionalGeneration from transformers\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Load preprocessor of \"Salesforce/blip-image-captioning-base\"\n",
    "preprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load pre-trained \"Salesforce/blip-image-captioning-base\" model\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessor classes of Python are designed for ease of use. You can simply pass in a single or a list of Pillow image/s, url/s or local path/s to your image as the main argument to `ImageProcessor` class instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 19,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695646218525,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Preprocess the first image of the dataset, return result as pytorch tensors\ninputs = preprocessor(dataset[0][\"image\"], return_tensors=\"pt\")\n\nfor key, value in inputs.items():\n    print(key, value.shape)",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the first image of the dataset, return result as pytorch tensors\n",
    "inputs = preprocessor(dataset[0][\"image\"], return_tensors=\"pt\")\n",
    "\n",
    "for key, value in inputs.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference is almost the same as the NLP task as BLIP is a conditional text generation model. We will need to call `model.generate()` to run the model and use the `decode` method of the preprocessor to convert the output caption to a human readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1761,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695646707940,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# With torch.no_grad()\nwith torch.no_grad():\n    # Predict caption using the generate method of the model\n    outputs = model.generate(**inputs)\n\n# Decode model output to text\ncaption = preprocessor.decode(outputs[0], skip_special_tokens=True)\n\n# Print decoded caption\nprint(caption)",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# With torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    # Predict caption using the generate method of the model\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode model output to text\n",
    "caption = preprocessor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print decoded caption\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: CV / Multi-Modal Pipeline - Part [2/2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we were able to come up with a more intuitive yet less detailed image caption, but that is good enough for us. For this final task, you will learn \n",
    "\n",
    "- How to create a mapping function to preprocess and generate new captions for all samples in the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create a `replace_caption()` function that takes a dictionary `{\"image\": ..., \"text\": ...}` as input.\n",
    "\n",
    "- Create `replace_caption()` function with a single argument named data\n",
    "    - Place the preprocessing and inference steps of the previous task within this function and use the \"image\" value as input\n",
    "    - Set the decoded output as the new value of the \"text\" key\n",
    "    - Return the updated data dictionary\n",
    "- Use [Dataset.map()](https://huggingface.co/docs/datasets/process#map) method to apply the function to whole dataset\n",
    "- Use [Dataset.push_to_hub()](https://huggingface.co/docs/datasets/upload_dataset) method to push the updated dataset to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 216690,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1695647651405,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Cerate a replace_caption function that takes a data dictionary as input\ndef replace_caption(data):\n    # Preprocess the image value of the data dictionary\n    inputs = preprocessor(data[\"image\"], return_tensors=\"pt\")\n\n    # Predict the caption with torch.no_grad and the generate method\n    with torch.no_grad():\n        output = model.generate(**inputs)\n\n    # Decode model output to text\n    caption = preprocessor.decode(output[0], skip_special_tokens=True)\n    \n    # Set caption as the new text value of the data dictionary\n    data[\"text\"] = caption\n    return data\n\n# Use the map function to replace the captions of whole dataset\nnew_dataset = dataset.map(replace_caption)"
   },
   "outputs": [],
   "source": [
    "# Cerate a replace_caption function that takes a data dictionary as input\n",
    "def replace_caption(data):\n",
    "    # Preprocess the image value of the data dictionary\n",
    "    inputs = preprocessor(data[\"image\"], return_tensors=\"pt\")\n",
    "\n",
    "    # Predict the caption with torch.no_grad and the generate method\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs)\n",
    "\n",
    "    # Decode model output to text\n",
    "    caption = preprocessor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Set caption as the new text value of the data dictionary\n",
    "    data[\"text\"] = caption\n",
    "    return data\n",
    "\n",
    "# Use the map function to replace the captions of whole dataset\n",
    "new_dataset = dataset.map(replace_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Pushing Datasets (and Models) to the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Hugging Face Hub](https://huggingface.co) is a Git platform specialized for machine learning. You can search for pre-trained model and dataset repositories on the website, clone or create and update repositories public or private repositories.\n",
    "\n",
    "Alternatively, you can use the `huggingface_hub` Python package to programmatically:\n",
    "\n",
    "- Login to the Hub.\n",
    "- Search for open-source models on the Hub based on task type, model name, description, etc.\n",
    "- Download specific or all files from a repository.\n",
    "- Manage your repositories.\n",
    "\n",
    "When in doubt, refer to the corresponding Hugging Face [documentation](https://huggingface.co/docs/huggingface_hub/) to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `huggingface_hub` library to programatically interact with the Hugging Face Hub.\n",
    "\n",
    "- Login to your Hugging Face account using your HUGGINGFACE_TOKEN.\n",
    "- Push the updated dataset to the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub==0.16.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695652638465,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Assign your HUGGING_FACE_TOKEN to a variable named hf_token\nhf_token = os.environ[\"HUGGING_FACE_TOKEN\"]\n\n# Login to the HF Hub using your hf_token\nhf_hub.login(hf_token)",
    "outputsMetadata": {
     "0": {
      "height": 97,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Login to the HF Hub using your hf_token\n",
    "hf_hub.login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Log in to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 392,
    "lastExecutedAt": 1695652688648,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Push the new / improved dataset to the hub with the push_to_hub method\nyour_username = \"adirik\"\nnew_dataset.push_to_hub(f\"{your_username}/fashion_image_caption-100-v2\")"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Push the dataset to the hub\n",
    "# Replace 'your_dataset' with your actual dataset variable\n",
    "# Replace 'your_username' with your actual Hugging Face username\n",
    "your_username = \"josemh301\"\n",
    "new_dataset.push_to_hub(f\"{your_username}/fashion_image_caption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
