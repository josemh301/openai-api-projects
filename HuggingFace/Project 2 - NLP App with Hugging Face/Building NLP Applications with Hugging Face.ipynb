{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building NLP Applications with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! In this project, you will be learning how to perform common Natural Language Processing (NLP) tasks using Hugging Face. Some of these tasks include:\n",
    "- **sentiment analysis** (i.e. categorizing text as negative or positive);\n",
    "- **text embedding** (i.e. transforming a piece of text into a numerical, n-dimensional vector, representation);\n",
    "- **semantic search** (i.e. matching a query with the most appropriate result based on embeddings);\n",
    "- and more!\n",
    "\n",
    "The dataset comes from \"Rent the Runway\" [link](https://cseweb.ucsd.edu//~jmcauley/datasets.html#clothing_fit) and is comprised of user reviews on clothing items, their ratings on fit, and other metadata about the user (i.e. gender, height, size, age, reason for renting) and the item (i.e. category). It is a nice mixture of data types, but most importantly, lots of text! \n",
    "\n",
    "In order to be successful, you should have:\n",
    "\n",
    "**Intermediate knowledge of Python**\n",
    "- list comprehension\n",
    "- for loops and while loops\n",
    "- installing packages\n",
    "- creating and using functions\n",
    "- using NumPy and Pandas\n",
    "\n",
    "**Basic understanding of NLP**\n",
    "- What it is\n",
    "- Data preparation steps and why they're important\n",
    "- Familiarity, though not necessarily expert proficiency, in some NLP tasks\n",
    "\n",
    "**Brief usage of Hugging Face**\n",
    "\n",
    "\n",
    "Most of all, you should have a curiosity about NLP workflows, specifically those in Hugging Face using transformers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will need several Python packages:\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `datetime`\n",
    "- `re`\n",
    "- `string`\n",
    "- `matplotlib.pyplot`\n",
    "- `seaborn`\n",
    "- `transformers`\n",
    "- `sentence_transformers`\n",
    "\n",
    "These packages will help us with the data preprocessing steps, visualization, and, of course, NLP tasks using Hugging Face (i.e. `transformers` and `sentence_transformers`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following packages.\n",
    "\n",
    "- Import `re`, `datetime`, and `string`.\n",
    "- Import `pandas` using the alias `pd`.\n",
    "- Import `numpy` using the alias `np`.\n",
    "- Import `matplotlib.pyplot` using the alias `plt`.\n",
    "- From the `transformers` package, import `pipeline`.\n",
    "- From the `sentence_transformers` package, import `SentenceTransformer`.\n",
    "- From the `sentence_transformers.util` package, import `semantic_search`. \n",
    "- From the `IPython.display` package, import `display` and `Markdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 86.0/86.0 kB 1.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.36.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\josem\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.12.0)\n",
      "Collecting torchvision (from sentence-transformers)\n",
      "  Downloading torchvision-0.16.2-cp38-cp38-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\josem\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\josem\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\josem\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\josem\\appdata\\roaming\\python\\python38\\site-packages (from sentence-transformers) (3.7)\n",
      "Collecting sentencepiece (from sentence-transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-win_amd64.whl (977 kB)\n",
      "     -------------------------------------- 977.6/977.6 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.20.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\josem\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\josem\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\josem\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\josem\\anaconda3\\Lib\\site-packages\\~-rch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy-transformers 1.0.2 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.5 which is incompatible.\n",
      "spacy-transformers 1.0.2 requires transformers<4.6.0,>=3.4.0, but you have transformers 4.36.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
      "Requirement already satisfied: click in c:\\users\\josem\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\josem\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Collecting torch>=1.6.0 (from sentence-transformers)\n",
      "  Downloading torch-2.1.2-cp38-cp38-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (7.2.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\josem\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\josem\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (2.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (2.11.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from networkx->torch>=1.6.0->sentence-transformers) (4.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\josem\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.1.0)\n",
      "Downloading torchvision-0.16.2-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 1.1/1.1 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading torch-2.1.2-cp38-cp38-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 192.3/192.3 MB 1.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=54ad707c34dfa489190f90f01f61ea2d522b0a0c19d3aba949ca516189dc0533\n",
      "  Stored in directory: c:\\users\\josem\\appdata\\local\\pip\\cache\\wheels\\5e\\6f\\8c\\d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentencepiece, torch, torchvision, sentence-transformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.0\n",
      "    Uninstalling torch-1.12.0:\n",
      "      Successfully uninstalled torch-1.12.0\n",
      "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99 torch-2.1.2 torchvision-0.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6393,
    "lastExecutedAt": 1695259699866,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import the other required packages and modules.\nimport pandas as pd\nimport datetime\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import semantic_search\n\n\n# From the IPython.display package, import display and Markdown\nfrom IPython.display import display, Markdown",
    "outputsMetadata": {
     "0": {
      "height": 117,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import the other required packages and modules.\n",
    "import re\n",
    "import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "# From the IPython.display package, import display and Markdown\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import the Runway Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runway data is contained in a CSV file named `runway.csv`.\n",
    "\n",
    "The dataset contains the following columns.\n",
    "\n",
    "- `user_id`: the unique identifier for the user.\n",
    "- `item_id`: the unique identifier for the item/product rented.\n",
    "- `rating`: the rating by the user.\n",
    "- `rented_for`: the reason the item was rented.\n",
    "- `review_text`: the actual text for the submitted user review.\n",
    "- `category`: the category of the item rented.\n",
    "- `height`: the height of the user in the format {feet}'{inches}\".\n",
    "- `size`: the size of the item rented by the user.\n",
    "- `age`: the age of the user.\n",
    "- `review_date`: the date the review was made by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the runway data to a pandas dataframe.\n",
    "\n",
    "- Read the data from `runway.csv`, making sure to parse the date column. |Assign to `runway`.\n",
    "- Print the column info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 140,
    "lastExecutedAt": 1695261923135,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Read the data from runway.csv\nrunway = pd.read_csv(\"runway.csv\", parse_dates=['review_date'])\n\n# Print the column info\nprint(runway.info())",
    "outputsMetadata": {
     "0": {
      "height": 377,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'item_id', 'rating', 'rented for', 'review_text', 'category',\n",
      "       'height', 'size', 'age', 'review_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the data from runway.csv\n",
    "runway = pd.read_csv(\"runway.csv\")\n",
    "\n",
    "# Print the column info\n",
    "print(runway.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1506.000000</td>\n",
       "      <td>1.506000e+03</td>\n",
       "      <td>1506.000000</td>\n",
       "      <td>1506.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>498540.554449</td>\n",
       "      <td>8.423255e+05</td>\n",
       "      <td>9.102258</td>\n",
       "      <td>12.492696</td>\n",
       "      <td>33.663333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>291071.125644</td>\n",
       "      <td>6.057790e+05</td>\n",
       "      <td>1.395354</td>\n",
       "      <td>8.749235</td>\n",
       "      <td>7.950354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>772.000000</td>\n",
       "      <td>1.233730e+05</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>249571.000000</td>\n",
       "      <td>1.815240e+05</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>495970.500000</td>\n",
       "      <td>8.100440e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>754884.250000</td>\n",
       "      <td>1.373701e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>998134.000000</td>\n",
       "      <td>1.991314e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>116.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id       item_id       rating         size          age\n",
       "count    1506.000000  1.506000e+03  1506.000000  1506.000000  1500.000000\n",
       "mean   498540.554449  8.423255e+05     9.102258    12.492696    33.663333\n",
       "std    291071.125644  6.057790e+05     1.395354     8.749235     7.950354\n",
       "min       772.000000  1.233730e+05     2.000000     0.000000    15.000000\n",
       "25%    249571.000000  1.815240e+05     8.000000     8.000000    29.000000\n",
       "50%    495970.500000  8.100440e+05    10.000000    12.000000    32.000000\n",
       "75%    754884.250000  1.373701e+06    10.000000    16.000000    37.000000\n",
       "max    998134.000000  1.991314e+06    10.000000    58.000000   116.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runway.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>rented for</th>\n",
       "      <th>review_text</th>\n",
       "      <th>category</th>\n",
       "      <th>height</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>476109</td>\n",
       "      <td>139086</td>\n",
       "      <td>8</td>\n",
       "      <td>formal affair</td>\n",
       "      <td>it hit the floor perfectly with a pair of heel...</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 3\"</td>\n",
       "      <td>15</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Dec 19 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203660</td>\n",
       "      <td>1126889</td>\n",
       "      <td>6</td>\n",
       "      <td>party</td>\n",
       "      <td>the dress is absolutely gorgeous unfortunately...</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 4\"</td>\n",
       "      <td>12</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Jan 03 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>868581</td>\n",
       "      <td>652189</td>\n",
       "      <td>8</td>\n",
       "      <td>wedding</td>\n",
       "      <td>even though it was lined with satin this was a...</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 5\"</td>\n",
       "      <td>24</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Aug 05 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>935076</td>\n",
       "      <td>1879504</td>\n",
       "      <td>8</td>\n",
       "      <td>wedding</td>\n",
       "      <td>this dress was greatit fit really well and was...</td>\n",
       "      <td>sheath</td>\n",
       "      <td>5' 3\"</td>\n",
       "      <td>14</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Oct 02 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>995023</td>\n",
       "      <td>1179146</td>\n",
       "      <td>10</td>\n",
       "      <td>party</td>\n",
       "      <td>super flattering i am usually a sizemi have a ...</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 2\"</td>\n",
       "      <td>14</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Nov 20 2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating     rented for  \\\n",
       "0   476109   139086       8  formal affair   \n",
       "1   203660  1126889       6          party   \n",
       "2   868581   652189       8        wedding   \n",
       "3   935076  1879504       8        wedding   \n",
       "4   995023  1179146      10          party   \n",
       "\n",
       "                                         review_text category height  size  \\\n",
       "0  it hit the floor perfectly with a pair of heel...     gown  5' 3\"    15   \n",
       "1  the dress is absolutely gorgeous unfortunately...    dress  5' 4\"    12   \n",
       "2  even though it was lined with satin this was a...    dress  5' 5\"    24   \n",
       "3  this dress was greatit fit really well and was...   sheath  5' 3\"    14   \n",
       "4  super flattering i am usually a sizemi have a ...    dress  5' 2\"    14   \n",
       "\n",
       "    age  review_date  \n",
       "0  27.0  Dec 19 2017  \n",
       "1  28.0  Jan 03 2022  \n",
       "2  30.0  Aug 05 2021  \n",
       "3  37.0  Oct 02 2021  \n",
       "4  37.0  Nov 20 2022  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runway.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Preprocessing the `review_text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most unstructured text, such as reviews for products, are messy. They contain special characters which may not be necessary, extra spaces, irrelevant digits, and more. Therefore, it is common practice to process, or clean, the text before performing NLP tasks on it.\n",
    "\n",
    "You will create several processing steps for the `review_text` strings.\n",
    "\n",
    "Note: there may be some instances where special characters, digits, and the like are important to the meaning, or context, of the sentence. It's best to think through the implications of such preprocessing steps before blindly doing so.\n",
    "\n",
    "Also note: some Pythonistas will say preprocessing text before using transformers, such as those from Hugging Face, is unnecessary. We will explore this in the following tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create several steps that will preprocess the text for the following areas:\n",
    "- replace the special character \"/\" with a space;\n",
    "- remove punctuations;\n",
    "- remove digits;\n",
    "- replace runs of whitespace, i.e. more than one in a row, with no space; and\n",
    "- make lowercase.\n",
    "\n",
    "Apply these changes to the `review_text` column in the `runway` dataset. \n",
    "\n",
    "Save as `review_text_cleaned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 653,
    "lastExecutedAt": 1696016455807,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Remove the forward-slash character\nrunway['review_text_cleaned'] = runway['review_text_cleaned'].str.replace(r\"\\/\",\"\")\n\n# Remove punctuation\nrunway['review_text_cleaned'] = runway['review_text_cleaned'].str.translate(string.punctuation)\n\n# Remove digits\nrunway['review_text_cleaned'] = runway['review_text_cleaned'].str.replace(r\"\\d+\",\"\")\n\n# Remove running spaces\nrunway['review_text_cleaned'] = runway['review_text_cleaned'].str.replace(r\"\\s{2,}\",\"\")"
   },
   "outputs": [],
   "source": [
    "# Remove the forward-slash character\n",
    "\n",
    "\n",
    "# Remove punctuation\n",
    "\n",
    "\n",
    "# Remove digits\n",
    "\n",
    "\n",
    "# Remove running spaces\n",
    "\n",
    "\n",
    "# Make the text lowercase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Sentiment Analysis on `review_text_cleaned`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The review text is now clean from extraneous characters. You are ready to start performing some NLP tasks!\n",
    "\n",
    "You will start with sentiment analysis, or the categorizing of text into two buckets - negative or positive. Some models on Hugging Face have the ability to categorize text as neutral as well. \n",
    "\n",
    "Sentiment analysis is a common, and useful, NLP task performed in a typical business settings. It can help decision makers determine trends in product or service perception, customer experience, reviews, and more. It can also act as a starting point for deeper analysis.\n",
    "\n",
    "In this task, you will use the `pipeline()` function with the [\"distilbert-base-uncased-finetuned-sst-2-english\"](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) model. This is a smaller general-purpose language representation model that is great at text classification (i.e. sentiment analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline()` function can help you achieve this.\n",
    "\n",
    "- Instantiate a new pipeline object for `sentiment-analysis`. Save as `sentimentAnalysis`.\n",
    "- Use this with a list of the cleaned review text as the input to return sentiment for each review. Save the output as `sent_analysis_output`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this task is computationally intensive and may take several minutes to run. It runs faster in Premium Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 36792,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1695260793634,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n# Instantiate the new pipeline() object for sentiment analysis. Include the model defined above as the model input.\nsentimentAnalysis = pipeline(\"sentiment-analysis\", model = model)\n\n# Run sentiment analysis on the cleaned review text.\nsent_analysis_output = sentimentAnalysis(list(runway[\"review_text_cleaned\"]))",
    "outputsMetadata": {
     "0": {
      "height": 134,
      "type": "stream"
     },
     "4": {
      "height": 76,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Instantiate the new pipeline() object for sentiment analysis. Include the model defined above as the model input.\n",
    "\n",
    "\n",
    "# Run sentiment analysis on the cleaned review text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Histogram of Sentiment Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was straightfoward, yeah?\n",
    "\n",
    "As I'm sure you're aware by now, Hugging Face hosts a lot of models which can be employed for a variety of NLP tasks. The sentiment analysis pipeline above used the default model, but you can use any available model to suit your data (as long as they are in the [model hub](https://huggingface.co/models)). For example, [FashionClip](https://huggingface.co/patrickjohncyh/fashion-clip) is a model trained specifically for fashion-related datasets - such as this one - and tasks.\n",
    "\n",
    "On a related note, the [`pipeline()` module](https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines) is quite versatile as well! It encapsulates multiple transformer pipelines including NLP, computer vision, audio, and multi-modal. Within NLP alone, it covers 12 different tasks such as sentiment analysis (listed as \"Text Classification\"), Named Entity Recognition, Summarization, Conversations, and Q & A.\n",
    "\n",
    "We encourage you to further explore this powerful tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you're going to understand the distribution of the sentiment of review texts via a histogram. This will give you an overall understanding of the reviews specifically how customers viewed their rented items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse the output of `sent_analysis_output` into two new columns - `clean_sentiment_category` and `clean_sentiment_score`.\n",
    "- Use `seaborn` and the new sentiment score column to create a histogram. Set the number of bins to 20.\n",
    "- Add a title, \"Distribution of Sentiment Score\", subtitle - \"For Clean Review Text\" - and x and y labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 182,
    "lastExecutedAt": 1695260829103,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Parse out the two pieces of output from sent_analysis_output - \"label\" and \"score\".\nrunway['clean_sentiment_category'] = [s[\"label\"] for s in sent_analysis_output]\nrunway['clean_sentiment_score'] = [s[\"score\"] for s in sent_analysis_output]\n\n# Create the first histogram using the built-in Pandas histogram method.\nsns.histplot(data=runway, x=\"clean_sentiment_score\", bins=20)\nplt.suptitle(\"Distribution of Sentiment Score\")\nplt.xlabel(\"Sentiment Score\")\nplt.ylabel(\"Count of Reviews\")\nplt.title(\"For Clean Review Text\")\nplt.show()"
   },
   "outputs": [],
   "source": [
    "# Parse out the two pieces of output from sent_analysis_output - \"label\" and \"score\".\n",
    "\n",
    "\n",
    "# Create the first histogram using the built-in Pandas histogram method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Sentiment Over the Years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is informative from a global perspective - i.e., the reviews skew positive (\"great customer experience\"). However, it doesn't tell a complete story.\n",
    "\n",
    "Now that you have sentiment, you can explore the trends of sentiment across different facets of the data (i.e. over time, between products, etc.). Dissecting the data in this way, based on the business questions of interest, will explain more about what is going well and what needs improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you'll explore how sentiment changed over the years through a bar plot visualization. Feel free to explore other facets as well, if interested.\n",
    "\n",
    "- Create a new column called `year`.\n",
    "- Count the number of reviews by `year` and `clean_sentiment_category`. Save as `chart_data`.\n",
    "- Visualize the count of reviews using a seaborn `.barplot()`.\n",
    "- Add an appropriate title and axes labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5265,
    "lastExecutedAt": 1695260891322,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create a new column called \"year\" which is the year of the \"review_date\".\nrunway['year'] = runway['review_date'].dt.year\n\n# Create a new dataframe of review counts by \"year\" and \"cleaned_sentiment_category\".\nchart_data = runway.groupby(['year', 'clean_sentiment_category'], as_index=False)['user_id'].count()\nchart_data.columns = ['year', 'clean_sentiment_category', 'cnt']\n\n# Create a bar plot showing the count of reviews for each sentiment category over the years.\nsns.barplot(data=chart_data, x=\"year\", y=\"cnt\", hue=\"clean_sentiment_category\", errorbar = None)\nplt.title(\"Sentiment between 2016 - 2023\")\nplt.xlabel(\"Review Year\")\nplt.ylabel(\"Count of Reviews\")\nplt.show()"
   },
   "outputs": [],
   "source": [
    "# Create a new column called \"year\" which is the year of the \"review_date\".\n",
    "\n",
    "\n",
    "# Create a new dataframe of review counts by \"year\" and \"cleaned_sentiment_category\".\n",
    "\n",
    "\n",
    "# Create a bar plot showing the count of reviews for each sentiment category over the years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Does Cleaning Text Matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - that's quite a set of positive reviews! It seems like the amount of positive reviews increased dramatically in 2021. Potentially a revamp of the customer experience, product quality, more customers, or something else we don't have data for. \n",
    "\n",
    "From a business context, digging into the common words/n-grams of the positive reviews can be revealing as to what customers love about the products. Doing the same for the negative reviews can be just as informative for what they don't love (i.e. sizing issues, rental service, etc.). \n",
    "\n",
    "We will look into that soon. But first, let's test the previous comment on preprocessing text. Does it matter for sentiment analysis?\n",
    "\n",
    "In this task, you will build another sentiment analysis pipeline but, in this case, for the non-cleaned `review_text`. Then, you will compare the output from the two pipelines to understand if the cleaning made a difference in categorizing sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duplicate the steps from the previous exercise to run a sentiment analysis pipeline. Save as `sent_analysis_2`.\n",
    "- Parse the output and save as `sentiment_category` and `sentiment_score`.\n",
    "- Create a confusion matrix using `pd.crosstab()` comparing the output of `clean_sentiment_category` and `sentiment_category`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this task is computationally intensive and may take several minutes to run. It runs faster in Premium Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "chartConfig": {
     "bar": {
      "hasRoundedCorners": true,
      "stacked": false
     },
     "type": "bar",
     "version": "v1"
    },
    "executionCancelledAt": null,
    "executionTime": 32649,
    "lastExecutedAt": 1695260966484,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create a new sentiment analysis output for \"review_text\".\nsent_analysis_2 = sentimentAnalysis(list(runway[\"review_text\"]))\n\n# Parse the output into \"sentiment_category\" and \"sentiment_score\".\nrunway['sentiment_category'] = [s[\"label\"] for s in sent_analysis_2]\nrunway['sentiment_score'] = [s[\"score\"] for s in sent_analysis_2]\n\n# Print the confusion matrix comparing the two sentiment category outputs.\ndisplay(pd.crosstab(runway['clean_sentiment_category'], runway['sentiment_category']))",
    "outputsMetadata": {
     "0": {
      "height": 122,
      "type": "dataFrame"
     }
    },
    "visualizeDataframe": false
   },
   "outputs": [],
   "source": [
    "# Create a new sentiment analysis output for \"review_text\".\n",
    "\n",
    "\n",
    "# Parse the output into \"sentiment_category\" and \"sentiment_score\".\n",
    "\n",
    "\n",
    "# Print the confusion matrix comparing the two sentiment category outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous results, the sentiment of the review was pretty much the same with cleaned text. Good for us! We can use the original text to perform further analysis. Again, this is situation dependent, so be sure to think through the benefits of cleaning vs. leaving the text as is. \n",
    "\n",
    "Transformers, such as those on Hugging Face, are a component of machine learning / deep learning models which are designed to learn sequential data. Using a concept known as \"self-attention\", they use other tokens (words) in the sequence (sentence) to build an understanding of a specific token (word).\n",
    "\n",
    "All of this to say (in a simplified manner) transformers, specifically those trained on VERY large datasets, work well on unstructured, un-cleaned text because it takes the whole sentence into context. And every piece adds to that context [see BERT paper here that argues this point](https://arxiv.org/pdf/1904.07531.pdf).\n",
    "\n",
    "This can be particularly powerful from 1) a time-savings perspective and 2) enabling more unstructured text to be leveraged. With less hassle of data preparation, more data can be used to make decisions. Again, make sure you evaluate the use case and determine if the NLP results are meeting performance expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next task, we will switch gears to exploring another common use case for Hugging Face and transformers - text embeddings. \n",
    "\n",
    "Embeddings, in a very simplistic definition, are a vector (numerical) representation of something within n-dimensions. In this case, they are text embeddings. (Note: each transformer model will have a different number of dimensions for its results). Embeddings are useful because they represent human language to computers which enables a more sophisticated execution of similarity, text generation, semantic search, and the like. This can be extremely valuable for business tasks such as recommendations and searching within websites or products.\n",
    "\n",
    "You will now try this out on the `rented for` column using the `sentence_transformer` [package](https://huggingface.co/sentence-transformers). The model you'll use is the BERT-based [\"all-MiniLM-L6-v2\" model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) which transformer that maps sentences and paragraphs to an n-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate a new instance of `SentenceTransformer` using the specified model (`model_id`). Save as `model`.\n",
    "- Generate an embedding for each review text using `model.encode()`. Save as `embeddings`.\n",
    "- Print the shape of the new embeddings array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 920,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1695262275579,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# Instantiate a new SentenceTransformer object.\nmodel = SentenceTransformer(model_id)\n\n# Generate the embeddings for the \"rented for\" column.\nembeddings = model.encode(list(runway[\"rented for\"]))\n\nprint(embeddings.shape)",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     },
     "14": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Instantiate a new SentenceTransformer object.\n",
    "\n",
    "\n",
    "# Generate the embeddings for the \"rented for\" column.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The all-MiniLM-L6-v2 model outputs embeddings of 384 dimensions. A higher-dimensional embedding can capture more of the relationship between words which is fantastic!\n",
    "\n",
    "However, for some tasks, such as clustering, it can be a challenge. Clustering tends to perform poorly with higher dimensions of data (i.e. the Curse of Dimensionality), which poses a common challenge in text mining and NLP. Therefore, dimensionality reduction is often employed in order to calculate distance (e.g. Euclidean) between embeddings, then calculate clusters.\n",
    "\n",
    "Since the transformer is constructed in such a way to be better at learning context, text - words, sentences, documents - that are similar should have similar vectors and therefore be closer together (i.e. in the same cluster).\n",
    "\n",
    "Understanding which embeddings are closer together can then be used to determine which products, users, reviews, etc. are similar to each other. This is an important step in common method for building recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you'll conduct dimensionality reduction using the `TSNE` module from the `sklearn` package.\n",
    "\n",
    "- Import `TSNE` from `sklearn.manifold`.\n",
    "- Create a new TSNE object using the defined input parameters. Save as `tsne`.\n",
    "- Fit the `tsne` model with the saved embeddings. Save this as `tsne_dims`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2829,
    "lastExecutedAt": 1695262282682,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import TSNE\nfrom sklearn.manifold import TSNE\n\n# Instantiate a new TSNE object. \ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\n\n# Generate the tsne dimensions using the saved embeddings.\ntsne_dims = tsne.fit_transform(embeddings)\n\nprint(tsne_dims.shape)",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import TSNE\n",
    "\n",
    "\n",
    "# Instantiate a new TSNE object. \n",
    "\n",
    "\n",
    "# Generate the tsne dimensions using the saved embeddings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Visualizing the Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each embedding is 2-dimensional. You don't need to be so drastic in dimensionality reduction for clustering, i.e. use explained variance ratio to determine the right number of components, but is important for visualizing the clusters.\n",
    "\n",
    "With these smaller vectors, let's move on to generating basic clusters (the average of each category) and building a visualization for them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract the x and y dimensions from the `tsne_dims` result. Save as new columns in `runway` named `x` and `y`, respectively.\n",
    "- Use `seaborn.scatterplot` to create a new scatterplot using the x and y dimensions and `category` as the \"hue\".\n",
    "- Set the title as `Embeddings visualized for 'rented for' reasons and category using t-SNE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 264,
    "lastExecutedAt": 1695262283786,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "plt.rcParams['figure.figsize'] = (15, 8)\n\n# Parse out the x and y dimensions from the tsne output.\nrunway['x'] = [x for x,y in tsne_dims]\nrunway['y'] = [y for x,y in tsne_dims]\n\n# Create the scatterplot\nsns.scatterplot(x = \"x\", y = \"y\", hue = \"category\", data = runway)\nplt.legend()\nplt.title(\"Embeddings visualized for 'rented for' reasons and category using t-SNE\")\nplt.show()"
   },
   "outputs": [],
   "source": [
    "# Parse out the x and y dimensions from the tsne output.\n",
    "\n",
    "\n",
    "# Create the scatterplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is showing us all of the review embeddings - based on the \"rented_for\" column - in a two-dimensional space (since our eyes can't handle 384 dimensions)! The color is indicating the type of item rented, i.e. \"dress\", \"gown\", \"sheath\".\n",
    "\n",
    "There are clearly two main clusters in this dataset for \"rented for\" - the mainly blue cluster and the mainly orange one. The blue cluster consists primarily of gowns while the other is mainly dresses.\n",
    "\n",
    "This is a simple example yet still very informative about how customers rent items for which types of occasions.\n",
    "\n",
    "Embeddings are not only cool to visualize, as mentioned above, they also facilitate \"semantic search\". Semantic search denotes \"searching with meaning\" and with context when available. The goal is to infer what the user's intent is then find the most relevant results. This is different from \"lexical search\" where a search engine looks for literal matches of a query (or defined variants). Both have their strengths and weaknesses, which we encourage you to research further.\n",
    "\n",
    "Semantic search can be incredibly powerful in a customer experience setting. Specifically, they can help with recommendations (i.e. which product is similar based on reviews, descriptions, etc.) and - obviously - search (i.e. find me products with these phrases or words in their reviews). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you'll build a simple semantic search pipeline using the `sentence-transformer` package.\n",
    "\n",
    "- First, generate an embedding of the pre-defined query using the same model object saved above. Save this query embedding as `query_emb`.\n",
    "- With the `semantic_search()` function, use the query embedding and the other embeddings to get the top three \"hits\", i.e. most similar items by review.\n",
    "- Use a for-loop to print the category and rented for reason for each hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1695262319213,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "query = \"a gorgeous and flattering dress\"\nquery_emb = model.encode(query, convert_to_tensor=True)\n\nhits = semantic_search(query_emb, embeddings, top_k=3)\n\nfor hit in hits[0]:\n    idx = hit['corpus_id']\n    print(\n        \"ITEM ID: \", runway.iloc[idx]['item_id'], \n        \"; RENTED FOR: \", runway.iloc[idx]['rented for'],\n        \"; REVIEW\", runway.iloc[idx]['review_text'])",
    "outputsMetadata": {
     "0": {
      "height": 357,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define query\n",
    "query = \"a gorgeous and flattering dress\"\n",
    "\n",
    "# Embed query\n",
    "\n",
    "\n",
    "# Calculate similarity between query and item embeddings\n",
    "\n",
    "\n",
    "# Print top similar items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11: Generate New Marketing Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic search returned three different items that appear to be similar to what we were searching for, i.e. \"the dress was gorgeous\" and \"this dress was a great fit\". With some fine-tuning, you can achieve even greater performance. We can also notice some constructive feedback on other parts of the dress. Further investigation here would reveal important insights for the business team.\n",
    "\n",
    "Let's again switch gears for this final task. Let's generate some new text based on a prompt. Specifically, you want to create some brainstorming material for a new marketing campaign. This type of process can save a lot of time in the creation process - helping you start at \"I have a good idea\" vs. \"I need an idea\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you'll switch back to the `pipeline()` module.\n",
    "\n",
    "- Instantiate a text generation pipeline using `model = \"gpt2\"` as the input parameter. Save as `generator`.\n",
    "- Using this generator, create 1 new piece of text based on the pre-defined prompt. Save as `output`.\n",
    "- Print the prompt and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "1": {
      "height": 37,
      "type": "stream"
     },
     "2": {
      "height": 95,
      "type": "stream"
     },
     "6": {
      "height": 37,
      "type": "stream"
     },
     "7": {
      "height": 117,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the pipeline for generating text\n",
    "\n",
    "\n",
    "# Pre-defined prompt\n",
    "prompt = \"New for this winter season, a lovely dress that\"\n",
    "\n",
    "# Use the prompt as input to the generator to return output.\n",
    "\n",
    "\n",
    "# Print the generated text.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
