{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Multimodal AI Applications with LangChain & the OpenAI API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\n",
    "\n",
    "In this project, you'll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understanding the building blocks of working with Multimodal AI projects\n",
    "- Working with some of the fundamental concepts of LangChain  \n",
    "- How to use the Whisper API to transcribe audio to text \n",
    "- How to combine both LangChain and Whisper API to create ask questions of any YouTube video "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need a developer account with [OpenAI ](https://auth0.openai.com/u/signup/identifier?state=hKFo2SAyeTZBU1pzbUNWYWs3Wml5OWVvUVh4enZldC1LYU9PMaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIDFUakNoUGFMLUdNWFpfQkpqdncyZjVDQk9xUTE4U0xDo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q) and a create API Key. The API secret key will be stored in your 'Environment Variables' on the side menu. See the *getting-started.ipynb* notebook for details on setting this up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project requires several packages that need to be installed into Workspace.\n",
    "\n",
    "- `langchain` is a framework for developing generative AI applications.\n",
    "- `yt_dlp` lets you download YouTube videos.\n",
    "- `tiktoken` converts text into tokens.\n",
    "- `docarray` makes it easier to work with multi-model data (in this case mixing audio and text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Run the following code to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10465,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705467366,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install langchain yt_dlp",
    "outputsMetadata": {
     "0": {
      "height": 463,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Install langchain\n",
    "#!pip install langchain==0.0.292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8477,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694802259584,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Install yt_dlp\n!pip install yt_dlp==2023.7.6",
    "outputsMetadata": {
     "0": {
      "height": 447,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Install yt_dlp\n",
    "# !pip install yt_dlp==2023.7.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6113,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705815654,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install tiktoken docarray",
    "outputsMetadata": {
     "0": {
      "height": 447,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install docarray==0.38.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import The Required Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we need the `os` and the `yt_dlp` packages to download the YouTube video of your choosing, convert it to an `.mp3` and save the file. We will also be using the `openai` package to make easy calls to the OpenAI models we will use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following packages.\n",
    "\n",
    "- Import `os` \n",
    "- Import `openai`\n",
    "- Import `yt_dlp` with the alias `youtube_dl`\n",
    "- From the `yt_dlp` package, import `DowloadError`\n",
    "- Assign `openai_api_key` to `os.getenv(\"OPENAI_API_KEY\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 176,
    "lastExecutedAt": 1694705470957,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n\nimport os   #import os package \nimport glob\nimport openai #import the openai package \nimport yt_dlp as youtube_dl #import the yt_dlp package as youtube_dl\nfrom yt_dlp import DownloadError #import DownloadError from yt_dlp ",
    "outputsMetadata": {
     "0": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n",
    "\n",
    "# Import the os package \n",
    "import os\n",
    "\n",
    "# Import glob\n",
    "import glob\n",
    "\n",
    "# Import the openai package \n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Import the yt_dlp package as youtube_dl\n",
    "import yt_dlp as youtube_dl\n",
    "\n",
    "# Import DownloadError from yt_dlp \n",
    "from yt_dlp import DownloadError\n",
    "\n",
    "# Import Docarray\n",
    "import docarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also assign the variable `openai_api_key` to the environment variable \"OPEN_AI_KEY\". This will help keep our key secure and remove the need to write it in the code here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10,
    "lastExecutedAt": 1694705474406,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Download the YouTube Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3). \n",
    "\n",
    "We'll download a DataCamp tutorial about machine learning in Python.\n",
    "\n",
    "We will do this by setting a variable to store the `youtube_url` and the `output_dir` that we want the file to be stored. \n",
    "\n",
    "The `yt_dlp` allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you. \n",
    "\n",
    "Lastly, we will create a loop that looks in the `output_dir` to find any .mp3 files. Then we will store those in a list called `audio_files` that will be used later to send each file to the Whisper model for transcription. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following: \n",
    "- Two variables - `youtube_url` to store the Video URL and `output_dir` that will be the directory where the audio files will be saved. \n",
    "- For this tutorial, we can set the `youtube_url` to the following `\"https://www.youtube.com/watch?v=aqzxYofJ_ck\"`and the `output_dir`to `files/audio/`. In the future, you can change these values. \n",
    "- Use the `ydl_config` that is provided to you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 533,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694802393469,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(f\"Downloading video from {youtube_url}\")\n\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\n\n\n",
    "outputsMetadata": {
     "0": {
      "height": 357,
      "type": "stream"
     },
     "1": {
      "height": 137,
      "type": "stream"
     },
     "2": {
      "height": 97,
      "type": "stream"
     },
     "3": {
      "height": 37,
      "type": "stream"
     },
     "4": {
      "height": 257,
      "type": "stream"
     },
     "5": {
      "height": 77,
      "type": "stream"
     },
     "6": {
      "height": 57,
      "type": "stream"
     },
     "7": {
      "height": 57,
      "type": "stream"
     },
     "8": {
      "height": 97,
      "type": "stream"
     },
     "9": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Encodings: locale cp1252, fs utf-8, pref cp1252, out UTF-8 (No VT), error UTF-8 (No VT), screen UTF-8 (No VT)\n",
      "[debug] yt-dlp version stable@2023.07.06 [b532a3481] (pip) API\n",
      "[debug] params: {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'outtmpl': 'files/audio/%(title)s.%(ext)s', 'verbose': True, 'compat_opts': set()}\n",
      "[debug] Python 3.8.3 (CPython AMD64 64bit) - Windows-10-10.0.19041-SP0 (OpenSSL 1.1.1g  21 Apr 2020)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video from https://www.youtube.com/watch?v=QEaBAZQCtwE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] exe versions: ffmpeg 2023-01-16-git-01f46f18db-full_build-www.gyan.dev (setts), ffprobe 2023-01-16-git-01f46f18db-full_build-www.gyan.dev\n",
      "[debug] Optional libraries: Cryptodome-3.19.1, brotli-None, certifi-2022.12.07, mutagen-1.47.0, sqlite3-2.6.0, websockets-12.0\n",
      "[debug] Proxy map: {}\n",
      "[debug] Loaded 1855 extractors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=QEaBAZQCtwE\n",
      "[youtube] QEaBAZQCtwE: Downloading webpage\n",
      "[youtube] QEaBAZQCtwE: Downloading ios player API JSON\n",
      "[youtube] QEaBAZQCtwE: Downloading android player API JSON\n",
      "[youtube] QEaBAZQCtwE: Downloading m3u8 information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec:vp9.2, channels, acodec, lang, proto\n",
      "[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec:vp9.2(10), channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] QEaBAZQCtwE: Downloading 1 format(s): 251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Invoking http downloader on \"https://rr2---sn-8vq54voxn25po-mgte.googlevideo.com/videoplayback?expire=1705442289&ei=kaemZcKrHpLThcIPhomD8AU&ip=84.125.157.205&id=o-AB9-Wuu3pMWZ_HEtMxTVWTrtmMAz4BYXvt2AV9iWV3Hs&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=2r&mm=31%2C29&mn=sn-8vq54voxn25po-mgte%2Csn-h5q7knes&ms=au%2Crdu&mv=m&mvi=2&pl=21&initcwndbps=1578750&spc=UWF9fzCykIsOVk2lFk55LOmS_vIv6QaqhNQR&vprv=1&svpuc=1&mime=audio%2Fwebm&gir=yes&clen=12464652&dur=888.341&lmt=1649011484063146&mt=1705420142&fvip=3&keepalive=yes&fexp=24007246&c=ANDROID&txp=5318224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIhAKLms8KOUjE14dLeQ3mH69BFbOj-h3u7hfA4y6tu56jgAiAP-2nxHyZVryspELjtcspMrmhWnTI3zurOeIFpgQ1MdA%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AAO5W4owRQIgWn0EL0cQCIJHAVOmIfX6Ib7ZnqqPyGu_CH7_4MFBkcgCIQDxCgFsNpxIIJQ4eJjq8y4EbO0DWErwyCSiSFJxsZr9ZA%3D%3D\"\n",
      "[debug] File locking is not supported. Proceeding without locking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Destination: files\\audio\\Getting Started With Hugging Face in 15 Minutes ｜ Transformers, Pipeline, Tokenizer, Models.webm\n",
      "[download] 100% of   11.89MiB in 00:00:03 at 3.52MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffprobe -show_streams \"file:files\\audio\\Getting Started With Hugging Face in 15 Minutes ｜ Transformers, Pipeline, Tokenizer, Models.webm\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExtractAudio] Destination: files\\audio\\Getting Started With Hugging Face in 15 Minutes ｜ Transformers, Pipeline, Tokenizer, Models.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffmpeg -y -loglevel \"repeat+info\" -i \"file:files\\audio\\Getting Started With Hugging Face in 15 Minutes ｜ Transformers, Pipeline, Tokenizer, Models.webm\" -vn -acodec libmp3lame \"-b:a\" 192.0k -movflags \"+faststart\" \"file:files\\audio\\Getting Started With Hugging Face in 15 Minutes ｜ Transformers, Pipeline, Tokenizer, Models.mp3\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file files\\audio\\Getting Started With Hugging Face in 15 Minutes ｜ Transformers, Pipeline, Tokenizer, Models.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "# An example YouTube tutorial video\n",
    "youtube_url = \"https://www.youtube.com/watch?v=QEaBAZQCtwE\"\n",
    "# Directory to store the downloaded video\n",
    "output_dir = \"files/audio/\"\n",
    "\n",
    "# Config for youtube-dl\n",
    "ydl_config = {\n",
    "    \"format\": \"bestaudio/best\",\n",
    "    \"postprocessors\": [\n",
    "        {\n",
    "            \"key\": \"FFmpegExtractAudio\",\n",
    "            \"preferredcodec\": \"mp3\",\n",
    "            \"preferredquality\": \"192\",\n",
    "        }\n",
    "    ],\n",
    "    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "# Check if the output directory exists, if not create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# Print a message indicating which video is being downloaded\n",
    "print(f\"Downloading video from {youtube_url}.\")\n",
    "\n",
    "# Attempt to download the video using the specified configuration\n",
    "# If a DownloadError occurs, attempt to download the video again\n",
    "try:\n",
    "    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "except DownloadError as DE:\n",
    "    print(f\"An error occured: {DE}\")\n",
    "    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([youtube_url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the audio files that we will use the `glob`module that looks in the `output_dir` to find any .mp3 files. Then we will append the file to a list called `audio_files`. This will be used later to send each file to the Whisper model for transcription. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following: \n",
    "- A variable called `audio_files`that uses the glob module to find all matching files with the `.mp3` file extension \n",
    "- Select the first first file in the list and assign it to `audio_filename`\n",
    "- To verify the filename, print `audio_filename` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1694705587367,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Find the audio file in the output directory\n\n# Define function parameters\noutput_dir = \"files/audio/\"\n\n# Find the audio file in the output directory\naudio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\naudio_filename = audio_files[0]\nprint(audio_filename)",
    "outputsMetadata": {
     "0": {
      "height": 56,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/audio\\Getting Started With Hugging Face in 15 Minutes ｜ Transformers, Pipeline, Tokenizer, Models.mp3\n"
     ]
    }
   ],
   "source": [
    "# Find the audio file in the output directory\n",
    "\n",
    "# Find all the audio files in the output directory\n",
    "audio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n",
    "\n",
    "# Select the first audio file in the list\n",
    "audio_filename = audio_files[0]\n",
    "\n",
    "# Print the name of the selected audio file\n",
    "print(audio_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Transcribe the Video using Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the `audio_file`, for the `output_file` and the model. \n",
    "\n",
    "Using these variables we will:\n",
    "- create a list to store the transcripts\n",
    "- Read the Audio File \n",
    "- Send the file to the Whisper Model using the OpenAI package "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this step, create the following: \n",
    "- A variable named `audio_file`that is assigned the `audio_filename` we created in the last step\n",
    "- A variable named `output_file` that is assigned the value `\"files/transcripts/transcript.txt\"`\n",
    "- A variable named `model` that is assigned the value  `\"whisper-1\"`\n",
    "- An empty list called `transcripts`\n",
    "- A variable named `audio` that uses the `open` method and `\"rb\"` modifier on the `audio_file`\n",
    "- A variable to store the `response` from the `openai.Audio.transcribe` method that takes in the `model`and `audio` variables \n",
    "- Append the `response[\"text\"]`to the `transcripts` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 35820,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705690478,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Define function parameters\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Transcribe audio to text\nprint(audio_file)\nprint(\"converting audio to text...\")\nwith open(audio_file, \"rb\") as audio:\n    response = openai.Audio.transcribe(model, audio)\n\ntranscript = (response[\"text\"])\n\n",
    "outputsMetadata": {
     "0": {
      "height": 76,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting audio to text...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define function parameters\n",
    "audio_file = audio_filename\n",
    "\n",
    "\n",
    "# Transcribe the audio file to text using OpenAI API\n",
    "print(\"Converting audio to text...\\n\")\n",
    "\n",
    "client = OpenAI()\n",
    "with open(audio_filename, \"rb\") as file:\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\", \n",
    "        file=file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the transcript from the response\n",
    "transcript = transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the transcripts to text files we will use the below provided code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 27,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705694076,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "if output_file is not None:\n    # save transcript to a .txt file\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    with open(output_file, \"w\") as file:\n        file.write(transcript)\n\nprint(transcript)\n\n",
    "outputsMetadata": {
     "0": {
      "height": 532,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "output_file = \"files/transcripts/transcript.txt\"\n",
    "\n",
    "# If an output file is specified, save the transcript to a .txt file\n",
    "\n",
    "if output_file is not None:\n",
    "    \n",
    "    # Create the directory for the output file if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Write the transcript to the output file\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi everyone, today I show you how to get started with HuggingFace and the Transformers library. The HuggingFace Transformers library is the most popular NLP library in Python with over 60,000 stars on GitHub. It provides state-of-the-art natural language processing models and a very clean API that makes it super simple to build powerful NLP pipelines even for beginners. So today I show you how to get started with it. I show you how to use the pipeline, how to use model and tokenizer, how to combine it with PyTorch or TensorFlow, how to save and load models, how to use models from the official model hub and also how to fine tune your own models. So let's get started. So first of all, how do we install the Transformers library? So the Transformers library should be combined with your favorite deep learning library. So this could be PyTorch or TensorFlow or even Flex. So go ahead and install these first and then you can install the Transformers library by saying pip install Transformers and that's all you need to do. First let's have a look at the pipeline. So a pipeline makes it super simple to apply an NLP task because it abstracts a lot of things away for us and the way it works is that we say from Transformers import pipeline. Then we create a pipeline object. So we say classifier equals pipeline and here we put in a task. So in this case, we want to do sentiment analysis. There are a lot of more tasks available and we will have a look at them in a moment. But for now, let's do the sentiment analysis. So we create our object and then we apply this classifier and here we put in the data that we want to test. So in this case, we only put in one string and the string is I've been waiting for a hugging face course my whole life and then we print the results. So now let's run this and see how the result looks like. All right, and here's the result. So we see the label which is positive and we also get a score. So almost 96%. So yeah, this is super cool. And the way this pipeline works is that it will do three things for us. So the first one is the preprocessing. So it's preprocessing the text. So in this case, it's applying a tokenizer. Then it feeds the preprocessed text to the model. Then it applies the model and then it also does the post processing. So post processing means it will show us the result how we would expect it. So in this case of a sentiment analysis pipeline, it for example, shows us the label positive or negative, but it can also look different for different tasks. So yeah, that's how it works. And now let's look at a few other examples of pipelines. For example, we can also use a text generation pipeline and we can also give it a specific model. So in the first example, we just use the default model, which you can also see here in the output, but you can give it a specific model, either one that you have saved locally or one from the model hub. So we will also have a look at this in a moment. So yeah, let's apply this example to generate some text. And you can also see there are different available arguments. So for this, I just recommend to check out the documentation. So yeah, here's the results. We wanted to have two possible return sequences. So the first generated text is this one in this course, we will teach you how to play chess. Or here's a second one in this course, we will teach you how to use a combination of a traditional and simple blah, blah, blah. So yeah, this also works. And now let's have a look at a third example. For example, we can do zero shot classification. This means we can give it a text without knowing the corresponding label. And then we put different candidate labels. For example, this text can be education, politics or business. And then let's run this and see the result. And here we get the results of all the different labels and the different scores and the highest scores with over 96% is the education which is correct. So let's have a look at the different other available pipelines. So for this, I recommend to go to the official documentation. And here you see all the available tasks. For example, we can do audio classification, we can do automatic speech recognition, we can do image classification, question answering, and translation summarization. So yeah, this is super cool. And yeah, I just recommend to play around with different ones and see how it looks like. Now let's have a look behind the pipeline and understand the different steps a little bit better. So for this, we have a look at a tokenizer and the model class. So we can say from transformers import auto tokenizer and auto model for sequence classification. So this one here is a very generic class. And this is also a generic class, but a little bit more specified for the sequence classification task. So for this, I just recommend to have a look at the official documentation. But for example, if you know you want a specific one, there's for example, also a bird tokenizer class and a bird model class. So yeah, so we import those classes, and then we create instances of this. So for this, we specify a model name. So in this case, this is just the default model that is used for this pipeline. And then we call the model class and say dot from pre trained with the model name and the same for the tokenizer. And this from pre trained method is a very important method in hugging face that you will see a lot of times. So just keep this one in mind. And now that we have this, we can for example, copy and paste the same code. And now for the pipeline, we can say model equals model and tokenizer equals the tokenizer. And now since this is just the same default model, this should produce the very same result. So let's run this and have a look at the output. And the result is the very same like I said, so this works. So yeah, this is what's going on under the hood. So there will be a tokenizer and a model. So now let's have a look at the tokenizer and see what this is doing. So a tokenizer basically puts a text in a mathematical representation that the model understands. And in order to use this, we can call the tokenizer directly and give it a text as input or we can also put in multiple texts as once as a list. And we can so here we do this and print this. And we can also do this separately. So we can call tokenizer dot tokenize. This will give us tokens back, then we can call tokenizer dot convert tokens to IDs, this will give us the IDs. And we can do it the other way around. So we can call tokenizer dot decode IDs. And this will give us the original string back. So let's run this and have a look at the different outputs. Alright, so here we see the output. So if we apply the tokenizer directly, then here we get this dictionary. And the dictionary contains the input IDs that look like this, then we also have a attention mask. So for now, we don't have to worry about this. A attention mask basically is a list of zeros and ones and a zero means that the attention layer should ignore this token. Then if we do this separately, so if we call tokenizer dot tokenize, then here we see the different tokens. Then if we convert the tokens to IDs, then each token has a unique corresponding ID. So we see this here. And if we decode this, then we get the original string back. But here, please note that we basically remove the capitalization. But yeah, and now if we compare this one with this one, then you see this should be the very same IDs. But here we also have this ID and this ID. So this means beginning of sentence and end of sentence. But basically, yeah, it's the same. And yeah, and this is how a tokenizer works. Now let's see how we can combine the code with pytorch or TensorFlow. So in this example, we use pytorch, but the code is very similar with TensorFlow. So with TensorFlow, usually we have a TF before all those classes. And yeah, in the first case, I simply apply the pipeline like before. And now we use multiple sentences. So usually we just put in one sentence, but we can use a list of all those sentences. So we call this our X train data. And yeah, here we feed it to the pipeline classifier and print the result. Now we do this separately. So first we call the tokenizer with the X train data, and we call this our batch. And then we can give it different arguments like padding equals true, truncation equals true, max length equals 512, and return tensors equals PT. So this will be in pytorch format. So you will see how this looks in a moment because we print the batch. So yeah, usually we apply the tokenizer directly instead of doing the different functions separately. And then we do the inference in pytorch. So for this, we say with torch dot no grad, then we call our model. And here we unpack this batch because this is a dictionary. And then we can apply different functions like F dot softmax to get the predictions or torch dot argmax to get the labels. And again, these predictions should be the same scores that we get from our pipeline because it essentially is the same step except that now we do it for ourselves. So let's run this and have a look at the result. So yeah, here we print the batch and you see this is a dictionary with the input IDs. And now we see this is a tensor and this is because we specified in pytorch tensor format. So without this, this would just be a normal list and then we had to take care of putting it in the correct format ourselves. But yeah, this makes it super handy to work with pytorch and TensorFlow. And then here we print the predictions and the labels. And you see if we compare this prediction score with this one, then it's the very same. So yeah, this is how it works if we do it step by step. And this could be useful if we, for example, want to fine tune our model with a pytorch training loop. Now to save a tokenizer and model, we can specify a safe directory. And then we can call tokenizer dot save pre trained and also model dot save pre trained. And when we want to load this again, we can pick a class like this one and then call auto tokenizer dot from pre trained and also for the model we say dot from pre trained and then we get the loaded tokenizer and model and this should get you the same results as before. Now let's have a look at how we can use different models from the model hub. So on the official homepage, we can click on models and then you see there are almost 35,000 models available created from the community, which is just awesome. So here on the left side, we can filter for example, we can filter for the different pipeline tasks, or we can filter for libraries or data sets or languages. And we can also use the search bar. For example, if I want a German model, I can simply search for this. So here, let's filter for text classification. So this is the same as the text analysis task. And in this case, this is the default model. And usually the name says the name of the models. So in this case, it's a distributed based on case model. And then it's fine tuned on the SST to data set and it's in English. So yeah, and then you can read through this and find out more information. So let's for example, clear this and search for summarization and then click on this one. And yeah, sometimes you even find code examples. And the way you can use this now is either you grab the code example, or here in the top next to the model name, you can click on this copy icon. This will copy the whole name. And now we can jump back to the code. And then for example, here again, we want a pipeline and in this case, we know it's a summarization pipeline. And then as model now here we paste in this model name and then it's applying this one from the model hub. And this is how you can use the model hub to use different models. Now let's briefly go over how we can fine tune our own model. So I'm not going into detail here, but they have excellent documentation on the official pages. So I will put the link in the description. And by the way, also, you could switch here between pytorch and TensorFlow code and then open a call up and have a look at the example code. So this is super helpful, but usually the way it works. So of course, for fine tuning, we use our own data sets. So we prepare this, then we load a pre trained tokenizer and call it with this data set and get the encodings. Then in case of pytorch, we prepare a pytorch data set with the encodings. Then we also load a pre trained model. And now we can use this trainer class from the transformers library and also the trainer arguments. And then we set this up with the model that we want to use and the data sets that we prepared. And then we simply call trainer dot train. And this is how we do this. We could also again do this with our native pytorch training loop. But this just makes the life super simple. And this is how you fine tune your own data. All right, I hope you enjoyed this tutorial. If you have any questions, let us know in the comments. Also, you might enjoy this video about how to get started with OpenAI and GPT-3. So if you haven't already, then check this out and then I hope to see you in the next video. Bye.\n"
     ]
    }
   ],
   "source": [
    "# Print the transcript to the console to verify it worked \n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create a TextLoader using LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the `TextLoader` that will take the text from our transcript and load it into a document. \n",
    "\n",
    "---\n",
    "In LangChain, a document is essentially a piece of text along with associated metadata. You can create a document object by importing the `Document` class from the `langchain/document` module and then passing the text and metadata to the constructor of this class. The text is the main content that interacts with the language model, and the metadata can include information such as the source of the document.\n",
    "\n",
    "Document loaders in LangChain are used for importing data from various sources and converting them into document objects. These loaders can handle different types of input, such as a simple text file, the text contents from a web page, or even transcriptions from videos. The loaders provide a method called `load` which is used to import data as a document from a pre-configured source.\n",
    "\n",
    "Furthermore, LangChain offers Document Chains, which are sets of tools that allow for efficient processing and analysis of large amounts of text data. These chains can be used for a range of purposes, such as summarizing documents, answering questions over documents, and extracting information from them.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this step, do the following: \n",
    "- Import `TextLoader` from `langchain.document_loaders`\n",
    "- Create a variable called `loader` that uses the `TextLoader` method which takes in the directory of the transcripts `\"./files/text\"`\n",
    "- Create a variable called `docs` that is assigned the result of calling the `loader.load()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1694705724878,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.document_loaders import TextLoader\n\nloader = TextLoader(\"./files/text\")\ndocs = loader.load()"
   },
   "outputs": [],
   "source": [
    "# Import the TextLoader class from the langchain.document_loaders module\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# Create a new instance of the TextLoader class, specifying the directory containing the text files\n",
    "loader = TextLoader(\"./files/transcripts/transcript.txt\")\n",
    "\n",
    "\n",
    "# Load the documents from the specified directory using the TextLoader instance\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705727440,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "docs[0]"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./files/transcripts/transcript.txt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first element metadata of docs to verify it has been loaded \n",
    "docs[0].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705727440,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "docs[0]"
   },
   "outputs": [],
   "source": [
    "# Show the first element of docs to verify it has been loaded \n",
    "#docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Creating an In-Memory Vector Store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space. \n",
    "\n",
    "For large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the `docarray` package. \n",
    "\n",
    "We will also tokenize our queries using the `tiktoken` package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model \"understand\" the text and relationships with other tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Import the `tiktoken` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 46,
    "lastExecutedAt": 1694705815702,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import tiktoken"
   },
   "outputs": [],
   "source": [
    "# Import the tiktoken package\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Create the Document Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use LangChain to complete some important operations to create the Question and Answer experience. Let´s import the follwing: \n",
    "\n",
    "- Import `RetrievalQA` from `langchain.chains` - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents \n",
    "- Import `ChatOpenAI` from `langchain.chat_models` - this imports the ChatOpenAI model that we will use to query the data \n",
    "- Import `DocArrayInMemorySearch` from `langchain.vectorstores` - this gives the ability to search over the vector store we have created. \n",
    "- Import `OpenAIEmbeddings` from `langchain.embeddings` - this will create embeddings for the data store in the vector store. \n",
    "- Import `display` and `Markdown`from `IPython.display` - this will create formatted responses to the queries. ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8,
    "lastExecutedAt": 1694706214485,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain.embeddings import OpenAIEmbeddings"
   },
   "outputs": [],
   "source": [
    "# Import the RetrievalQA class from the langchain.chains module\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Import the ChatOpenAI class from the langchain.chat_models module\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# Import the OpenAIEmbeddings class from the langchain.embeddings module\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a vector store that will use the `DocArrayInMemory` search methods which will search through the created embeddings created by the OpenAI Embeddings function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this step: \n",
    "- Create a variable called `db`\n",
    "- Assign the `db` variable to store the result of the method `DocArrayInMemorySearch.from_documents`\n",
    "- In the DocArrayInMemorySearch method, pass in the `docs` and a function call to `OpenAIEmbeddings()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 502,
    "lastExecutedAt": 1694706217261,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "db = DocArrayInMemorySearch.from_documents(\n    docs, \n    OpenAIEmbeddings()\n)"
   },
   "outputs": [],
   "source": [
    "# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs,\n",
    "    OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a retriever from the `db` we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the `ChatOpenAI` model, will assigned that as our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following: \n",
    "- A variable called `retriever` that is assigned `db.as_retriever()`\n",
    "- A variable called `llm` that creates the `ChatOpenAI` method with a set `temperature`of `0.0`. This will controle the variability in the responses we receive from the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8,
    "lastExecutedAt": 1694706219264,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "retriever = db.as_retriever() \nllm = ChatOpenAI(temperature = 0.0)"
   },
   "outputs": [],
   "source": [
    "# Convert the DocArrayInMemorySearch instance to a retriever\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create a new ChatOpenAI instance with a temperature of 0.0\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last step before starting to ask questions is to create the `RetrievalQA` chain. This chain takes in the:  \n",
    "- The `llm` we want to use\n",
    "- The `chain_type` which is how the model retrieves the data\n",
    "- The `retriever` that we have created \n",
    "- An option called `verbose` that allows use to see the seperate steps of the chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable called `qa_stuff`. This variable will be assigned the method `RetrievalQA.from_chain_type`. \n",
    "\n",
    "Use the following settings inside this method: \n",
    "- `llm=llm`\n",
    "- `chain_type=\"stuff\"`\n",
    "- `retriever=retriever`\n",
    "- `verbose=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9,
    "lastExecutedAt": 1694706178555,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "qa_stuff = RetrievalQA.from_chain_type(\n\nllm=llm,\n\nchain_type=\"stuff\",\n\nretriever=retriever,\n\nverbose=True\n\n)"
   },
   "outputs": [],
   "source": [
    "# Create a new RetrievalQA instance with the specified parameters\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    \n",
    "    # The ChatOpenAI instance to use for generating responses\n",
    "    llm=llm,\n",
    "    \n",
    "    # The type of chain to use for the QA system\n",
    "    chain_type=\"stuff\",\n",
    "    \n",
    "    # The retriever to use for retrieving relevant documents\n",
    "    retriever=retriever,\n",
    "    \n",
    "    # Whether to print verbose output during retrieval and generation\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Create the Queries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the questions to ask the model complete the following steps: \n",
    "- Create a variable call `query` and assigned it a string value of `\"What is this tutorial about?\"`\n",
    "- Create a `response` variable that will store the result of `qa_stuff.run(query)` \n",
    "- Show the `resposnse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This tutorial is about a data pre-processing technique for machine learning called splitting your data. It explains why it is important to split your data set into a training set and a testing set, and when in the machine learning workflow this should be done. The tutorial also provides code examples using the pandas and scikit-learn libraries to demonstrate how to split the data.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"What is this tutorial about?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The training set and test set are two subsets of a dataset that are used in machine learning. \\n\\nThe training set is used to train the machine learning model. It is the portion of the data that the model learns from. The model uses the patterns and relationships in the training set to make predictions or classifications.\\n\\nThe test set, on the other hand, is used to evaluate the performance of the trained model. It is a separate portion of the data that the model has not seen during training. The'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"What is the difference between a training set and test set?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This lesson is suitable for anyone who is interested in data pre-processing techniques for machine learning, specifically the technique of splitting data into training and testing sets. It is particularly relevant for individuals who are new to machine learning and want to understand the importance of splitting data and how to implement it in their workflow.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"Who should watch this lesson?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know the answer to that question as it is subjective and can vary depending on personal opinions and criteria used to determine greatness.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query =\"Who is the greatest football/soccer team on earth?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     },
     "1": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know the answer to that question.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"How long is the circumference of the earth?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
