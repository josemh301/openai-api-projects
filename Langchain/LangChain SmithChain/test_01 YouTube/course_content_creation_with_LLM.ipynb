{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating course content with Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\n",
    "\n",
    "In this project, you'll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understanding the building blocks of working with Multimodal AI projects\n",
    "- Working with some of the fundamental concepts of LangChain  \n",
    "- How to use the Whisper API to transcribe audio to text \n",
    "- How to combine both LangChain and Whisper API to create ask questions of any YouTube video "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need a developer account with [OpenAI ](https://auth0.openai.com/u/signup/identifier?state=hKFo2SAyeTZBU1pzbUNWYWs3Wml5OWVvUVh4enZldC1LYU9PMaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIDFUakNoUGFMLUdNWFpfQkpqdncyZjVDQk9xUTE4U0xDo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q) and a create API Key. The API secret key will be stored in your 'Environment Variables' on the side menu. See the *getting-started.ipynb* notebook for details on setting this up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project requires several packages that need to be installed into Workspace.\n",
    "\n",
    "- `langchain` is a framework for developing generative AI applications.\n",
    "- `yt_dlp` lets you download YouTube videos.\n",
    "- `tiktoken` converts text into tokens.\n",
    "- `docarray` makes it easier to work with multi-model data (in this case mixing audio and text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Run the following code to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10465,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705467366,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install langchain yt_dlp",
    "outputsMetadata": {
     "0": {
      "height": 463,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Install langchain\n",
    "#!pip install langchain==0.0.292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8477,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694802259584,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Install yt_dlp\n!pip install yt_dlp==2023.7.6",
    "outputsMetadata": {
     "0": {
      "height": 447,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Install yt_dlp\n",
    "# !pip install yt_dlp==2023.7.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6113,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705815654,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install tiktoken docarray",
    "outputsMetadata": {
     "0": {
      "height": 447,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install docarray==0.38.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "## Task 0: Select the YouTube video to transcribw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example YouTube tutorial video\n",
    "# youtube_url = input(\"Paste the YouTube URL: \")\n",
    "youtube_url = \"https://www.youtube.com/watch?v=tFXm5ijih98\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import The Required Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we need the `os` and the `yt_dlp` packages to download the YouTube video of your choosing, convert it to an `.mp3` and save the file. We will also be using the `openai` package to make easy calls to the OpenAI models we will use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following packages.\n",
    "\n",
    "- Import `os` \n",
    "- Import `openai`\n",
    "- Import `yt_dlp` with the alias `youtube_dl`\n",
    "- From the `yt_dlp` package, import `DowloadError`\n",
    "- Assign `openai_api_key` to `os.getenv(\"OPENAI_API_KEY\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 176,
    "lastExecutedAt": 1694705470957,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n\nimport os   #import os package \nimport glob\nimport openai #import the openai package \nimport yt_dlp as youtube_dl #import the yt_dlp package as youtube_dl\nfrom yt_dlp import DownloadError #import DownloadError from yt_dlp ",
    "outputsMetadata": {
     "0": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n",
    "\n",
    "# Import the os package \n",
    "import os\n",
    "\n",
    "# Import glob\n",
    "import glob\n",
    "\n",
    "# Import the openai package \n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Import the yt_dlp package as youtube_dl\n",
    "import yt_dlp as youtube_dl\n",
    "\n",
    "# Import DownloadError from yt_dlp \n",
    "from yt_dlp import DownloadError\n",
    "\n",
    "# Import Docarray\n",
    "import docarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also assign the variable `openai_api_key` to the environment variable \"OPEN_AI_KEY\". This will help keep our key secure and remove the need to write it in the code here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10,
    "lastExecutedAt": 1694705474406,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Download the YouTube Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3). \n",
    "\n",
    "We'll download a DataCamp tutorial about machine learning in Python.\n",
    "\n",
    "We will do this by setting a variable to store the `youtube_url` and the `output_dir` that we want the file to be stored. \n",
    "\n",
    "The `yt_dlp` allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you. \n",
    "\n",
    "Lastly, we will create a loop that looks in the `output_dir` to find any .mp3 files. Then we will store those in a list called `audio_files` that will be used later to send each file to the Whisper model for transcription. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following: \n",
    "- Two variables - `youtube_url` to store the Video URL and `output_dir` that will be the directory where the audio files will be saved. \n",
    "- For this tutorial, we can set the `youtube_url` to the following `\"https://www.youtube.com/watch?v=aqzxYofJ_ck\"`and the `output_dir`to `files/audio/`. In the future, you can change these values. \n",
    "- Use the `ydl_config` that is provided to you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 533,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694802393469,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(f\"Downloading video from {youtube_url}\")\n\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\n\n\n",
    "outputsMetadata": {
     "0": {
      "height": 357,
      "type": "stream"
     },
     "1": {
      "height": 137,
      "type": "stream"
     },
     "2": {
      "height": 97,
      "type": "stream"
     },
     "3": {
      "height": 37,
      "type": "stream"
     },
     "4": {
      "height": 257,
      "type": "stream"
     },
     "5": {
      "height": 77,
      "type": "stream"
     },
     "6": {
      "height": 57,
      "type": "stream"
     },
     "7": {
      "height": 57,
      "type": "stream"
     },
     "8": {
      "height": 97,
      "type": "stream"
     },
     "9": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Encodings: locale cp1252, fs utf-8, pref cp1252, out UTF-8 (No VT), error UTF-8 (No VT), screen UTF-8 (No VT)\n",
      "[debug] yt-dlp version stable@2023.07.06 [b532a3481] (pip) API\n",
      "[debug] params: {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'outtmpl': 'files/audio/%(title)s.%(ext)s', 'verbose': True, 'compat_opts': set()}\n",
      "[debug] Python 3.8.3 (CPython AMD64 64bit) - Windows-10-10.0.19041-SP0 (OpenSSL 1.1.1g  21 Apr 2020)\n",
      "[debug] exe versions: ffmpeg 2023-01-16-git-01f46f18db-full_build-www.gyan.dev (setts), ffprobe 2023-01-16-git-01f46f18db-full_build-www.gyan.dev\n",
      "[debug] Optional libraries: Cryptodome-3.19.1, brotli-None, certifi-2022.12.07, mutagen-1.47.0, sqlite3-2.6.0, websockets-12.0\n",
      "[debug] Proxy map: {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video from https://www.youtube.com/watch?v=tFXm5ijih98.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Loaded 1855 extractors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=tFXm5ijih98\n",
      "[youtube] tFXm5ijih98: Downloading webpage\n",
      "[youtube] tFXm5ijih98: Downloading ios player API JSON\n",
      "[youtube] tFXm5ijih98: Downloading android player API JSON\n",
      "[youtube] tFXm5ijih98: Downloading m3u8 information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec:vp9.2, channels, acodec, lang, proto\n",
      "[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec:vp9.2(10), channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] tFXm5ijih98: Downloading 1 format(s): 251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Invoking http downloader on \"https://rr4---sn-8vq54voxn25po-mgte.googlevideo.com/videoplayback?expire=1706975080&ei=CAu-ZcuTCdnMp-oPmoOFUA&ip=84.125.157.205&id=o-ANJe-MRUDKu3VxcAf6v0_yaaJNTmLxmNcevCnAoNkDfT&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=ho&mm=31%2C29&mn=sn-8vq54voxn25po-mgte%2Csn-h5qzen7d&ms=au%2Crdu&mv=m&mvi=4&pcm2cms=yes&pl=21&initcwndbps=2563750&vprv=1&svpuc=1&mime=audio%2Fwebm&gir=yes&clen=31439839&dur=2169.881&lmt=1692885148814198&mt=1706953029&fvip=4&keepalive=yes&fexp=24007246&c=ANDROID&txp=6308224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cvprv%2Csvpuc%2Cmime%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIhAPm6meqFfDhe_DJs2oDQ-evh6If-ClcaMnXnfHoYnf8QAiBXhFZr3yNJaYleomH1bb9Sj1snWTI4O5_R7fRdE-JDfA%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpcm2cms%2Cpl%2Cinitcwndbps&lsig=AAO5W4owRAIgXEJDWr3HhEyI70Y4DIlCoaY-9IMtgvDLZ3PQDtwS5OsCIFiQDzhm_aeiGWyX-ZmwZf-3eRhEbuZ41z5DuyyEHFeH\"\n",
      "[debug] File locking is not supported. Proceeding without locking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Destination: files\\audio\\LangSmith Tutorial - LLM Evaluation for Beginners.webm\n",
      "[download] 100% of   29.98MiB in 00:00:25 at 1.20MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffprobe -show_streams \"file:files\\audio\\LangSmith Tutorial - LLM Evaluation for Beginners.webm\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExtractAudio] Destination: files\\audio\\LangSmith Tutorial - LLM Evaluation for Beginners.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffmpeg -y -loglevel \"repeat+info\" -i \"file:files\\audio\\LangSmith Tutorial - LLM Evaluation for Beginners.webm\" -vn -acodec libmp3lame \"-b:a\" 192.0k -movflags \"+faststart\" \"file:files\\audio\\LangSmith Tutorial - LLM Evaluation for Beginners.mp3\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file files\\audio\\LangSmith Tutorial - LLM Evaluation for Beginners.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "# Directory to store the downloaded video\n",
    "output_dir = \"files/audio/\"\n",
    "\n",
    "# Config for youtube-dl\n",
    "ydl_config = {\n",
    "    \"format\": \"bestaudio/best\",\n",
    "    \"postprocessors\": [\n",
    "        {\n",
    "            \"key\": \"FFmpegExtractAudio\",\n",
    "            \"preferredcodec\": \"mp3\",\n",
    "            \"preferredquality\": \"192\",\n",
    "        }\n",
    "    ],\n",
    "    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "# Check if the output directory exists, if not create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# Print a message indicating which video is being downloaded\n",
    "print(f\"Downloading video from {youtube_url}.\")\n",
    "\n",
    "# Attempt to download the video using the specified configuration\n",
    "# If a DownloadError occurs, attempt to download the video again\n",
    "try:\n",
    "    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "except DownloadError as DE:\n",
    "    print(f\"An error occured: {DE}\")\n",
    "    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([youtube_url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the audio files that we will use the `glob` module that looks in the `output_dir` to find any .mp3 files. Then we will append the file to a list called `audio_files`. This will be used later to send each file to the Whisper model for transcription. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following: \n",
    "- A variable called `audio_files`that uses the glob module to find all matching files with the `.mp3` file extension \n",
    "- Select the first first file in the list and assign it to `audio_filename`\n",
    "- To verify the filename, print `audio_filename` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1694705587367,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Find the audio file in the output directory\n\n# Define function parameters\noutput_dir = \"files/audio/\"\n\n# Find the audio file in the output directory\naudio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\naudio_filename = audio_files[0]\nprint(audio_filename)",
    "outputsMetadata": {
     "0": {
      "height": 56,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/audio\\LangSmith Tutorial - LLM Evaluation for Beginners.mp3\n"
     ]
    }
   ],
   "source": [
    "# Find the audio file in the output directory\n",
    "\n",
    "# Find all the audio files in the output directory\n",
    "audio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n",
    "\n",
    "# Select the first audio file in the list\n",
    "audio_filename = audio_files[0]\n",
    "\n",
    "# Print the name of the selected audio file\n",
    "print(audio_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Transcribe the Video using Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the `audio_file`, for the `output_file` and the model. \n",
    "\n",
    "Using these variables we will:\n",
    "- create a list to store the transcripts\n",
    "- Read the Audio File \n",
    "- Send the file to the Whisper Model using the OpenAI package "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this step, create the following: \n",
    "- A variable named `audio_file`that is assigned the `audio_filename` we created in the last step\n",
    "- A variable named `output_file` that is assigned the value `\"files/transcripts/transcript.txt\"`\n",
    "- A variable named `model` that is assigned the value  `\"whisper-1\"`\n",
    "- An empty list called `transcripts`\n",
    "- A variable named `audio` that uses the `open` method and `\"rb\"` modifier on the `audio_file`\n",
    "- A variable to store the `response` from the `openai.Audio.transcribe` method that takes in the `model`and `audio` variables \n",
    "- Append the `response[\"text\"]`to the `transcripts` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been split into 6 parts.\n"
     ]
    }
   ],
   "source": [
    "#split the audio file\n",
    "# !pip install pydub\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Load the MP3 file\n",
    "mp3_file = audio_file\n",
    "audio = AudioSegment.from_mp3(mp3_file)\n",
    "\n",
    "# Length of the audio in milliseconds and calculate the duration of each part\n",
    "length = len(audio)\n",
    "part_duration = length // 6\n",
    "\n",
    "# Split the audio into 4 parts and save them\n",
    "for i in range(6):\n",
    "    start = i * part_duration\n",
    "    end = start + part_duration\n",
    "    part = audio[start:end]\n",
    "    part_name = f\"part_{i+1}.mp3\"\n",
    "    part.export(part_name, format=\"mp3\")\n",
    "\n",
    "print(\"The file has been split into 6 parts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_segments = [\"part_1.mp3\", \"part_2.mp3\", \"part_3.mp3\", \"part_4.mp3\", \"part_5.mp3\", \"part_6.mp3\"]\n",
    "transcripts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:   0%|                                                                    | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_1.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:  17%|██████████                                                  | 1/6 [00:21<01:45, 21.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_2.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:  33%|████████████████████                                        | 2/6 [00:40<01:20, 20.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_3.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:  50%|██████████████████████████████                              | 3/6 [01:00<01:00, 20.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_4.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:  67%|████████████████████████████████████████                    | 4/6 [01:20<00:39, 20.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_5.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:  83%|██████████████████████████████████████████████████          | 5/6 [01:41<00:20, 20.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_6.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting audio files: 100%|████████████████████████████████████████████████████████████| 6/6 [01:59<00:00, 19.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "for audio_segment in tqdm(audio_segments, desc=\"Converting audio files\"):\n",
    "    print(f\"Converting {audio_segment} to text...\\n\")\n",
    "    with open(audio_segment, \"rb\") as file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\", \n",
    "            file=file\n",
    "        )\n",
    "        transcripts.append(transcript)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Transcription(text=\"Now, if you are building applications with large language models, then LangSmith is definitely a platform you can not ignore. So in this video, I'm going to introduce you to LangSmith. And now LangSmith is a platform for building production grade large language models applications. Now, in today's tutorial, we will be going over what LangSmith is really starting from scratch at the beginner level. And then we will be going over some concepts like data sets and evaluation to really understand how to use this platform. Now, as always, I will be walking you through every step of the way within VS Code, showing all the code examples, showing you how to do this. And also all of the code will be made available on the GitHub repository that I will link in the description. And now, quick note, LangSmith is currently still in private beta. So if you don't have access yet already, make sure to sign up and get on that wait list. And now, for those of you that are new here, my name is Dave Abelard and I run a company called Data Lumina, which is a data and artificial intelligence consulting business. And we also have two training programs. One is called Data Alchemy, completely free to join to learn the technical side of artificial intelligence and data science. And the other one is a paid coaching program for data professionals that want to learn how to monetize their data and AI skills similar to how I am doing that. So what is LangSmith? Well, it's a platform to debug, test, evaluate and monitor your large language model chains and outputs. Now, why would you do something like that? For example, if we have a question and answer pair that we want to do a search over, for example, in an application, then we want, of course, if you put this into production, have some way to evaluate whether the application is coming up with the right answers. LangSmith is the platform for that. So what can you do with it? You can log runs, meaning every time you interact with a large language model, you can leave a trace that you can look into. You can organize your work, visualize your runs. And this is what it looks like if you actually go into the application, which I have access to and I will show you what that looks like. But as you can see, it's very visually oriented. So you can exactly see the chains and each of the steps and then see the inputs and the outputs. So there's also an opportunity to run prompts in the playground to debug and tweak some prompt templates, inputs that you might be working with. You can share your work and you can create datasets for testing and evaluation. And this is what we will be mainly looking into in today's tutorial, because I believe this really is the key strength of LangSmith right now. And it's not something you can do easily with other platforms or frameworks out there. Now, this really is going to be a hands on tutorial, really showing you how the platform actually works. Now, if you want a bit of a more high level overview of what LangSmith can do, then I would refer you to this previous video that I already made. And then now some quick context of where this fits into the whole pipeline of building your large language model applications. And it's really about putting them into production, putting them to the next level. So you build something locally, you evaluated it, and now it's ready for actual customers, actual people to use these applications. And in my work with DataLumina, I've been working on chatbots for various clients. And what you typically see if you get really to like the next level is you want to introduce some institutional knowledge, some company data that you can use to base your answers upon. Because if you're just interacting with chat GPT, with open AI, the answers will be very generic. So typically what you do is you upload frequently asked questions, documents, and you want to retrieve the information from that through vector stores and similarity searches, the whole thing. If you want to learn more about that, check out my other tutorials. But the thing is you want a way to evaluate that as you are scaling your vector stores, as you are adding more data to them, as you are integrating more logic into your application, that the answers still remain valid. So that's why this is important. And this is also why this is like the next logical step in the realm of LLM applications. And it's also nothing new in the sense that with regular machine learning, this is basically already very common. When you put a model into production, you can, for example, use something like MLflow to like track all your experiments and monitor your drift, model drift over time to really make sure that the models you put in place as you introduce new training data or new unseen data, that the results remain accurate. And by the way, MLflow is also putting features in place to track large language model outputs. So this could be an interesting comparison for the future. So now let's get into the actual code. And in order to get started, we first have to set up four environment variables, which I will be loading from static variables as well as a .env file. So the only thing that I'm getting from the environment variable file is the Langsmith API key, which you can get by going to the platform and then in the lower left corner, you can check out the API keys and export or get that, save that. And that should be the setup. So I'm now going to run and start all of this up in an interactive session. And we are going to now first log our first run to show you what this looks like. So we're going to set up a client, which we import from Langsmith. And then it's as easy as interacting with Langchain like you normally would. So we have a large language model that we specify and we say LLM predict and we put in hello world. So this is all with the default settings. We just create a simple one shot prediction and we get a response back. Hello, how can I assist you today? But now the cool thing is, if we come over to the Langsmith platform and select the project that we are running this under.\"), Transcription(text=\"tutorial, you can see that the runs are now showing up over here. So this is the one we just did and here we can see the human input, hello world and here the AI output. So now we have an actual trace of this start time, end time, status, tokens, latency. So you can see all kinds of metadata that is very interesting to log if you want to monitor this application. Also you can see what model we were using, what version, what system we ran this on and here you can also see how the chat open AI was constructed. So very interesting stuff and let's see if I now come back to the runs, you can also see like how fast this is in terms of like let's say we run this one more time, let's say what can you do. Another prompt, what can you do, wait for this and then we have the result as an AI and I can perform etc etc. Now let's go over here, boom, we have the other one. You can see this one took pretty long, so this is already interesting, like why was this taking 10 seconds. So that is how you get started with Langsmith, pretty easy right. So I love how easy and convenient they made this. All you have to do basically is specify the project that you want to run this under. Now another cool thing is that you can use various organizations within Langsmith. So you can do this from the UI and you can add members to different organizations as well and then you can have projects at the organization level and get an API key for every organization for example. So if you're working with various clients and you want to keep all of that separate, you can do that which is ideal for the projects that I'm working on with multiple clients. Alright, so now let's check out how evaluation works which I believe right now is that most important feature of Langsmith and we're first going to cover the quick start so to quickly get started and then we'll dive into more details into how data sets actually work, how you can create them and then also the various evaluators that they have in place. But for now let's continue with the code example and for this we're going to create a quick data set. So we'll create some example inputs and we also give the data set a name. So we call it rep battle data set and now we can register this data set by running the code over here. So we have client and then we create a data set we give it a name and also a description and now we can create this data set by running this piece of code and then coming back to our projects and then to data sets and we can now see that we have the rep battle data set which currently has no examples yet. So let's put in some examples and that we can do over here so we loop over our list that we've just created which is just four entries in here and you can see it's just a single input so it's not a key value pair like I've said we will be diving into that in a bit but for now we're just adding some examples. So we'll loop over it and we say the input is a question and then we put in the input prompt we have no outputs and we put in the data set idea. So now let's come back over here and here you can see we have an input and we don't have any outputs so just inputs. Alright let's come back to the next step so now we're going to quickly evaluate this data set. So how this works this evaluation is we're first going to configure a run eval config and now this is a bit tricky in the beginning at least I found it quite tricky but you can look up more information on how this works on the documents in the lang chain evaluators but I will walk you through it and now they start out in the documentation with the QA evaluation so if you have question and answer pair key and value but right now for the simple start like I've explained we just have single inputs so this could be when you are running your application in production and users just ask a question and there's no correct or there's no right or wrong answer but you still want to do some evaluation. For that they have come up with some clever ways to do that and let me scroll down a bit no labels criteria evaluation so to configure the run eval config we can put in criteria so that is exactly what we're doing over here so we're putting in criteria as the evaluator and now here we can do a couple of things so how do we evaluate outputs if we don't have a right or wrong answer meaning we just have the inputs well we can evaluate them based on a certain set of criterias and now out of the box they have put in place some let's see some criteria that we can already use so these are things like conciseness, relevance, correctness, coherence, harmfulness etc so these you can use straight out of the box so how does it work you select a criteria and plug in any of the strings that you see within this list over here but you can also specify a custom criteria and in the example they give this one of cliche so since we're creating rap battles we are going to evaluate whether the lyrics are cliche so how do we do that it's a dictionary so we have a criteria cliche and you respond with the following so that is the structure of how you set that up and now what we're going to do is we're going to run all of this so first we configure the eval config and then we do run on data set which again has the client that we previously defined, dataset name, large language model and the eval config so again they made some pretty nice wrappers to do all of this and now as you'll find out so this is now going to run in the background and this will take some time because it's going to loop over all the input\"), Transcription(text=\"inputs and all the criteria and then within that multiple large language model prompts to do the evaluation. So this now finished. Let's come back to the data set and click on the last run that we just did. And now we get this nice dashboard. So for all of our examples within the data set, we have a run over here and we can click on that. So let's check out the one with Barbie and Oppenheimer, so the rap battle. So first of all, top level, we have the input and we get the output. So this is the whole rap battle with some verses. Okay, so this is top level what we can see. But now we also get feedback and this is where it gets interesting. So now for all of the criteria that we've defined, there is an additional run that we can look into. Now why do we have four? Well if we use criteria, it says the default criterion is helpfulness and then we've added harmfulness, misogyny and the custom cliche one. So four in total. So let's now have a quick look at what's going on. So first helpfulness. We can drill down into this and then here what you can see is you enter into the middle of this tree basically where you can see the input and the output and then also the final output, the verdict basically of the assessment of the evaluation. So let's see what's going on over here. So this is just the input and the output that we've already seen. But if we look at what's going on over here, here you can see how the large language model is actually assessing and evaluating whether this is helpfulness. So based on the criteria helpfulness, this is kind of like the prompt instruction that LangChain already put into place in LangSmith to do the assessment. So you can see helpfulness, insightfulness and appropriateness. And then it concludes with a yes, so with a why. So based on the criteria, it can be considered helpful, insightful and appropriate. But now what's even more interesting is if we drill down to this bottom layer, here we can actually look at the prompt engineering itself. So you are assessing a submitted answer or a given task or input based on a set of criteria. So we have the data, we have input and we have the submission. And then if we scroll down, here we have the criteria helpfulness. Is this submission helpful, insightful and appropriate? If so, response why? If not, respond and. And then we have the end data. So here you can see how the prompt was actually engineered and how it was sent to the large language model to get to the final conclusion. So I know it's quite complex, takes some time to cover all of this, but I think it's really important to understand how this works top level. Because if we now look at the other, let's see, if we look at the other ones going on over here, so we have the feedback. We have the same for harmfulness, misogyny and cliche, but just in a different way. So we can go over here and then again, we have the tree structure over here where we can go all the way to the bottom where we can now see. Let's see how it was structured. So again, begin data input submission, and then we have the criteria that we created. So this is the custom criteria. So here you can also see how this relates to the input structure over here, which is kind of like, it's a dictionary, but it is split up in such a way where it's just one key and one value, even though it's split up. So you can see it's cliche and then other lyrics, cliche, respond, why, if they are, and if they are entirely unique. So this also shows you how you can create custom criteria pretty easily based on your data and your use case that you're working with. And then also all the way trace back how then Langsmith is using your custom criteria to evaluate this. And then finally, if we look at the top level of this tree, if this trace structure, we get all the top level metadata and get to the final output that is also locked here within the UI. So we get the feedback, we have a key score and value, and we also have a comment on that. So now if we go back to our dashboard, we can start to understand how they come up with this feedback over here. So you can see we have four total runs in here. You can also see the tokens and the time it took. And now here you can see you get a score and these are averages. So if we look at, let's see, let's open this up a little bit. You can see over here, so cliche, we have three zeros and one is identified as cliche. So then it just averages that into a 0.25 for cliche score, where one would be all of the outputs would be cliche. And now it's just one of the four. Then we have misogyny, which is at zero, nothing in there. Also no harmful content in here, which could be very interesting to monitor your business application for harmful responses. And we also have helpfulness, which has a score of one, a hundred percent score, meaning that all of the prompts were actually helpful. So I think it's quite clever in how they've set this up and you kind of have to get used to it and really identify criteria that are relevant for your application in order to really make this work. But this is a very good starting point to start your evaluation if you don't have key value pairs. So you don't know exactly what the output should be for your application. All right. So that was the quick start on how to set this up and how to run your first evaluation. But now let's look into the data sets where it.\"), Transcription(text=\"that's a little more interesting when we start to look at actual key value pairs. So inputs and outputs, questions and answers, where a lot of the use cases, especially if you're looking at information retrieval, kind of like chatbots with your own data, enterprise data, this is really where it gets interesting. So top level in Langsmith, there are three data types right now. So we have simple key value pairs, which we will look into as it's the most straightforward. You also have large language model datasets, which is a combination of input dictionaries and output dictionaries. And we also have it for chat, where it's kind of like the same, but then you have a series of inputs and outputs back and forth like in a chat. But like I've said for now, let's go over the key value pairs. And I will be going over these examples a little quickly because they're pretty straightforward in the sense that they all were kind of the same, but I'm just gonna show you a couple of ways you can go about this. So first of all, you can either do it using code, like we're doing right here, which is probably the best way to go about it, but you can also do it from the UI. So let me come back. Where were we? You can also come to datasets and create a new test run or upload a CSV over here. So that also works. But I'm going to show you first how we can get some example inputs. And now we're going to use key values. So we have a question, what's the largest mammal? And we have an answer, the blue whale and so on. So these are now our inputs. Now we can give this a name and similar to how we uploaded the initial dataset, we are going to do the same for this and create it. We have a description and then let's just run this. So this goes pretty quickly. We can come to the UI. Boom, we have another dataset. And now look here, we just had, let's see examples. Just inputs. And now we have inputs and outputs. And now here you can also see, since we have not specified what type of database we wanted to create, it defaulted to key value pair. All right, so now the next thing, we can also create dataset from existing runs. So let's create a quick dataset, again, example dataset. Come over here. Now we have an empty one. And now what we can do is we can get all of the runs for a given project. So we have the Langsmith tutorial, for example. So if we go to all of our projects, all the evaluations will be put by default under the evaluators project, but there are a lot of runs in here. So let's actually look at this one that we ran in the beginning to do like the hello world example. So we can take that and take all of the runs and just put it into the database. Now there's probably, so there's already an example. Ah, I see. So this is an error that you get if you try to add duplicate entries to datasets. So if I come to the example dataset, here you can see it was working well, but it just encountered, since we ran the hello world example quite a couple of times with the exact same output, you can only have unique inputs, outputs in here, which makes sense. So that is how you create a dataset from an existing run. And now we're getting all of the runs under a project, but you could probably be a little bit more clever about this and strategically extract the relevant pieces of information, but I haven't looked into that yet. Now we can do the same thing from a data frame. So this is kind of similar to a key value pair, but then we use the upload data frame. So let's just create this one and then let's see what it looks like. So we now have the questions and answers in a data frame and we can upload that as well. So let's do that. Let's see, it was my data frame dataset and go back over here, boom, another input output. So those are, oh wait, we have one more CSV upload, which is also very interesting. So here we can specify or link to a path in our directory. So here you can see in the data, we have our dataset.csv, which is again, question and answer. Same stuff, but now let's do it from a CSV and let's come back over here, come to the datasets and we also have a CSV dataset. So those are various ways that you can upload data to the Langsmith portal. All right, so now that we have actual key value pairs within our dataset, we can look into the QA evaluation. And there are three metrics or three functions that we can use for that. And that is the context QA, the QA in a chain of thought QA and the chain of thought QA is similar to the QA, but it just adds additional chain of thought reasoning. If you don't know what that is, it's a series of intermediate reasoning steps. It has been shown to improve the performance, the output of large language models. And they also say in the docs over here that this will get you the best results, but also takes longer, cost more tokens because of intermediate steps. QA, we'll look into all of these three, but basically the QA is simply looking at the input and the output to decide whether it's correct or not. A chain of thought does that, but then with more steps and context QA looks at the context. So they say this is useful if you have a larger corpus of grounding docs, but not necessarily have a ground through to answer the query. So let's see what that looks like and come over to the code over here. And now you can see that we can configure the run eval config again, similar to we did before, but now instead of putting in a criteria, we can put in these parameters over here and then we can hit run on data set again, fill in all of the information.\"), Transcription(text=\"So let's run this and see what we get. All right, so the evaluation is finished. Now let's come back to the data sets over here. So we, what we had, we had the elementary animal questions and here we can see the test runs. So this is the one we just did. And here we can see total time was 14 seconds to evaluate this. And here you can see that we get perfect scores, correctness, contextual accuracy, and also chain of thoughts, contextual accuracy. But now let's look into these individual, into all of the methods that we've used to evaluate them and actually look at the prompt underneath that. So let's start with the largest mammal, the whale. So top level, we have what is the largest mammal and then we get the largest mammal is the blue whale, et cetera, and it gives some more context. And then we have the reference output, which is the blue whale. So this was in the data, this is the data set, but this was the output. And now you can see how sometimes it can be quite tricky to evaluate whether an output is correct because it is based on a partial match of the overall output. So let's look at how this is taken care of within the evaluation model. So let's start with contextual accuracy. Let's look at this one. And here you can see what's going on. So we have a query, we have a result, and we have context. And then it is instructed to say, or the output is correct. So then let's look at the actual prompt. And okay, so here you can see the prompt engineering again. You are a teacher grading a quiz. You are giving a question. Okay, this is very interesting. And if question, context, student answer, and then we grade it, correct or incorrect. So this is pretty interesting to see. And they really instructed the AI to be a teacher. So they've probably experimented with this. And instead of saying like, hey, this is an evaluation tool to score large language model outputs, et cetera, they made it really simple and understandable in a way for humans as well, but probably also for large language models. So this is pretty interesting. So then we have question, context, and then we grade it. Interesting. Okay, so that's the first one. Now let's come back and now let's quickly compare all of them so we kind of get an understanding what the difference is. So first we have the context Q&A. So here you can see the prompt that went into it. And I'm not gonna go over all of this. If you're interested, you can read over this. And here we have the QA eval chain. So you can see what's going on over here. And then these are actually quite similar. It's just a different way of prompting. And then in the end, we have the chain of thought reasoning where there is a larger prompt. There's more to it, basically. So those are the differences, but they all, if we look like in the middle one over here, they all have the output, correct, correct. And then there's a little bit more information over here, but we get the grade correct. Now, and then coming back to top level, your data set, this is really where you can start to monitor everything. So you can have a look at the overall scores on all the metrics, all the evaluations that you've put into place. And now when you change something to your application, either like in the actual application itself through like prompt engineering or adding additional logic, or for example, in your factor database. So you're putting more examples into it. It becomes more and more important to quickly assess, okay, does it still pass the test? Are the answers that were previously correct, are they still correct? That's really where this comes into play and where this is so useful. And now you can also start a test run straight from the UI over here. So you can also see how you can have a, you can select all of the criteria. You can even do multiple and it generates the code for you already. And let's see, I don't think you can run it from the UI, but it generates the code. So you can just copy paste this and then run it within Python and you can update this. So this is all very powerful. And now within the code, I have some more examples on how you can create these evaluations with custom criterias, with labels and also without labels, like we've already seen. And then the final thing that I quickly wanna get into, which is another interesting evaluation, are two interesting evaluation metrics. We can use an embedding distance and a string distance. And I won't go into great detail on what these exactly are, but if you scroll down, you can see the embedding distance, how we can do that, and also the string distance. And if I come over here to the GitHub repository, there's also some more information on that where you can see the, we can use the cosine distance for the embeddings where we have zero to one. And we can also evaluate the string distance where it's the other way around. So zero is an exact match and one is no similarity. So those are also evaluation metrics that you can look into. And they put these into place to counter the fact that the evaluation scores that we were previously discussing don't have a direction, meaning like higher is not necessarily better. We just say, hey, if it's helpful, we have a one. But if we have the criteria of malicious, then we also get a one. And it's hard to compare that from like a modeling perspective. So the embedding.\"), Transcription(text=\"Distance and String Distance are ways to quantify the similarity, where you can monitor small changes over time. So for example, if you have two answers which are both helpful, both have a 1 on helpful, they cannot really be compared, right? Whereas if you use Embedding Distance, for example, or String Difference, you can have two answers which are both helpful and correct, but one might be better than the other, because it's closer to the actual answer, if that makes sense. So let's quickly have a look at what that looks like. So we first have the Embedding Distance, which uses the cosine, here you can see 0 to 1, one is more similar. So let's run that on the elementary animal questions, let's wait for that, and then also run the String Distance after that, where it's the other way around. 0 is exact match, and 1 is no similarity. Alright so that one is also finished right now, let's now get back to the runs, and here you can see we have the Embedding Distance, and now the other one should also come up in here, I believe. So we have the Embedding Distance, and let's see, here you can see the correctness, and again coming back to that, what we got going on, it's a range between 0 and 1. So this is quite interesting to see how this can be problematic in some way, where the answer is correct, we have the blue whale, but if we compare the reference output to what was set over here, we get a very low score, and that is because there's just a lot more information in here, but the answer is still correct. But you probably, if you really want to build this out, you have to combine both. So at some point you want to do some distance metrics, or string metrics that you want to put into place, and at other times you just want to evaluate, is it correct, yes or no. Now let's come back and see the other one, okay so that one also uploaded, so kind of like similar, but just another way of looking at it. We also have, so here it's a little higher, but then again remember that it is flipped over here, so 0 would be an exact match, and 1 is no similarity at all. Alright now, what have been some of my observations so far using Langsmith? Well first of all, the logging function provides a really transparent and structured way to examine the outputs of large language models, so it's excellent for that, great tool. Logging dataset allows different evaluations to be made independently on different runs, so I also like that, so that you have the ability to create different datasets, different projects, different organizations, so you can really customize this towards your need, your application, your organization. And then also, it's very useful to evaluate and compare LMA outputs with ground truths, and use both the existing and the customized evaluation criteria. Like I've said, we have quantifiable metrics, but the customizations also are really beneficial at some point. And then finally, the customized evaluations are really helpful, but they can take up a lot of tokens, so if we compare that, for example, let's go back to the first experiments that we were running on the rap battle dataset, so here you can see that we had 2200 tokens, and if we go back to the elementary animal questions where we were just running the correctness, for example, on a similar sized dataset, we can see that it's a lot less. And if we look at the cost, so this is also something to consider, so these are, we can look at the cost for today, so it's $1.30, just from today's experiments, but you can see we just run a couple of experiments with a dataset with like four records in it. So you really got to be mindful of that and closely monitor your cost as you start to run these evaluations, because I can really see how if you really scale this up to hundreds of thousands, potentially millions of records, that running an evaluation is going to be expensive. So you got to then really be careful about when to run that and also make sure you create good subsets of the data potentially to not like run it over everything. So that is just something to keep in mind and something we all have to like learn and experience how this actually works, putting large language models applications into production and monitoring costs and performance over time. All right, and that's it for this Langsmith tutorial. So I now hope that you have a solid understanding of how the platform works and also why it's actually useful and I would say even crucial if you're building applications with large language models. So if you found this video helpful, then please make sure to like this video and also subscribe to the channel. So go do that now. And then if you want to learn more, if you want to stay in the loop, then make sure to check out Data Alchemy, completely free, link is in the description. And in here I share my entire data and AI workflow that I use to not only create these videos but also complete the projects that I work on for my clients. So that's that. Make sure to check that out. And also if you want to learn more about what I do with DataLumina, you can check that out. Also look at the freelancer mastermind that we have if you're interested in that. Then that's it for now. I want to thank you all for watching and then I'll see you in the next video.\")]\n"
     ]
    }
   ],
   "source": [
    "print(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(transcripts[1].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_transcripts = \" \".join([t.text for t in transcripts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, if you are building applications with large language models, then LangSmith is definitely a platform you can not ignore. So in this video, I'm going to introduce you to LangSmith. And now LangSmith is a platform for building production grade large language models applications. Now, in today's tutorial, we will be going over what LangSmith is really starting from scratch at the beginner level. And then we will be going over some concepts like data sets and evaluation to really understand how to use this platform. Now, as always, I will be walking you through every step of the way within VS Code, showing all the code examples, showing you how to do this. And also all of the code will be made available on the GitHub repository that I will link in the description. And now, quick note, LangSmith is currently still in private beta. So if you don't have access yet already, make sure to sign up and get on that wait list. And now, for those of you that are new here, my name is Dave Abelard and I run a company called Data Lumina, which is a data and artificial intelligence consulting business. And we also have two training programs. One is called Data Alchemy, completely free to join to learn the technical side of artificial intelligence and data science. And the other one is a paid coaching program for data professionals that want to learn how to monetize their data and AI skills similar to how I am doing that. So what is LangSmith? Well, it's a platform to debug, test, evaluate and monitor your large language model chains and outputs. Now, why would you do something like that? For example, if we have a question and answer pair that we want to do a search over, for example, in an application, then we want, of course, if you put this into production, have some way to evaluate whether the application is coming up with the right answers. LangSmith is the platform for that. So what can you do with it? You can log runs, meaning every time you interact with a large language model, you can leave a trace that you can look into. You can organize your work, visualize your runs. And this is what it looks like if you actually go into the application, which I have access to and I will show you what that looks like. But as you can see, it's very visually oriented. So you can exactly see the chains and each of the steps and then see the inputs and the outputs. So there's also an opportunity to run prompts in the playground to debug and tweak some prompt templates, inputs that you might be working with. You can share your work and you can create datasets for testing and evaluation. And this is what we will be mainly looking into in today's tutorial, because I believe this really is the key strength of LangSmith right now. And it's not something you can do easily with other platforms or frameworks out there. Now, this really is going to be a hands on tutorial, really showing you how the platform actually works. Now, if you want a bit of a more high level overview of what LangSmith can do, then I would refer you to this previous video that I already made. And then now some quick context of where this fits into the whole pipeline of building your large language model applications. And it's really about putting them into production, putting them to the next level. So you build something locally, you evaluated it, and now it's ready for actual customers, actual people to use these applications. And in my work with DataLumina, I've been working on chatbots for various clients. And what you typically see if you get really to like the next level is you want to introduce some institutional knowledge, some company data that you can use to base your answers upon. Because if you're just interacting with chat GPT, with open AI, the answers will be very generic. So typically what you do is you upload frequently asked questions, documents, and you want to retrieve the information from that through vector stores and similarity searches, the whole thing. If you want to learn more about that, check out my other tutorials. But the thing is you want a way to evaluate that as you are scaling your vector stores, as you are adding more data to them, as you are integrating more logic into your application, that the answers still remain valid. So that's why this is important. And this is also why this is like the next logical step in the realm of LLM applications. And it's also nothing new in the sense that with regular machine learning, this is basically already very common. When you put a model into production, you can, for example, use something like MLflow to like track all your experiments and monitor your drift, model drift over time to really make sure that the models you put in place as you introduce new training data or new unseen data, that the results remain accurate. And by the way, MLflow is also putting features in place to track large language model outputs. So this could be an interesting comparison for the future. So now let's get into the actual code. And in order to get started, we first have to set up four environment variables, which I will be loading from static variables as well as a .env file. So the only thing that I'm getting from the environment variable file is the Langsmith API key, which you can get by going to the platform and then in the lower left corner, you can check out the API keys and export or get that, save that. And that should be the setup. So I'm now going to run and start all of this up in an interactive session. And we are going to now first log our first run to show you what this looks like. So we're going to set up a client, which we import from Langsmith. And then it's as easy as interacting with Langchain like you normally would. So we have a large language model that we specify and we say LLM predict and we put in hello world. So this is all with the default settings. We just create a simple one shot prediction and we get a response back. Hello, how can I assist you today? But now the cool thing is, if we come over to the Langsmith platform and select the project that we are running this under. tutorial, you can see that the runs are now showing up over here. So this is the one we just did and here we can see the human input, hello world and here the AI output. So now we have an actual trace of this start time, end time, status, tokens, latency. So you can see all kinds of metadata that is very interesting to log if you want to monitor this application. Also you can see what model we were using, what version, what system we ran this on and here you can also see how the chat open AI was constructed. So very interesting stuff and let's see if I now come back to the runs, you can also see like how fast this is in terms of like let's say we run this one more time, let's say what can you do. Another prompt, what can you do, wait for this and then we have the result as an AI and I can perform etc etc. Now let's go over here, boom, we have the other one. You can see this one took pretty long, so this is already interesting, like why was this taking 10 seconds. So that is how you get started with Langsmith, pretty easy right. So I love how easy and convenient they made this. All you have to do basically is specify the project that you want to run this under. Now another cool thing is that you can use various organizations within Langsmith. So you can do this from the UI and you can add members to different organizations as well and then you can have projects at the organization level and get an API key for every organization for example. So if you're working with various clients and you want to keep all of that separate, you can do that which is ideal for the projects that I'm working on with multiple clients. Alright, so now let's check out how evaluation works which I believe right now is that most important feature of Langsmith and we're first going to cover the quick start so to quickly get started and then we'll dive into more details into how data sets actually work, how you can create them and then also the various evaluators that they have in place. But for now let's continue with the code example and for this we're going to create a quick data set. So we'll create some example inputs and we also give the data set a name. So we call it rep battle data set and now we can register this data set by running the code over here. So we have client and then we create a data set we give it a name and also a description and now we can create this data set by running this piece of code and then coming back to our projects and then to data sets and we can now see that we have the rep battle data set which currently has no examples yet. So let's put in some examples and that we can do over here so we loop over our list that we've just created which is just four entries in here and you can see it's just a single input so it's not a key value pair like I've said we will be diving into that in a bit but for now we're just adding some examples. So we'll loop over it and we say the input is a question and then we put in the input prompt we have no outputs and we put in the data set idea. So now let's come back over here and here you can see we have an input and we don't have any outputs so just inputs. Alright let's come back to the next step so now we're going to quickly evaluate this data set. So how this works this evaluation is we're first going to configure a run eval config and now this is a bit tricky in the beginning at least I found it quite tricky but you can look up more information on how this works on the documents in the lang chain evaluators but I will walk you through it and now they start out in the documentation with the QA evaluation so if you have question and answer pair key and value but right now for the simple start like I've explained we just have single inputs so this could be when you are running your application in production and users just ask a question and there's no correct or there's no right or wrong answer but you still want to do some evaluation. For that they have come up with some clever ways to do that and let me scroll down a bit no labels criteria evaluation so to configure the run eval config we can put in criteria so that is exactly what we're doing over here so we're putting in criteria as the evaluator and now here we can do a couple of things so how do we evaluate outputs if we don't have a right or wrong answer meaning we just have the inputs well we can evaluate them based on a certain set of criterias and now out of the box they have put in place some let's see some criteria that we can already use so these are things like conciseness, relevance, correctness, coherence, harmfulness etc so these you can use straight out of the box so how does it work you select a criteria and plug in any of the strings that you see within this list over here but you can also specify a custom criteria and in the example they give this one of cliche so since we're creating rap battles we are going to evaluate whether the lyrics are cliche so how do we do that it's a dictionary so we have a criteria cliche and you respond with the following so that is the structure of how you set that up and now what we're going to do is we're going to run all of this so first we configure the eval config and then we do run on data set which again has the client that we previously defined, dataset name, large language model and the eval config so again they made some pretty nice wrappers to do all of this and now as you'll find out so this is now going to run in the background and this will take some time because it's going to loop over all the input inputs and all the criteria and then within that multiple large language model prompts to do the evaluation. So this now finished. Let's come back to the data set and click on the last run that we just did. And now we get this nice dashboard. So for all of our examples within the data set, we have a run over here and we can click on that. So let's check out the one with Barbie and Oppenheimer, so the rap battle. So first of all, top level, we have the input and we get the output. So this is the whole rap battle with some verses. Okay, so this is top level what we can see. But now we also get feedback and this is where it gets interesting. So now for all of the criteria that we've defined, there is an additional run that we can look into. Now why do we have four? Well if we use criteria, it says the default criterion is helpfulness and then we've added harmfulness, misogyny and the custom cliche one. So four in total. So let's now have a quick look at what's going on. So first helpfulness. We can drill down into this and then here what you can see is you enter into the middle of this tree basically where you can see the input and the output and then also the final output, the verdict basically of the assessment of the evaluation. So let's see what's going on over here. So this is just the input and the output that we've already seen. But if we look at what's going on over here, here you can see how the large language model is actually assessing and evaluating whether this is helpfulness. So based on the criteria helpfulness, this is kind of like the prompt instruction that LangChain already put into place in LangSmith to do the assessment. So you can see helpfulness, insightfulness and appropriateness. And then it concludes with a yes, so with a why. So based on the criteria, it can be considered helpful, insightful and appropriate. But now what's even more interesting is if we drill down to this bottom layer, here we can actually look at the prompt engineering itself. So you are assessing a submitted answer or a given task or input based on a set of criteria. So we have the data, we have input and we have the submission. And then if we scroll down, here we have the criteria helpfulness. Is this submission helpful, insightful and appropriate? If so, response why? If not, respond and. And then we have the end data. So here you can see how the prompt was actually engineered and how it was sent to the large language model to get to the final conclusion. So I know it's quite complex, takes some time to cover all of this, but I think it's really important to understand how this works top level. Because if we now look at the other, let's see, if we look at the other ones going on over here, so we have the feedback. We have the same for harmfulness, misogyny and cliche, but just in a different way. So we can go over here and then again, we have the tree structure over here where we can go all the way to the bottom where we can now see. Let's see how it was structured. So again, begin data input submission, and then we have the criteria that we created. So this is the custom criteria. So here you can also see how this relates to the input structure over here, which is kind of like, it's a dictionary, but it is split up in such a way where it's just one key and one value, even though it's split up. So you can see it's cliche and then other lyrics, cliche, respond, why, if they are, and if they are entirely unique. So this also shows you how you can create custom criteria pretty easily based on your data and your use case that you're working with. And then also all the way trace back how then Langsmith is using your custom criteria to evaluate this. And then finally, if we look at the top level of this tree, if this trace structure, we get all the top level metadata and get to the final output that is also locked here within the UI. So we get the feedback, we have a key score and value, and we also have a comment on that. So now if we go back to our dashboard, we can start to understand how they come up with this feedback over here. So you can see we have four total runs in here. You can also see the tokens and the time it took. And now here you can see you get a score and these are averages. So if we look at, let's see, let's open this up a little bit. You can see over here, so cliche, we have three zeros and one is identified as cliche. So then it just averages that into a 0.25 for cliche score, where one would be all of the outputs would be cliche. And now it's just one of the four. Then we have misogyny, which is at zero, nothing in there. Also no harmful content in here, which could be very interesting to monitor your business application for harmful responses. And we also have helpfulness, which has a score of one, a hundred percent score, meaning that all of the prompts were actually helpful. So I think it's quite clever in how they've set this up and you kind of have to get used to it and really identify criteria that are relevant for your application in order to really make this work. But this is a very good starting point to start your evaluation if you don't have key value pairs. So you don't know exactly what the output should be for your application. All right. So that was the quick start on how to set this up and how to run your first evaluation. But now let's look into the data sets where it. that's a little more interesting when we start to look at actual key value pairs. So inputs and outputs, questions and answers, where a lot of the use cases, especially if you're looking at information retrieval, kind of like chatbots with your own data, enterprise data, this is really where it gets interesting. So top level in Langsmith, there are three data types right now. So we have simple key value pairs, which we will look into as it's the most straightforward. You also have large language model datasets, which is a combination of input dictionaries and output dictionaries. And we also have it for chat, where it's kind of like the same, but then you have a series of inputs and outputs back and forth like in a chat. But like I've said for now, let's go over the key value pairs. And I will be going over these examples a little quickly because they're pretty straightforward in the sense that they all were kind of the same, but I'm just gonna show you a couple of ways you can go about this. So first of all, you can either do it using code, like we're doing right here, which is probably the best way to go about it, but you can also do it from the UI. So let me come back. Where were we? You can also come to datasets and create a new test run or upload a CSV over here. So that also works. But I'm going to show you first how we can get some example inputs. And now we're going to use key values. So we have a question, what's the largest mammal? And we have an answer, the blue whale and so on. So these are now our inputs. Now we can give this a name and similar to how we uploaded the initial dataset, we are going to do the same for this and create it. We have a description and then let's just run this. So this goes pretty quickly. We can come to the UI. Boom, we have another dataset. And now look here, we just had, let's see examples. Just inputs. And now we have inputs and outputs. And now here you can also see, since we have not specified what type of database we wanted to create, it defaulted to key value pair. All right, so now the next thing, we can also create dataset from existing runs. So let's create a quick dataset, again, example dataset. Come over here. Now we have an empty one. And now what we can do is we can get all of the runs for a given project. So we have the Langsmith tutorial, for example. So if we go to all of our projects, all the evaluations will be put by default under the evaluators project, but there are a lot of runs in here. So let's actually look at this one that we ran in the beginning to do like the hello world example. So we can take that and take all of the runs and just put it into the database. Now there's probably, so there's already an example. Ah, I see. So this is an error that you get if you try to add duplicate entries to datasets. So if I come to the example dataset, here you can see it was working well, but it just encountered, since we ran the hello world example quite a couple of times with the exact same output, you can only have unique inputs, outputs in here, which makes sense. So that is how you create a dataset from an existing run. And now we're getting all of the runs under a project, but you could probably be a little bit more clever about this and strategically extract the relevant pieces of information, but I haven't looked into that yet. Now we can do the same thing from a data frame. So this is kind of similar to a key value pair, but then we use the upload data frame. So let's just create this one and then let's see what it looks like. So we now have the questions and answers in a data frame and we can upload that as well. So let's do that. Let's see, it was my data frame dataset and go back over here, boom, another input output. So those are, oh wait, we have one more CSV upload, which is also very interesting. So here we can specify or link to a path in our directory. So here you can see in the data, we have our dataset.csv, which is again, question and answer. Same stuff, but now let's do it from a CSV and let's come back over here, come to the datasets and we also have a CSV dataset. So those are various ways that you can upload data to the Langsmith portal. All right, so now that we have actual key value pairs within our dataset, we can look into the QA evaluation. And there are three metrics or three functions that we can use for that. And that is the context QA, the QA in a chain of thought QA and the chain of thought QA is similar to the QA, but it just adds additional chain of thought reasoning. If you don't know what that is, it's a series of intermediate reasoning steps. It has been shown to improve the performance, the output of large language models. And they also say in the docs over here that this will get you the best results, but also takes longer, cost more tokens because of intermediate steps. QA, we'll look into all of these three, but basically the QA is simply looking at the input and the output to decide whether it's correct or not. A chain of thought does that, but then with more steps and context QA looks at the context. So they say this is useful if you have a larger corpus of grounding docs, but not necessarily have a ground through to answer the query. So let's see what that looks like and come over to the code over here. And now you can see that we can configure the run eval config again, similar to we did before, but now instead of putting in a criteria, we can put in these parameters over here and then we can hit run on data set again, fill in all of the information. So let's run this and see what we get. All right, so the evaluation is finished. Now let's come back to the data sets over here. So we, what we had, we had the elementary animal questions and here we can see the test runs. So this is the one we just did. And here we can see total time was 14 seconds to evaluate this. And here you can see that we get perfect scores, correctness, contextual accuracy, and also chain of thoughts, contextual accuracy. But now let's look into these individual, into all of the methods that we've used to evaluate them and actually look at the prompt underneath that. So let's start with the largest mammal, the whale. So top level, we have what is the largest mammal and then we get the largest mammal is the blue whale, et cetera, and it gives some more context. And then we have the reference output, which is the blue whale. So this was in the data, this is the data set, but this was the output. And now you can see how sometimes it can be quite tricky to evaluate whether an output is correct because it is based on a partial match of the overall output. So let's look at how this is taken care of within the evaluation model. So let's start with contextual accuracy. Let's look at this one. And here you can see what's going on. So we have a query, we have a result, and we have context. And then it is instructed to say, or the output is correct. So then let's look at the actual prompt. And okay, so here you can see the prompt engineering again. You are a teacher grading a quiz. You are giving a question. Okay, this is very interesting. And if question, context, student answer, and then we grade it, correct or incorrect. So this is pretty interesting to see. And they really instructed the AI to be a teacher. So they've probably experimented with this. And instead of saying like, hey, this is an evaluation tool to score large language model outputs, et cetera, they made it really simple and understandable in a way for humans as well, but probably also for large language models. So this is pretty interesting. So then we have question, context, and then we grade it. Interesting. Okay, so that's the first one. Now let's come back and now let's quickly compare all of them so we kind of get an understanding what the difference is. So first we have the context Q&A. So here you can see the prompt that went into it. And I'm not gonna go over all of this. If you're interested, you can read over this. And here we have the QA eval chain. So you can see what's going on over here. And then these are actually quite similar. It's just a different way of prompting. And then in the end, we have the chain of thought reasoning where there is a larger prompt. There's more to it, basically. So those are the differences, but they all, if we look like in the middle one over here, they all have the output, correct, correct. And then there's a little bit more information over here, but we get the grade correct. Now, and then coming back to top level, your data set, this is really where you can start to monitor everything. So you can have a look at the overall scores on all the metrics, all the evaluations that you've put into place. And now when you change something to your application, either like in the actual application itself through like prompt engineering or adding additional logic, or for example, in your factor database. So you're putting more examples into it. It becomes more and more important to quickly assess, okay, does it still pass the test? Are the answers that were previously correct, are they still correct? That's really where this comes into play and where this is so useful. And now you can also start a test run straight from the UI over here. So you can also see how you can have a, you can select all of the criteria. You can even do multiple and it generates the code for you already. And let's see, I don't think you can run it from the UI, but it generates the code. So you can just copy paste this and then run it within Python and you can update this. So this is all very powerful. And now within the code, I have some more examples on how you can create these evaluations with custom criterias, with labels and also without labels, like we've already seen. And then the final thing that I quickly wanna get into, which is another interesting evaluation, are two interesting evaluation metrics. We can use an embedding distance and a string distance. And I won't go into great detail on what these exactly are, but if you scroll down, you can see the embedding distance, how we can do that, and also the string distance. And if I come over here to the GitHub repository, there's also some more information on that where you can see the, we can use the cosine distance for the embeddings where we have zero to one. And we can also evaluate the string distance where it's the other way around. So zero is an exact match and one is no similarity. So those are also evaluation metrics that you can look into. And they put these into place to counter the fact that the evaluation scores that we were previously discussing don't have a direction, meaning like higher is not necessarily better. We just say, hey, if it's helpful, we have a one. But if we have the criteria of malicious, then we also get a one. And it's hard to compare that from like a modeling perspective. So the embedding. Distance and String Distance are ways to quantify the similarity, where you can monitor small changes over time. So for example, if you have two answers which are both helpful, both have a 1 on helpful, they cannot really be compared, right? Whereas if you use Embedding Distance, for example, or String Difference, you can have two answers which are both helpful and correct, but one might be better than the other, because it's closer to the actual answer, if that makes sense. So let's quickly have a look at what that looks like. So we first have the Embedding Distance, which uses the cosine, here you can see 0 to 1, one is more similar. So let's run that on the elementary animal questions, let's wait for that, and then also run the String Distance after that, where it's the other way around. 0 is exact match, and 1 is no similarity. Alright so that one is also finished right now, let's now get back to the runs, and here you can see we have the Embedding Distance, and now the other one should also come up in here, I believe. So we have the Embedding Distance, and let's see, here you can see the correctness, and again coming back to that, what we got going on, it's a range between 0 and 1. So this is quite interesting to see how this can be problematic in some way, where the answer is correct, we have the blue whale, but if we compare the reference output to what was set over here, we get a very low score, and that is because there's just a lot more information in here, but the answer is still correct. But you probably, if you really want to build this out, you have to combine both. So at some point you want to do some distance metrics, or string metrics that you want to put into place, and at other times you just want to evaluate, is it correct, yes or no. Now let's come back and see the other one, okay so that one also uploaded, so kind of like similar, but just another way of looking at it. We also have, so here it's a little higher, but then again remember that it is flipped over here, so 0 would be an exact match, and 1 is no similarity at all. Alright now, what have been some of my observations so far using Langsmith? Well first of all, the logging function provides a really transparent and structured way to examine the outputs of large language models, so it's excellent for that, great tool. Logging dataset allows different evaluations to be made independently on different runs, so I also like that, so that you have the ability to create different datasets, different projects, different organizations, so you can really customize this towards your need, your application, your organization. And then also, it's very useful to evaluate and compare LMA outputs with ground truths, and use both the existing and the customized evaluation criteria. Like I've said, we have quantifiable metrics, but the customizations also are really beneficial at some point. And then finally, the customized evaluations are really helpful, but they can take up a lot of tokens, so if we compare that, for example, let's go back to the first experiments that we were running on the rap battle dataset, so here you can see that we had 2200 tokens, and if we go back to the elementary animal questions where we were just running the correctness, for example, on a similar sized dataset, we can see that it's a lot less. And if we look at the cost, so this is also something to consider, so these are, we can look at the cost for today, so it's $1.30, just from today's experiments, but you can see we just run a couple of experiments with a dataset with like four records in it. So you really got to be mindful of that and closely monitor your cost as you start to run these evaluations, because I can really see how if you really scale this up to hundreds of thousands, potentially millions of records, that running an evaluation is going to be expensive. So you got to then really be careful about when to run that and also make sure you create good subsets of the data potentially to not like run it over everything. So that is just something to keep in mind and something we all have to like learn and experience how this actually works, putting large language models applications into production and monitoring costs and performance over time. All right, and that's it for this Langsmith tutorial. So I now hope that you have a solid understanding of how the platform works and also why it's actually useful and I would say even crucial if you're building applications with large language models. So if you found this video helpful, then please make sure to like this video and also subscribe to the channel. So go do that now. And then if you want to learn more, if you want to stay in the loop, then make sure to check out Data Alchemy, completely free, link is in the description. And in here I share my entire data and AI workflow that I use to not only create these videos but also complete the projects that I work on for my clients. So that's that. Make sure to check that out. And also if you want to learn more about what I do with DataLumina, you can check that out. Also look at the freelancer mastermind that we have if you're interested in that. Then that's it for now. I want to thank you all for watching and then I'll see you in the next video.\n"
     ]
    }
   ],
   "source": [
    "print(full_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the transcripts to text files we will use the below provided code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to full_audio_transcription.txt\n"
     ]
    }
   ],
   "source": [
    "# Specify the filename\n",
    "txt_file_name = 'full_audio_transcription.txt'\n",
    "\n",
    "# Open the file in write mode ('w') and write the string to it\n",
    "with open(txt_file_name, 'w') as file:\n",
    "    file.write(full_transcripts)\n",
    "\n",
    "print(f\"Transcription saved to {txt_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create a TextLoader using LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the `TextLoader` that will take the text from our transcript and load it into a document. \n",
    "\n",
    "---\n",
    "In LangChain, a document is essentially a piece of text along with associated metadata. You can create a document object by importing the `Document` class from the `langchain/document` module and then passing the text and metadata to the constructor of this class. The text is the main content that interacts with the language model, and the metadata can include information such as the source of the document.\n",
    "\n",
    "Document loaders in LangChain are used for importing data from various sources and converting them into document objects. These loaders can handle different types of input, such as a simple text file, the text contents from a web page, or even transcriptions from videos. The loaders provide a method called `load` which is used to import data as a document from a pre-configured source.\n",
    "\n",
    "Furthermore, LangChain offers Document Chains, which are sets of tools that allow for efficient processing and analysis of large amounts of text data. These chains can be used for a range of purposes, such as summarizing documents, answering questions over documents, and extracting information from them.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this step, do the following: \n",
    "- Import `TextLoader` from `langchain.document_loaders`\n",
    "- Create a variable called `loader` that uses the `TextLoader` method which takes in the directory of the transcripts `\"./files/text\"`\n",
    "- Create a variable called `docs` that is assigned the result of calling the `loader.load()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1694705724878,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.document_loaders import TextLoader\n\nloader = TextLoader(\"./files/text\")\ndocs = loader.load()"
   },
   "outputs": [],
   "source": [
    "# Import the TextLoader class from the langchain.document_loaders module\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# Create a new instance of the TextLoader class, specifying the directory containing the text files\n",
    "loader = TextLoader(\"./files/transcripts/transcript.txt\")\n",
    "\n",
    "\n",
    "# Load the documents from the specified directory using the TextLoader instance\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705727440,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "docs[0]"
   },
   "outputs": [],
   "source": [
    "# Show the first element metadata of docs to verify it has been loaded \n",
    "docs[0].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705727440,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "docs[0]"
   },
   "outputs": [],
   "source": [
    "# Show the first element of docs to verify it has been loaded \n",
    "#docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Creating an In-Memory Vector Store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space. \n",
    "\n",
    "For large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the `docarray` package. \n",
    "\n",
    "We will also tokenize our queries using the `tiktoken` package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model \"understand\" the text and relationships with other tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Import the `tiktoken` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 46,
    "lastExecutedAt": 1694705815702,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import tiktoken"
   },
   "outputs": [],
   "source": [
    "# Import the tiktoken package\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Create the Document Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use LangChain to complete some important operations to create the Question and Answer experience. Let´s import the follwing: \n",
    "\n",
    "- Import `RetrievalQA` from `langchain.chains` - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents \n",
    "- Import `ChatOpenAI` from `langchain.chat_models` - this imports the ChatOpenAI model that we will use to query the data \n",
    "- Import `DocArrayInMemorySearch` from `langchain.vectorstores` - this gives the ability to search over the vector store we have created. \n",
    "- Import `OpenAIEmbeddings` from `langchain.embeddings` - this will create embeddings for the data store in the vector store. \n",
    "- Import `display` and `Markdown`from `IPython.display` - this will create formatted responses to the queries. ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8,
    "lastExecutedAt": 1694706214485,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain.embeddings import OpenAIEmbeddings"
   },
   "outputs": [],
   "source": [
    "# Import the RetrievalQA class from the langchain.chains module\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Import the ChatOpenAI class from the langchain.chat_models module\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# Import the OpenAIEmbeddings class from the langchain.embeddings module\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a vector store that will use the `DocArrayInMemory` search methods which will search through the created embeddings created by the OpenAI Embeddings function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this step: \n",
    "- Create a variable called `db`\n",
    "- Assign the `db` variable to store the result of the method `DocArrayInMemorySearch.from_documents`\n",
    "- In the DocArrayInMemorySearch method, pass in the `docs` and a function call to `OpenAIEmbeddings()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 502,
    "lastExecutedAt": 1694706217261,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "db = DocArrayInMemorySearch.from_documents(\n    docs, \n    OpenAIEmbeddings()\n)"
   },
   "outputs": [],
   "source": [
    "# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs,\n",
    "    OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a retriever from the `db` we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the `ChatOpenAI` model, will assigned that as our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following: \n",
    "- A variable called `retriever` that is assigned `db.as_retriever()`\n",
    "- A variable called `llm` that creates the `ChatOpenAI` method with a set `temperature`of `0.0`. This will controle the variability in the responses we receive from the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8,
    "lastExecutedAt": 1694706219264,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "retriever = db.as_retriever() \nllm = ChatOpenAI(temperature = 0.0)"
   },
   "outputs": [],
   "source": [
    "# Convert the DocArrayInMemorySearch instance to a retriever\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create a new ChatOpenAI instance with a temperature of 0.0\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last step before starting to ask questions is to create the `RetrievalQA` chain. This chain takes in the:  \n",
    "- The `llm` we want to use\n",
    "- The `chain_type` which is how the model retrieves the data\n",
    "- The `retriever` that we have created \n",
    "- An option called `verbose` that allows use to see the seperate steps of the chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable called `qa_stuff`. This variable will be assigned the method `RetrievalQA.from_chain_type`. \n",
    "\n",
    "Use the following settings inside this method: \n",
    "- `llm=llm`\n",
    "- `chain_type=\"stuff\"`\n",
    "- `retriever=retriever`\n",
    "- `verbose=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9,
    "lastExecutedAt": 1694706178555,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "qa_stuff = RetrievalQA.from_chain_type(\n\nllm=llm,\n\nchain_type=\"stuff\",\n\nretriever=retriever,\n\nverbose=True\n\n)"
   },
   "outputs": [],
   "source": [
    "# Create a new RetrievalQA instance with the specified parameters\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    \n",
    "    # The ChatOpenAI instance to use for generating responses\n",
    "    llm=llm,\n",
    "    \n",
    "    # The type of chain to use for the QA system\n",
    "    chain_type=\"stuff\",\n",
    "    \n",
    "    # The retriever to use for retrieving relevant documents\n",
    "    retriever=retriever,\n",
    "    \n",
    "    # Whether to print verbose output during retrieval and generation\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Create the Queries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the questions to ask the model complete the following steps: \n",
    "- Create a variable call `query` and assigned it a string value of `\"What is this tutorial about?\"`\n",
    "- Create a `response` variable that will store the result of `qa_stuff.run(query)` \n",
    "- Show the `resposnse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"What is this tutorial about?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"What is the difference between a training set and test set?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"Who should watch this lesson?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query =\"Who is the greatest football/soccer team on earth?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     },
     "1": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"How long is the circumference of the earth?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
