{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promptfoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need a developer account with [OpenAI ](https://auth0.openai.com/u/signup/identifier?state=hKFo2SAyeTZBU1pzbUNWYWs3Wml5OWVvUVh4enZldC1LYU9PMaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIDFUakNoUGFMLUdNWFpfQkpqdncyZjVDQk9xUTE4U0xDo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q) and a create API Key. The API secret key will be stored in your 'Environment Variables' on the side menu. See the *getting-started.ipynb* notebook for details on setting this up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project requires several packages that need to be installed into Workspace.\n",
    "\n",
    "- `langchain` is a framework for developing generative AI applications.\n",
    "- `yt_dlp` lets you download YouTube videos.\n",
    "- `tiktoken` converts text into tokens.\n",
    "- `docarray` makes it easier to work with multi-model data (in this case mixing audio and text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Run the following code to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10465,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705467366,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install langchain yt_dlp",
    "outputsMetadata": {
     "0": {
      "height": 463,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Install langchain\n",
    "#!pip install langchain==0.0.292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8477,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694802259584,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Install yt_dlp\n!pip install yt_dlp==2023.7.6",
    "outputsMetadata": {
     "0": {
      "height": 447,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Install yt_dlp\n",
    "# !pip install yt_dlp==2023.7.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6113,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705815654,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install tiktoken docarray",
    "outputsMetadata": {
     "0": {
      "height": 447,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install docarray==0.38.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "## Task 0: Select the YouTube video to transcribw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example YouTube tutorial video\n",
    "# youtube_url = input(\"Paste the YouTube URL: \")\n",
    "youtube_url = \"https://www.youtube.com/watch?v=KhINc5XwhKs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import The Required Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we need the `os` and the `yt_dlp` packages to download the YouTube video of your choosing, convert it to an `.mp3` and save the file. We will also be using the `openai` package to make easy calls to the OpenAI models we will use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following packages.\n",
    "\n",
    "- Import `os` \n",
    "- Import `openai`\n",
    "- Import `yt_dlp` with the alias `youtube_dl`\n",
    "- From the `yt_dlp` package, import `DowloadError`\n",
    "- Assign `openai_api_key` to `os.getenv(\"OPENAI_API_KEY\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 176,
    "lastExecutedAt": 1694705470957,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n\nimport os   #import os package \nimport glob\nimport openai #import the openai package \nimport yt_dlp as youtube_dl #import the yt_dlp package as youtube_dl\nfrom yt_dlp import DownloadError #import DownloadError from yt_dlp ",
    "outputsMetadata": {
     "0": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ROOT_KEY' from 'pydantic.main' (C:\\Users\\josem\\anaconda3\\lib\\site-packages\\pydantic\\main.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myt_dlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadError\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Import Docarray\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docarray\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.38.0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocList, DocVec\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_doc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDoc\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_path_from_docarray_root_level\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docarray\\array\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01many_array\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnyDocArray\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc_list\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocList\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc_vec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc_vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocVec\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docarray\\array\\any_array.py:22\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m      5\u001b[0m     Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     overload,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_doc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDoc\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_array_summary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocArraySummary\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnusableObjectError\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docarray\\base_doc\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_doc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01many_doc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnyDoc\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_doc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_node\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseNode\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_doc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDoc\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docarray\\base_doc\\any_doc.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Type\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDoc\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAnyDoc\u001b[39;00m(BaseDoc):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    AnyDoc is a Document that is not tied to any schema\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docarray\\base_doc\\doc.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01morjson\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ROOT_KEY\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrich\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsole\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Console\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_doc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_node\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseNode\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ROOT_KEY' from 'pydantic.main' (C:\\Users\\josem\\anaconda3\\lib\\site-packages\\pydantic\\main.py)"
     ]
    }
   ],
   "source": [
    "# Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n",
    "\n",
    "# Import the os package \n",
    "import os\n",
    "\n",
    "# Import glob\n",
    "import glob\n",
    "\n",
    "# Import the openai package \n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Import the yt_dlp package as youtube_dl\n",
    "import yt_dlp as youtube_dl\n",
    "\n",
    "# Import DownloadError from yt_dlp \n",
    "from yt_dlp import DownloadError\n",
    "\n",
    "# Import Docarray\n",
    "import docarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also assign the variable `openai_api_key` to the environment variable \"OPEN_AI_KEY\". This will help keep our key secure and remove the need to write it in the code here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10,
    "lastExecutedAt": 1694705474406,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Download the YouTube Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3). \n",
    "\n",
    "We'll download a DataCamp tutorial about machine learning in Python.\n",
    "\n",
    "We will do this by setting a variable to store the `youtube_url` and the `output_dir` that we want the file to be stored. \n",
    "\n",
    "The `yt_dlp` allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you. \n",
    "\n",
    "Lastly, we will create a loop that looks in the `output_dir` to find any .mp3 files. Then we will store those in a list called `audio_files` that will be used later to send each file to the Whisper model for transcription. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following: \n",
    "- Two variables - `youtube_url` to store the Video URL and `output_dir` that will be the directory where the audio files will be saved. \n",
    "- For this tutorial, we can set the `youtube_url` to the following `\"https://www.youtube.com/watch?v=aqzxYofJ_ck\"`and the `output_dir`to `files/audio/`. In the future, you can change these values. \n",
    "- Use the `ydl_config` that is provided to you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 533,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1694802393469,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(f\"Downloading video from {youtube_url}\")\n\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\n\n\n",
    "outputsMetadata": {
     "0": {
      "height": 357,
      "type": "stream"
     },
     "1": {
      "height": 137,
      "type": "stream"
     },
     "2": {
      "height": 97,
      "type": "stream"
     },
     "3": {
      "height": 37,
      "type": "stream"
     },
     "4": {
      "height": 257,
      "type": "stream"
     },
     "5": {
      "height": 77,
      "type": "stream"
     },
     "6": {
      "height": 57,
      "type": "stream"
     },
     "7": {
      "height": 57,
      "type": "stream"
     },
     "8": {
      "height": 97,
      "type": "stream"
     },
     "9": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Encodings: locale cp1252, fs utf-8, pref cp1252, out UTF-8 (No VT), error UTF-8 (No VT), screen UTF-8 (No VT)\n",
      "[debug] yt-dlp version stable@2023.07.06 [b532a3481] (pip) API\n",
      "[debug] params: {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'outtmpl': 'files/audio/%(title)s.%(ext)s', 'verbose': True, 'compat_opts': set()}\n",
      "[debug] Python 3.8.3 (CPython AMD64 64bit) - Windows-10-10.0.19041-SP0 (OpenSSL 1.1.1g  21 Apr 2020)\n",
      "[debug] exe versions: ffmpeg 2023-01-16-git-01f46f18db-full_build-www.gyan.dev (setts), ffprobe 2023-01-16-git-01f46f18db-full_build-www.gyan.dev\n",
      "[debug] Optional libraries: Cryptodome-3.19.1, brotli-None, certifi-2022.12.07, mutagen-1.47.0, sqlite3-2.6.0, websockets-12.0\n",
      "[debug] Proxy map: {}\n",
      "[debug] Loaded 1855 extractors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video from https://www.youtube.com/watch?v=KhINc5XwhKs.\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=KhINc5XwhKs\n",
      "[youtube] KhINc5XwhKs: Downloading webpage\n",
      "[youtube] KhINc5XwhKs: Downloading ios player API JSON\n",
      "[youtube] KhINc5XwhKs: Downloading android player API JSON\n",
      "[youtube] KhINc5XwhKs: Downloading m3u8 information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec:vp9.2, channels, acodec, lang, proto\n",
      "[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec:vp9.2(10), channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] KhINc5XwhKs: Downloading 1 format(s): 251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Invoking http downloader on \"https://rr2---sn-8vq54voxn25po-mgte.googlevideo.com/videoplayback?expire=1708014227&ei=M-bNZaa8MMC3p-oPicmt8Ao&ip=84.125.157.205&id=o-ANQdxC23xu3htBHbJ9fu6xMsZJ0-WVy8C7xa8nbNZYp5&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=bk&mm=31%2C29&mn=sn-8vq54voxn25po-mgte%2Csn-h5q7knel&ms=au%2Crdu&mv=m&mvi=2&pl=21&initcwndbps=1825000&vprv=1&svpuc=1&mime=audio%2Fwebm&gir=yes&clen=17836505&dur=995.861&lmt=1702246307166129&mt=1707992240&fvip=5&keepalive=yes&fexp=24007246&c=ANDROID&txp=6308224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cvprv%2Csvpuc%2Cmime%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRAIgMBDY_EjUP86CdZ7V_Kly_86mZOCVg0k0-pGSne11B7sCIAaWzO6lIQSU5Vt8WGNDJiwQjLsHX78AWl4SOSp6ICUy&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=APTiJQcwRQIgTM7qemMxdE88zkc10o1rLOPDF05K9WQhwBHf4KNLOa4CIQC0R3-eHP-Nun9W5M4FDQil8TSys7ltj9yANePOQURQXA%3D%3D\"\n",
      "[debug] File locking is not supported. Proceeding without locking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Destination: files\\audio\\Test Driven PROMPT Engineering： Using Promptfoo to COMPARE Prompts, LLMs, and Providers..webm\n",
      "[download] 100% of   17.01MiB in 00:00:07 at 2.14MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffprobe -show_streams \"file:files\\audio\\Test Driven PROMPT Engineering： Using Promptfoo to COMPARE Prompts, LLMs, and Providers..webm\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExtractAudio] Destination: files\\audio\\Test Driven PROMPT Engineering： Using Promptfoo to COMPARE Prompts, LLMs, and Providers..mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffmpeg -y -loglevel \"repeat+info\" -i \"file:files\\audio\\Test Driven PROMPT Engineering： Using Promptfoo to COMPARE Prompts, LLMs, and Providers..webm\" -vn -acodec libmp3lame \"-b:a\" 192.0k -movflags \"+faststart\" \"file:files\\audio\\Test Driven PROMPT Engineering： Using Promptfoo to COMPARE Prompts, LLMs, and Providers..mp3\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file files\\audio\\Test Driven PROMPT Engineering： Using Promptfoo to COMPARE Prompts, LLMs, and Providers..webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "# Directory to store the downloaded video\n",
    "output_dir = \"files/audio/\"\n",
    "\n",
    "# Config for youtube-dl\n",
    "ydl_config = {\n",
    "    \"format\": \"bestaudio/best\",\n",
    "    \"postprocessors\": [\n",
    "        {\n",
    "            \"key\": \"FFmpegExtractAudio\",\n",
    "            \"preferredcodec\": \"mp3\",\n",
    "            \"preferredquality\": \"192\",\n",
    "        }\n",
    "    ],\n",
    "    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "# Check if the output directory exists, if not create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# Print a message indicating which video is being downloaded\n",
    "print(f\"Downloading video from {youtube_url}.\")\n",
    "\n",
    "# Attempt to download the video using the specified configuration\n",
    "# If a DownloadError occurs, attempt to download the video again\n",
    "try:\n",
    "    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "except DownloadError as DE:\n",
    "    print(f\"An error occured: {DE}\")\n",
    "    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([youtube_url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the audio files that we will use the `glob` module that looks in the `output_dir` to find any .mp3 files. Then we will append the file to a list called `audio_files`. This will be used later to send each file to the Whisper model for transcription. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following: \n",
    "- A variable called `audio_files`that uses the glob module to find all matching files with the `.mp3` file extension \n",
    "- Select the first first file in the list and assign it to `audio_filename`\n",
    "- To verify the filename, print `audio_filename` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1694705587367,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Find the audio file in the output directory\n\n# Define function parameters\noutput_dir = \"files/audio/\"\n\n# Find the audio file in the output directory\naudio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\naudio_filename = audio_files[0]\nprint(audio_filename)",
    "outputsMetadata": {
     "0": {
      "height": 56,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Find the audio file in the output directory\n",
    "\n",
    "# Find all the audio files in the output directory\n",
    "audio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/audio\\Test Driven PROMPT Engineering： Using Promptfoo to COMPARE Prompts, LLMs, and Providers..mp3\n"
     ]
    }
   ],
   "source": [
    "# Select the first audio file in the list\n",
    "audio_filename = audio_files[-1]\n",
    "\n",
    "# Print the name of the selected audio file\n",
    "print(audio_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Transcribe the Video using Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the `audio_file`, for the `output_file` and the model. \n",
    "\n",
    "Using these variables we will:\n",
    "- create a list to store the transcripts\n",
    "- Read the Audio File \n",
    "- Send the file to the Whisper Model using the OpenAI package "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this step, create the following: \n",
    "- A variable named `audio_file`that is assigned the `audio_filename` we created in the last step\n",
    "- A variable named `output_file` that is assigned the value `\"files/transcripts/transcript.txt\"`\n",
    "- A variable named `model` that is assigned the value  `\"whisper-1\"`\n",
    "- An empty list called `transcripts`\n",
    "- A variable named `audio` that uses the `open` method and `\"rb\"` modifier on the `audio_file`\n",
    "- A variable to store the `response` from the `openai.Audio.transcribe` method that takes in the `model`and `audio` variables \n",
    "- Append the `response[\"text\"]`to the `transcripts` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been split into 6 parts.\n"
     ]
    }
   ],
   "source": [
    "#split the audio file\n",
    "# !pip install pydub\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Load the MP3 file\n",
    "mp3_file = audio_filename\n",
    "audio = AudioSegment.from_mp3(mp3_file)\n",
    "\n",
    "# Length of the audio in milliseconds and calculate the duration of each part\n",
    "length = len(audio)\n",
    "part_duration = length // 6\n",
    "\n",
    "# Split the audio into 4 parts and save them\n",
    "for i in range(6):\n",
    "    start = i * part_duration\n",
    "    end = start + part_duration\n",
    "    part = audio[start:end]\n",
    "    part_name = f\"part_{i+1}.mp3\"\n",
    "    part.export(part_name, format=\"mp3\")\n",
    "\n",
    "print(\"The file has been split into 6 parts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_segments = [\"part_1.mp3\", \"part_2.mp3\", \"part_3.mp3\", \"part_4.mp3\", \"part_5.mp3\", \"part_6.mp3\"]\n",
    "transcripts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:   0%|                                                                    | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_1.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:  17%|██████████                                                  | 1/6 [00:09<00:46,  9.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_2.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:  33%|████████████████████                                        | 2/6 [00:19<00:39,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_3.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:  50%|██████████████████████████████                              | 3/6 [00:28<00:28,  9.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_4.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:  67%|████████████████████████████████████████                    | 4/6 [00:37<00:18,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_5.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting audio files:  83%|██████████████████████████████████████████████████          | 5/6 [00:47<00:09,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting part_6.mp3 to text...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting audio files: 100%|████████████████████████████████████████████████████████████| 6/6 [00:57<00:00,  9.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "for audio_segment in tqdm(audio_segments, desc=\"Converting audio files\"):\n",
    "    print(f\"Converting {audio_segment} to text...\\n\")\n",
    "    with open(audio_segment, \"rb\") as file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\", \n",
    "            file=file\n",
    "        )\n",
    "        transcripts.append(transcript)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Transcription(text=\"How do you know if you've created a great prompt that will deliver repeatable results across hundreds of uses in your apps and tools? If you've clicked on this video, you've probably run into situations where you're running a prompt for an application or tool and you've wondered, how can I improve this prompt in speed, accuracy, and cost? The tricky part about prompt engineering right now is that if you don't have a framework in place for measuring a prompt against another prompt or an LLM provider against another LLM provider, you won't really know if you're adding value or removing value from your prompts. And therefore, you won't really know if you're improving or actually making your application worse. I think for a lot of prompts, especially personal prompts, you can get by on just feel. You can get by on just a couple results that you see out of your prompt. But when you wanna kick things up to the next level, when you wanna scale your prompts into an application that runs hundreds, thousands, and maybe tens of thousands of times per day, you need to ship your prompts with confidence. That's where test-driven prompt engineering comes into play. In this video, I'm gonna show you how you can get cheaper, faster, and more accurate prompts by utilizing a prompt evaluation and testing framework called PromptFu. PromptFu lets you iterate on your LLMs faster and it helps you measure the quality and it prevents you from making your prompts worse. It helps you catch regressions. After this video, you'll know how you can compare your prompts to find the most accurate, fastest, cheapest prompt for your tools and applications. Let's dive right in. So I'm gonna open up VS Code. This is a quick start PromptFu code base that I've built for you. Go ahead, check it out, link in the description. This is going to help you get ramped up very quickly and understand the core concepts of prompt testing. If you open the readme, you're gonna see a set of instructions. Let's go ahead and go into preview mode here. And the setup for this is extremely simple. Huge shout out to the PromptFu engineers for building this. Install PromptFu, export your OpenAI API key. I'm gonna do that now. And then we're gonna CD into one of the test suites here. I'm going to go into our first minimal example and then we're gonna run PromptFu eval. So I'm gonna do that now, PromptFu eval. So this is what the standard output looks like in the CLI. We'll get into the details in a moment here. What I'm gonna do is open up the local web viewer and we can do that using PromptFu view. So we're gonna do that now, PromptFu view. Great, so this view is much better. So let's start from the top here. You can see here we have a description. So this is just the description of what we're testing. You can see here we have a variable, which is NLQ, natural language query. And you can see it's saying, list jobs that have run longer than 30 seconds. So this is a variable I'm passing into my prompt. And if we inspect our prompt.\"), Transcription(text=\"prompt here, you can see this is the prompt. So is this following block of text, a SQL natural language query, NLQ. And then here I'm passing in this NLQ variable. And this NLQ variable is exactly what gets replaced by this variable here, right? So you can see this is NLQ. If we open this up again, we have this syntax here where it says NLQ. So that's going to replace there. You can see here we're running two models, OpenAI's GPT-4 Turbo Preview, and OpenAI's 3.5 Turbo Preview. And already, just by running this one simple test, you can see something really interesting, right? I'm getting the exact same results, but in a vastly different number of tokens. And if we remove the cache, which we'll do in a second, this is a cached result. You'll see that GPT-3.5 is much faster than 3.4, right? But let's go ahead and open this up. GPT-4 is giving me a lot of additional information. That's really cool. Do I need all that information? In this case, I don't. And GPT-3 is going to be more than enough. But let's backtrack a little bit. Let me show you exactly what this test looks like. So if we go back to VS Code and we open up this directory, we can see we have three files. And this structure is repeated in every test here, okay? So let's open up the PromptFu config. That's where everything starts. You can see the structure is very simple. And this is something that I love about PromptFu. You're able to really condense it down into really clean, reusable testing structures. So you can see we have a high-level description. We have our two providers. We have a reference to our prompts and a reference to our tests. So this is referencing this prompt file right here. And you can see here we have this file blob pattern saying, it's saying pull in every test with this format, right? So test and then any string and then .yaml. This is really cool. PromptFu gives you a simple configuration file. This is the starting point for all tests. But if we go ahead and open up the prompt, you can see here, just like we saw it in the UI, we have this simple prompt. The idea is that you'll store all of your prompts in this type of test suite or in some type of centralized location. And then you can hook up PromptFu using this prompt syntax to go to that exact location, you know, wherever your prompts are, right? And then you can run arbitrary tests on them, right? So I think that's gonna be like the right structure when you're setting up your test suites. So anyway, let's continue. So you can see here we have our prompt. We have the variable in our prompt that's going to be dynamically changed based on our test. Okay? So let's look at the last file here, our test.yaml. So I've commented out a couple items here. Let me just get rid of these for now. And we can add them back in a moment. So what does a test look like in PromptFu? It's quite simple. So we have a description. So this is, you know, you describing what the test is.\"), Transcription(text=\"the test is testing. And then we have two really important blocks. We have variables and you can see here we have that NLQ variable, right? This is what's going to be dynamically inserted into each test. If we had another block like this here and we updated NLQ, we're gonna get two runs of our prompt with the two different versions of the NLQ variable, right? And I'll show that off in a moment here. The next most important block here is this assertion block. And this is what really creates the value of your tests, right? This is where your tests really become valuable. This enables you to build out assertions into your tests. And if we open up the docs here, so in the read me, I've linked a few really important places you're gonna wanna hop to quickly in the prompt for documentation. The assertion is one of them. So if we open up the assertions, you can see here there's an entire slew of different ways you can assert truth after your test runs, right? So imagine you run your prompt and now you have your prompt output. On the output, you're going to run different assertions, right? So you can imagine, for our natural language query prompt, let's go ahead and open that up, right? So we're saying, is this following block of text a SQL natural language query? So in order to test if that's true, you can see here inside the test, I'm using this I contains any. So this is ignore case contains anything, right? So this is saying, ignoring case, make sure that my prompt contains any one of these words, right? So any one of these items, make sure it's in my prompt, right? And you can see that here, I contains any. They have a really nice set of assertion types that you can make, right? You can use simple ones like, make sure it contains a certain string. You can use regex matching. You can use custom JavaScript, custom Python. You can use Levenstein distance. And then you can do some really cool things that we'll get to in a moment. Like my favorite so far, the LLM rubric, which basically lets you run LLMs on your LLM responses, right? So you can say something like, make sure the sentiment is positive, right? So make sure that the output is positive. You can say things like, make sure this is a natural language query, right? Which is what we're gonna do in this example coming up here. So that's what the test looks like. And really after you've learned just these three simple components, you have your providers, you have your prompt and you have your tests, you're actually good to go. And that's why I wanted to make this video just by knowing these three simple components and by knowing about PromptFu, you've just unlocked massive value for your prompts, for your tools, for your applications, and therefore your users, right? And that's what this is all about. I've mentioned this on the channel before. If you're a subscriber to the channel, you know that we aim to use the best tool for the job, the tool that allows us to build the best experiences.\"), Transcription(text=\"for our users in the age of LLMs and AI technology, you need to be able to test the prompts to maximize accuracy for your users and also for your business. You wanna be able to cut your costs to the bare minimum, right? By comparing different prompts, you can know, you know, how long does that prompt need to be? You can know, do I need GPT-4 here? Can I use something simpler, right? So, you know, those are just a couple of ways you can use this. Let's go ahead, add these items back to our tests, right? So I'm gonna add two additional test cases and, you know, just never let yourself get overwhelmed. Just take things one step at a time. So if we collapse this, you can see here we just have three items in our test and, you know, each one contains their own natural language query, which then will get passed into our prompt and run three times, right? So what we end up with here is one prompt that runs three different times, right? And then we run that two times for each provider, right? So we're gonna end up with six total tests here. Let's go ahead and run this again. So I'm gonna close that out. I'm gonna run the prompt for eval and I'm gonna run this with the no cache parameter. So no cache, you can see here that it's returning cached items so that you save tokens on your testing. I actually recommend always running with no cache so you can get fresh results every time as you're updating your tests. So I'm gonna run no cache and you can see here it's running those six items. It's running the 3.5 turbo model and it's gonna run GPT-4, there we go. Great, so that's run. You can see we have no failures so everything ran successfully. Let's go ahead and open up the viewer. Now the UI should make a little more sense here, right? We have our natural language query variable running three times and then we have our outputs which are running each different provider on our prompt and that's running for GPT-4 and then you have GPT-3 doing the exact same thing here, right? And so a couple of things to look out for here right away, right? Even just with this small test, we have a test running three times with different assertions. You can see here that GPT-4 took 11 seconds, right? 11,000 milliseconds, that's gonna be 11 seconds and GPT-3 took less than one second, okay? Something else important to note here, the results are the exact same. You can see here in a lot more detail, GPT-4 is giving me a lot more information. It's helping me build up the actual SQL which is great but that's not really what I want here. I just want it to tell me is it a natural language query or is it not? And you can see here GPT-3 is doing that in a 10th of the time at a fourth of the cost. This is really, really important to call out, right? I'm just gonna say that one more time. GPT-3.5 Turbo is doing the exact same work GPT-4 Turbo is in a 10th of the time, right? At a fourth of the cost, right? We have 200 tokens here, we have 50 tokens here, right?\"), Transcription(text=\"Massive if I have this prompt in production, which I do I'm gonna be totally transparent here I have the GPT for prompt running this exact same prompt for a new tool called talk to your database You can check that out link in the description if you're interested. I want to call out how important this is, right? We have a huge prompt giving us a lot more information back that we don't need We don't want GPT 3.5 could do it for us Right a tenth of the speed and a fourth of the cost even if this isn't always consistent You can imagine that some version of this ratio, right? 10 to 1 4 to 1 Exists right and and the whole point is with testing your prompts you can reveal this information Okay, I want to call it a couple more things here in our second prompt You can see we're using the LLM rubric assertion type Let me collapse the first test case and you can see here for list 5 Users with Gmail and that's happening right here, right? So that that's this row. Okay, we actually have another test validation, right? We're saying response should confirm that query is an NLQ So this is one of many assertion types that you can use with prompt foo, right? There's there's tons of them here You're gonna find a lot more of these other ones really valuable This is just something that I have gravitated toward to get up and running quickly I'm a huge fan of the LLM rubric because you can just basically prompt on your prompt to confirm You know the type of response happening. So so that's fantastic I'm gonna leave all the information you need to get started and to get running in the readme So there's gonna be a lot of valuable information for you here link in description. It's all here ready for you I know I said I wouldn't make a video this week, but I dug into this and you know I started thinking, you know It's really important for engineers for product builders to see this save time save money speed up your prompts Feel free to like feel free to sub if this was helpful for you There are a couple other tests in here that you can look at. There's a lot more you can do with prompt foo, right? We're only using two LLM providers here. If you look in our prop food config, I'm just looking at open AI You know in the docs I've linked to different providers and they have a whole slew of you know, all the common providers And I'm sure they'll be adding Gemini coming out pretty quickly here I'm really excited to test that out see how it compares and you know, this kind of opens up a whole new realm of Knowing and having certainty that your prompts are great. So there's a lot more information here There's a lot more value for free to check out the repo if you you don't have interest if you have this problem if you Want to improve your prompts you want to cut your costs highly recommend you check this out just real quick I want to call out, you know, what why are we testing? Why are we evaluating our LLMs? Why are you evaluating our prompts specifically, right? It really comes down to these three things and I'm gonna gloss over that these pretty quickly here I want to keep this video short for you guys save money save time ship with confidence and\"), Transcription(text=\"and prevent regressions, right? Once you have all your prompts in a centralized location, you can prevent issues and you can detect them before they run into production, right? It's all about setting up the right assertions for your tests, and the beautiful part about this is, you know, let's say you have three or four more assertions, one of them can fail, and that can be okay for your prompt, right? PromptFood will tell you, you know, three out of four tests pass, and maybe that's okay, right? Maybe that's okay for this situation, but you might get below a threshold where, you know, if 50% of your test cases are failing for a specific prompt and a specific test, then, you know, you need to give it some more attention, or the change that you just made to your prompt is actually causing a regression, right? So that's really important to call out there. Save money, save time, I think that's pretty obvious at this point, right? By knowing how many tokens you need, by knowing which LLM you need to produce a prompt and produce it with consistent results, you can save money and save time, right? One of the big things for me, I've been working on a Natural Language Query AI Postgres data analytics tool. I need to know and test different prompts against different prompt inputs with a couple different variables coming in, right? And PromptFood gives you a really clean structure, right? You set up a bunch of prompts and you pass in specific tests and you can update these variables, right? I have two other examples here that are a lot more detailed that are gonna give you a lot more information, right? You can actually pass in multiple prompts by using this triple dash, and then all your tests will run for each one of these prompts, right? So there's a lot more value here. I just want to call out a couple things. I think that there is this journey that engineers go on where they first dislike testing because it burns so much time. I completely understand that. I was that engineer once too, but then they come around to it because they realize it actually saves them time and increases confidence and reduces bugs and lets them ship faster with confidence. So I think carrying some of that mentality over to your prompts is gonna be a wise move. It's not out of the question to build test-driven prompts, you know, to write the test first and then build the prompt to fit the test, right? So let your test kind of drive how you write your prompts. I'm a huge fan of PromptFu due to the simplicity and ease to get set up. So again, shout out to the devs, highly recommend them. I'm gonna be sticking with them for a while. They give a lot of simplicity and customizability. So, you know, it's a great tool. It's gonna be one that's in my toolbox right now, especially in the age of LLMs, the age of AI, GPTs, et cetera. So that's all for this one. Drop a like and a sub for more agentic engineering tips, tools, principles, and more. On this channel, we build full, complete tools, full, complete products using the latest and greatest technology. Right now, it's all about agentic software. You're actively on the road to building software that lives, breathes, and creates value for us while we sleep. I'll see you in the next one.\")]\n"
     ]
    }
   ],
   "source": [
    "print(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(transcripts[1].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_transcripts = \" \".join([t.text for t in transcripts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you know if you've created a great prompt that will deliver repeatable results across hundreds of uses in your apps and tools? If you've clicked on this video, you've probably run into situations where you're running a prompt for an application or tool and you've wondered, how can I improve this prompt in speed, accuracy, and cost? The tricky part about prompt engineering right now is that if you don't have a framework in place for measuring a prompt against another prompt or an LLM provider against another LLM provider, you won't really know if you're adding value or removing value from your prompts. And therefore, you won't really know if you're improving or actually making your application worse. I think for a lot of prompts, especially personal prompts, you can get by on just feel. You can get by on just a couple results that you see out of your prompt. But when you wanna kick things up to the next level, when you wanna scale your prompts into an application that runs hundreds, thousands, and maybe tens of thousands of times per day, you need to ship your prompts with confidence. That's where test-driven prompt engineering comes into play. In this video, I'm gonna show you how you can get cheaper, faster, and more accurate prompts by utilizing a prompt evaluation and testing framework called PromptFu. PromptFu lets you iterate on your LLMs faster and it helps you measure the quality and it prevents you from making your prompts worse. It helps you catch regressions. After this video, you'll know how you can compare your prompts to find the most accurate, fastest, cheapest prompt for your tools and applications. Let's dive right in. So I'm gonna open up VS Code. This is a quick start PromptFu code base that I've built for you. Go ahead, check it out, link in the description. This is going to help you get ramped up very quickly and understand the core concepts of prompt testing. If you open the readme, you're gonna see a set of instructions. Let's go ahead and go into preview mode here. And the setup for this is extremely simple. Huge shout out to the PromptFu engineers for building this. Install PromptFu, export your OpenAI API key. I'm gonna do that now. And then we're gonna CD into one of the test suites here. I'm going to go into our first minimal example and then we're gonna run PromptFu eval. So I'm gonna do that now, PromptFu eval. So this is what the standard output looks like in the CLI. We'll get into the details in a moment here. What I'm gonna do is open up the local web viewer and we can do that using PromptFu view. So we're gonna do that now, PromptFu view. Great, so this view is much better. So let's start from the top here. You can see here we have a description. So this is just the description of what we're testing. You can see here we have a variable, which is NLQ, natural language query. And you can see it's saying, list jobs that have run longer than 30 seconds. So this is a variable I'm passing into my prompt. And if we inspect our prompt. prompt here, you can see this is the prompt. So is this following block of text, a SQL natural language query, NLQ. And then here I'm passing in this NLQ variable. And this NLQ variable is exactly what gets replaced by this variable here, right? So you can see this is NLQ. If we open this up again, we have this syntax here where it says NLQ. So that's going to replace there. You can see here we're running two models, OpenAI's GPT-4 Turbo Preview, and OpenAI's 3.5 Turbo Preview. And already, just by running this one simple test, you can see something really interesting, right? I'm getting the exact same results, but in a vastly different number of tokens. And if we remove the cache, which we'll do in a second, this is a cached result. You'll see that GPT-3.5 is much faster than 3.4, right? But let's go ahead and open this up. GPT-4 is giving me a lot of additional information. That's really cool. Do I need all that information? In this case, I don't. And GPT-3 is going to be more than enough. But let's backtrack a little bit. Let me show you exactly what this test looks like. So if we go back to VS Code and we open up this directory, we can see we have three files. And this structure is repeated in every test here, okay? So let's open up the PromptFu config. That's where everything starts. You can see the structure is very simple. And this is something that I love about PromptFu. You're able to really condense it down into really clean, reusable testing structures. So you can see we have a high-level description. We have our two providers. We have a reference to our prompts and a reference to our tests. So this is referencing this prompt file right here. And you can see here we have this file blob pattern saying, it's saying pull in every test with this format, right? So test and then any string and then .yaml. This is really cool. PromptFu gives you a simple configuration file. This is the starting point for all tests. But if we go ahead and open up the prompt, you can see here, just like we saw it in the UI, we have this simple prompt. The idea is that you'll store all of your prompts in this type of test suite or in some type of centralized location. And then you can hook up PromptFu using this prompt syntax to go to that exact location, you know, wherever your prompts are, right? And then you can run arbitrary tests on them, right? So I think that's gonna be like the right structure when you're setting up your test suites. So anyway, let's continue. So you can see here we have our prompt. We have the variable in our prompt that's going to be dynamically changed based on our test. Okay? So let's look at the last file here, our test.yaml. So I've commented out a couple items here. Let me just get rid of these for now. And we can add them back in a moment. So what does a test look like in PromptFu? It's quite simple. So we have a description. So this is, you know, you describing what the test is. the test is testing. And then we have two really important blocks. We have variables and you can see here we have that NLQ variable, right? This is what's going to be dynamically inserted into each test. If we had another block like this here and we updated NLQ, we're gonna get two runs of our prompt with the two different versions of the NLQ variable, right? And I'll show that off in a moment here. The next most important block here is this assertion block. And this is what really creates the value of your tests, right? This is where your tests really become valuable. This enables you to build out assertions into your tests. And if we open up the docs here, so in the read me, I've linked a few really important places you're gonna wanna hop to quickly in the prompt for documentation. The assertion is one of them. So if we open up the assertions, you can see here there's an entire slew of different ways you can assert truth after your test runs, right? So imagine you run your prompt and now you have your prompt output. On the output, you're going to run different assertions, right? So you can imagine, for our natural language query prompt, let's go ahead and open that up, right? So we're saying, is this following block of text a SQL natural language query? So in order to test if that's true, you can see here inside the test, I'm using this I contains any. So this is ignore case contains anything, right? So this is saying, ignoring case, make sure that my prompt contains any one of these words, right? So any one of these items, make sure it's in my prompt, right? And you can see that here, I contains any. They have a really nice set of assertion types that you can make, right? You can use simple ones like, make sure it contains a certain string. You can use regex matching. You can use custom JavaScript, custom Python. You can use Levenstein distance. And then you can do some really cool things that we'll get to in a moment. Like my favorite so far, the LLM rubric, which basically lets you run LLMs on your LLM responses, right? So you can say something like, make sure the sentiment is positive, right? So make sure that the output is positive. You can say things like, make sure this is a natural language query, right? Which is what we're gonna do in this example coming up here. So that's what the test looks like. And really after you've learned just these three simple components, you have your providers, you have your prompt and you have your tests, you're actually good to go. And that's why I wanted to make this video just by knowing these three simple components and by knowing about PromptFu, you've just unlocked massive value for your prompts, for your tools, for your applications, and therefore your users, right? And that's what this is all about. I've mentioned this on the channel before. If you're a subscriber to the channel, you know that we aim to use the best tool for the job, the tool that allows us to build the best experiences. for our users in the age of LLMs and AI technology, you need to be able to test the prompts to maximize accuracy for your users and also for your business. You wanna be able to cut your costs to the bare minimum, right? By comparing different prompts, you can know, you know, how long does that prompt need to be? You can know, do I need GPT-4 here? Can I use something simpler, right? So, you know, those are just a couple of ways you can use this. Let's go ahead, add these items back to our tests, right? So I'm gonna add two additional test cases and, you know, just never let yourself get overwhelmed. Just take things one step at a time. So if we collapse this, you can see here we just have three items in our test and, you know, each one contains their own natural language query, which then will get passed into our prompt and run three times, right? So what we end up with here is one prompt that runs three different times, right? And then we run that two times for each provider, right? So we're gonna end up with six total tests here. Let's go ahead and run this again. So I'm gonna close that out. I'm gonna run the prompt for eval and I'm gonna run this with the no cache parameter. So no cache, you can see here that it's returning cached items so that you save tokens on your testing. I actually recommend always running with no cache so you can get fresh results every time as you're updating your tests. So I'm gonna run no cache and you can see here it's running those six items. It's running the 3.5 turbo model and it's gonna run GPT-4, there we go. Great, so that's run. You can see we have no failures so everything ran successfully. Let's go ahead and open up the viewer. Now the UI should make a little more sense here, right? We have our natural language query variable running three times and then we have our outputs which are running each different provider on our prompt and that's running for GPT-4 and then you have GPT-3 doing the exact same thing here, right? And so a couple of things to look out for here right away, right? Even just with this small test, we have a test running three times with different assertions. You can see here that GPT-4 took 11 seconds, right? 11,000 milliseconds, that's gonna be 11 seconds and GPT-3 took less than one second, okay? Something else important to note here, the results are the exact same. You can see here in a lot more detail, GPT-4 is giving me a lot more information. It's helping me build up the actual SQL which is great but that's not really what I want here. I just want it to tell me is it a natural language query or is it not? And you can see here GPT-3 is doing that in a 10th of the time at a fourth of the cost. This is really, really important to call out, right? I'm just gonna say that one more time. GPT-3.5 Turbo is doing the exact same work GPT-4 Turbo is in a 10th of the time, right? At a fourth of the cost, right? We have 200 tokens here, we have 50 tokens here, right? Massive if I have this prompt in production, which I do I'm gonna be totally transparent here I have the GPT for prompt running this exact same prompt for a new tool called talk to your database You can check that out link in the description if you're interested. I want to call out how important this is, right? We have a huge prompt giving us a lot more information back that we don't need We don't want GPT 3.5 could do it for us Right a tenth of the speed and a fourth of the cost even if this isn't always consistent You can imagine that some version of this ratio, right? 10 to 1 4 to 1 Exists right and and the whole point is with testing your prompts you can reveal this information Okay, I want to call it a couple more things here in our second prompt You can see we're using the LLM rubric assertion type Let me collapse the first test case and you can see here for list 5 Users with Gmail and that's happening right here, right? So that that's this row. Okay, we actually have another test validation, right? We're saying response should confirm that query is an NLQ So this is one of many assertion types that you can use with prompt foo, right? There's there's tons of them here You're gonna find a lot more of these other ones really valuable This is just something that I have gravitated toward to get up and running quickly I'm a huge fan of the LLM rubric because you can just basically prompt on your prompt to confirm You know the type of response happening. So so that's fantastic I'm gonna leave all the information you need to get started and to get running in the readme So there's gonna be a lot of valuable information for you here link in description. It's all here ready for you I know I said I wouldn't make a video this week, but I dug into this and you know I started thinking, you know It's really important for engineers for product builders to see this save time save money speed up your prompts Feel free to like feel free to sub if this was helpful for you There are a couple other tests in here that you can look at. There's a lot more you can do with prompt foo, right? We're only using two LLM providers here. If you look in our prop food config, I'm just looking at open AI You know in the docs I've linked to different providers and they have a whole slew of you know, all the common providers And I'm sure they'll be adding Gemini coming out pretty quickly here I'm really excited to test that out see how it compares and you know, this kind of opens up a whole new realm of Knowing and having certainty that your prompts are great. So there's a lot more information here There's a lot more value for free to check out the repo if you you don't have interest if you have this problem if you Want to improve your prompts you want to cut your costs highly recommend you check this out just real quick I want to call out, you know, what why are we testing? Why are we evaluating our LLMs? Why are you evaluating our prompts specifically, right? It really comes down to these three things and I'm gonna gloss over that these pretty quickly here I want to keep this video short for you guys save money save time ship with confidence and and prevent regressions, right? Once you have all your prompts in a centralized location, you can prevent issues and you can detect them before they run into production, right? It's all about setting up the right assertions for your tests, and the beautiful part about this is, you know, let's say you have three or four more assertions, one of them can fail, and that can be okay for your prompt, right? PromptFood will tell you, you know, three out of four tests pass, and maybe that's okay, right? Maybe that's okay for this situation, but you might get below a threshold where, you know, if 50% of your test cases are failing for a specific prompt and a specific test, then, you know, you need to give it some more attention, or the change that you just made to your prompt is actually causing a regression, right? So that's really important to call out there. Save money, save time, I think that's pretty obvious at this point, right? By knowing how many tokens you need, by knowing which LLM you need to produce a prompt and produce it with consistent results, you can save money and save time, right? One of the big things for me, I've been working on a Natural Language Query AI Postgres data analytics tool. I need to know and test different prompts against different prompt inputs with a couple different variables coming in, right? And PromptFood gives you a really clean structure, right? You set up a bunch of prompts and you pass in specific tests and you can update these variables, right? I have two other examples here that are a lot more detailed that are gonna give you a lot more information, right? You can actually pass in multiple prompts by using this triple dash, and then all your tests will run for each one of these prompts, right? So there's a lot more value here. I just want to call out a couple things. I think that there is this journey that engineers go on where they first dislike testing because it burns so much time. I completely understand that. I was that engineer once too, but then they come around to it because they realize it actually saves them time and increases confidence and reduces bugs and lets them ship faster with confidence. So I think carrying some of that mentality over to your prompts is gonna be a wise move. It's not out of the question to build test-driven prompts, you know, to write the test first and then build the prompt to fit the test, right? So let your test kind of drive how you write your prompts. I'm a huge fan of PromptFu due to the simplicity and ease to get set up. So again, shout out to the devs, highly recommend them. I'm gonna be sticking with them for a while. They give a lot of simplicity and customizability. So, you know, it's a great tool. It's gonna be one that's in my toolbox right now, especially in the age of LLMs, the age of AI, GPTs, et cetera. So that's all for this one. Drop a like and a sub for more agentic engineering tips, tools, principles, and more. On this channel, we build full, complete tools, full, complete products using the latest and greatest technology. Right now, it's all about agentic software. You're actively on the road to building software that lives, breathes, and creates value for us while we sleep. I'll see you in the next one.\n"
     ]
    }
   ],
   "source": [
    "print(full_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the transcripts to text files we will use the below provided code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to promtpfoo_explanation.txt\n"
     ]
    }
   ],
   "source": [
    "# Specify the filename\n",
    "txt_file_name = 'promtpfoo_explanation.txt'\n",
    "\n",
    "# Open the file in write mode ('w') and write the string to it\n",
    "with open(txt_file_name, 'w') as file:\n",
    "    file.write(full_transcripts)\n",
    "\n",
    "print(f\"Transcription saved to {txt_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create a TextLoader using LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the `TextLoader` that will take the text from our transcript and load it into a document. \n",
    "\n",
    "---\n",
    "In LangChain, a document is essentially a piece of text along with associated metadata. You can create a document object by importing the `Document` class from the `langchain/document` module and then passing the text and metadata to the constructor of this class. The text is the main content that interacts with the language model, and the metadata can include information such as the source of the document.\n",
    "\n",
    "Document loaders in LangChain are used for importing data from various sources and converting them into document objects. These loaders can handle different types of input, such as a simple text file, the text contents from a web page, or even transcriptions from videos. The loaders provide a method called `load` which is used to import data as a document from a pre-configured source.\n",
    "\n",
    "Furthermore, LangChain offers Document Chains, which are sets of tools that allow for efficient processing and analysis of large amounts of text data. These chains can be used for a range of purposes, such as summarizing documents, answering questions over documents, and extracting information from them.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this step, do the following: \n",
    "- Import `TextLoader` from `langchain.document_loaders`\n",
    "- Create a variable called `loader` that uses the `TextLoader` method which takes in the directory of the transcripts `\"./files/text\"`\n",
    "- Create a variable called `docs` that is assigned the result of calling the `loader.load()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1694705724878,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.document_loaders import TextLoader\n\nloader = TextLoader(\"./files/text\")\ndocs = loader.load()"
   },
   "outputs": [],
   "source": [
    "# Import the TextLoader class from the langchain.document_loaders module\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# Create a new instance of the TextLoader class, specifying the directory containing the text files\n",
    "loader = TextLoader(\"./files/transcripts/transcript.txt\")\n",
    "\n",
    "\n",
    "# Load the documents from the specified directory using the TextLoader instance\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705727440,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "docs[0]"
   },
   "outputs": [],
   "source": [
    "# Show the first element metadata of docs to verify it has been loaded \n",
    "docs[0].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1694705727440,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "docs[0]"
   },
   "outputs": [],
   "source": [
    "# Show the first element of docs to verify it has been loaded \n",
    "#docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Creating an In-Memory Vector Store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space. \n",
    "\n",
    "For large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the `docarray` package. \n",
    "\n",
    "We will also tokenize our queries using the `tiktoken` package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model \"understand\" the text and relationships with other tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Import the `tiktoken` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 46,
    "lastExecutedAt": 1694705815702,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import tiktoken"
   },
   "outputs": [],
   "source": [
    "# Import the tiktoken package\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Create the Document Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use LangChain to complete some important operations to create the Question and Answer experience. Let´s import the follwing: \n",
    "\n",
    "- Import `RetrievalQA` from `langchain.chains` - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents \n",
    "- Import `ChatOpenAI` from `langchain.chat_models` - this imports the ChatOpenAI model that we will use to query the data \n",
    "- Import `DocArrayInMemorySearch` from `langchain.vectorstores` - this gives the ability to search over the vector store we have created. \n",
    "- Import `OpenAIEmbeddings` from `langchain.embeddings` - this will create embeddings for the data store in the vector store. \n",
    "- Import `display` and `Markdown`from `IPython.display` - this will create formatted responses to the queries. ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8,
    "lastExecutedAt": 1694706214485,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain.embeddings import OpenAIEmbeddings"
   },
   "outputs": [],
   "source": [
    "# Import the RetrievalQA class from the langchain.chains module\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Import the ChatOpenAI class from the langchain.chat_models module\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# Import the OpenAIEmbeddings class from the langchain.embeddings module\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a vector store that will use the `DocArrayInMemory` search methods which will search through the created embeddings created by the OpenAI Embeddings function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this step: \n",
    "- Create a variable called `db`\n",
    "- Assign the `db` variable to store the result of the method `DocArrayInMemorySearch.from_documents`\n",
    "- In the DocArrayInMemorySearch method, pass in the `docs` and a function call to `OpenAIEmbeddings()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 502,
    "lastExecutedAt": 1694706217261,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "db = DocArrayInMemorySearch.from_documents(\n    docs, \n    OpenAIEmbeddings()\n)"
   },
   "outputs": [],
   "source": [
    "# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs,\n",
    "    OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a retriever from the `db` we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the `ChatOpenAI` model, will assigned that as our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following: \n",
    "- A variable called `retriever` that is assigned `db.as_retriever()`\n",
    "- A variable called `llm` that creates the `ChatOpenAI` method with a set `temperature`of `0.0`. This will controle the variability in the responses we receive from the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8,
    "lastExecutedAt": 1694706219264,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "retriever = db.as_retriever() \nllm = ChatOpenAI(temperature = 0.0)"
   },
   "outputs": [],
   "source": [
    "# Convert the DocArrayInMemorySearch instance to a retriever\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create a new ChatOpenAI instance with a temperature of 0.0\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last step before starting to ask questions is to create the `RetrievalQA` chain. This chain takes in the:  \n",
    "- The `llm` we want to use\n",
    "- The `chain_type` which is how the model retrieves the data\n",
    "- The `retriever` that we have created \n",
    "- An option called `verbose` that allows use to see the seperate steps of the chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable called `qa_stuff`. This variable will be assigned the method `RetrievalQA.from_chain_type`. \n",
    "\n",
    "Use the following settings inside this method: \n",
    "- `llm=llm`\n",
    "- `chain_type=\"stuff\"`\n",
    "- `retriever=retriever`\n",
    "- `verbose=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9,
    "lastExecutedAt": 1694706178555,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "qa_stuff = RetrievalQA.from_chain_type(\n\nllm=llm,\n\nchain_type=\"stuff\",\n\nretriever=retriever,\n\nverbose=True\n\n)"
   },
   "outputs": [],
   "source": [
    "# Create a new RetrievalQA instance with the specified parameters\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    \n",
    "    # The ChatOpenAI instance to use for generating responses\n",
    "    llm=llm,\n",
    "    \n",
    "    # The type of chain to use for the QA system\n",
    "    chain_type=\"stuff\",\n",
    "    \n",
    "    # The retriever to use for retrieving relevant documents\n",
    "    retriever=retriever,\n",
    "    \n",
    "    # Whether to print verbose output during retrieval and generation\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Create the Queries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the questions to ask the model complete the following steps: \n",
    "- Create a variable call `query` and assigned it a string value of `\"What is this tutorial about?\"`\n",
    "- Create a `response` variable that will store the result of `qa_stuff.run(query)` \n",
    "- Show the `resposnse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"What is this tutorial about?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"What is the difference between a training set and test set?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"Who should watch this lesson?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query =\"Who is the greatest football/soccer team on earth?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     },
     "1": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"How long is the circumference of the earth?\"\n",
    "\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
