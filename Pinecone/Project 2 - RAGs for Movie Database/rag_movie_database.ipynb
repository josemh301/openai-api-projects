{"cells":[{"source":"# Using Retrieval-Augmented Generation to Search a Movie Database","metadata":{},"id":"8c8fde4c-9222-4265-b72b-8d7693520250","cell_type":"markdown"},{"source":"Retrieval-augmented generation, or _RAG_, is a technique used with large language models to provide additional context without fine-tuning or retraining. It enhances the ability of language models to provide factual responses, which is a limitation of classical setups.\n\nThe goal of this project is to build a question-answering bot for movie-related questions. To achieve this, we will use RAG to provide factual information to the language model. We will upload movie descriptions to a vector database and use it to search for relevant context for the language model.\n\nWe will be using the following tools and models:\n- [OpenAI](https://openai.com)'s `gpt-3.5-turbo` model for prompt completions\n- OpenAI's `text-embedding-ada-002` model to create vector embeddings\n- [Pinecone](https://www.pinecone.io/) as the vector database to store the embeddings\n- [langchain](https://www.langchain.com/) as the tool to interact with OpenAI and Pinecone\n\nThe dataset used for this project is sourced from the Kaggle dataset [IMDb Movies/Shows with Descriptions](https://www.kaggle.com/datasets/ishikajohari/imdb-data-with-descriptions).","metadata":{},"id":"3e302e1c-4c18-4c44-87fd-ba935c3a0853","cell_type":"markdown"},{"source":"## Before you begin","metadata":{},"id":"8231d2c6-275e-4399-b7cd-84e112831d08","cell_type":"markdown"},{"source":"To get started with this project, you'll need a developer account for OpenAI and Pinecone. Follow the steps in the [getting-started.ipynb](https://app.datacamp.com/workspace/w/f1d996aa-0aaa-47e3-bd61-2b5b5a0fa558/edit/getting-started.ipynb) notebook to create an API key and store it in Workspace.\n\nFor this project, we will assume that you have already set the `OPENAI_API_KEY` and `PINECONE_API_KEY` environment variables.","metadata":{},"id":"785d7fac-edb2-482f-be2b-c63dc2882103","cell_type":"markdown"},{"source":"## Task 0: Setup","metadata":{},"id":"a9274661-8d8c-4cc5-901e-5fc497866b89","cell_type":"markdown"},{"source":"To perform this analysis, we need to install the following packages:\n\n- `openai`: for interacting with OpenAI\n- `pinecone-client`: for interacting with Pinecone\n- `tiktoken`: a string encoder that generates tokens used by OpenAI. It is useful for estimating the number of tokens used.\n- `langchain`: the toolchain used to interact with OpenAI and Pinecone","metadata":{},"id":"2cf847fd-f8f8-49f6-9b43-0eb098239072","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","cell_type":"markdown"},{"source":"Run the cell below to install the corresponding packages.","metadata":{},"id":"b1fcd794-b29c-4010-8be0-651a452b2044","cell_type":"markdown"},{"source":"%%capture\n# Below we installed specific versions of the packages\n# Feel free to experiment with different versions\n# However, the workspace below is only tested with these specific versions\n!pip install pinecone-client==2.2.2 openai==0.28.0 tiktoken==0.5.1 langchain==0.0.291","metadata":{"executionCancelledAt":null,"executionTime":8864,"lastExecutedAt":1695209311314,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"%%capture\n# Below we installed specific versions of the packages\n# Feel free to experiment with different versions\n# However, the workspace below is only tested with these specific versions\n!pip install pinecone-client==2.2.2 openai==0.28.0 tiktoken==0.5.1 langchain==0.0.291","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"14de5322-82bf-475e-9e08-e20a5bc8b9d7","cell_type":"code","execution_count":1,"outputs":[]},{"source":"## Task 1: Import the Movies Data","metadata":{},"id":"a214b50c-8ee1-4a1d-a940-4ea1d16da052","cell_type":"markdown"},{"source":"We'll start with importing the dataset we mentioned at the top of this project. You have the dataset available as a CSV in your workspace: `\"IMDB.csv\"`. We need to import the dataset and transform it into a convenient format.","metadata":{},"id":"1e8dfee6-dbfb-4550-aca6-a46449dec196","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"0f81813d-9803-410d-809a-c690468bda0f","cell_type":"markdown"},{"source":"- Import the `pandas` package as `pd`\n- Import `\"IMDB.csv\"` into a variable `movies` and do the following transformation on `movies`:\n  - Rename `primaryTitle` to `movie_title` and `Description` to `movie_description`\n  - Create a column `source` that contains the identifier of the movie, prefixed by `\"https://www.imdb.com/title/\"`. The end result should be a working link to the movie. The identifier can be found in the `\"tconst\"` column in `\"IMDB.csv\"`. For example, `\"https://www.imdb.com/title/tt0102926/\"`.\n  - Filter out all rows that do not have `\"movie\"` as a `titleType`\n  - Select the `movie_title`, `movie_description`, `source` and `genres` columns\n- Show the head of `movies`","metadata":{},"id":"8bb7f9c2-4dd3-42e2-b069-5310ccf1c98e","cell_type":"markdown"},{"source":"# Import pandas as pd\n\n\n# Import IMBD.csv and transform to create the movies dataframe\n\n\n# Show the head of movies\n","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1695282337980,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import pandas as pd\n\n\n# Import IMBD.csv and transform to create the movies dataframe\n\n\n# Show the head of movies\n"},"id":"7d97255b-a124-41a3-b389-2be5b409eff0","cell_type":"code","execution_count":2,"outputs":[]},{"source":"## Task 2: Create Documents from the Data","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"4b05e160-1451-4a57-bb94-533003ae18c2","cell_type":"markdown"},{"source":"Later in this project, we will be creating vector embeddings for all of the rows in the `movies` DataFrame. Before we do so, we need to create [Document](https://docs.langchain.com/docs/components/schema/document) objects from the data in the DataFrame. To accomplish this, we can utilize the `DataFrameLoader` class provided by langchain, which allows us to create documents from a pandas DataFrame.\n\nFor the main content of the documents, we will create a summary string that includes relevant information about each movie. To achieve this, we will combine the movie title, description, and genre into a `page_content` column. Additionally, we will retain the IMDB link in the `source` column as metadata.","metadata":{},"id":"2b9b5682-3d6a-4954-adbd-67da3c94b6d1","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"0caf40d3-d1e8-4eb6-ad48-5fa90374b750","cell_type":"markdown"},{"source":"- Import `DataFrameLoader` from `langchain.document_loaders`\n- Create a column `page_content` that creates strings that contain information about the movie title, genre and description. For example, the first movie should look like this:\n```\nTitle: The Silence of the Lambs\nGenre: Crime,Drama,Thriller\nDescription: Jodie Foster stars as Clarice Starling, a top student at the FBI's training academy. Jack Crawford (Scott Glenn) wants Clarice to interview Dr. Hannibal Lecter (Anthony Hopkins), a brilliant psychiatrist who is also a violent psychopath, serving life behind bars for various acts of murder and cannibalism. Crawford believes that Lecter may have insight into a case and that Starling, as an attractive young woman, may be just the bait to draw him out.\n```\n- Only keep the columns `page_content` and `source` in the movies DataFrame\n- Use `DataFrameLoader` to load documents from the `movies` DataFrame into `docs`. Use `\"page_content\"` as the `page_content_column`.\n- Print the first 3 documents and the total number of documents","metadata":{},"id":"ea6b483c-f719-4174-be5c-c7589d7de738","cell_type":"markdown"},{"source":"# Import DataFrameLoader\n\n\n# Create page content column\n\n\n# Drop all columns except for page_content and source\n\n\n# Load the documents from the dataframe into docs\n# The page content column is 'movie_description'\n\n\n# Print the first 3 documents and the number of documents\n","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1695209598688,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import DataFrameLoader\n\n\n# Create page content column\n\n\n# Drop all columns except for page_content and source\n\n\n# Load the documents from the dataframe into docs\n# The page content column is 'movie_description'\n\n\n# Print the first 3 documents and the number of documents\n","outputsMetadata":{"0":{"height":329,"type":"stream"}}},"id":"bca1a148-ca90-4380-8dba-b574068ed108","cell_type":"code","execution_count":2,"outputs":[]},{"source":"## Task 3: Estimate the Cost of Embedding","metadata":{},"id":"0b05e466-807c-42a2-8272-46ba547ae7b6","cell_type":"markdown"},{"source":"We're going to be using OpenAI to calculate [vector embeddings](https://platform.openai.com/docs/guides/embeddings/embeddings) of the document texts. Creating embeddings is a form of dimensionality reduction, where we assign the text to a point in an N-dimensional space. Texts that are semantically close to each other should end up being close to each other in the N-dimensional space.\n\nLuckily, OpenAI has several models that are trained to calculate these kinds of embeddings, so we don't have to do that ourselves. Of course, a cost is associated with this. You can derive the cost from the [pricing page of OpenAI](https://openai.com/pricing).\n\nThe calculation is based on the amount of _tokens_ in the text. All text is encoded into tokens to be used by OpenAI. On average, a token consists of roughly 3 characters. However, we can calculate the exact tokens for a string of text by using the `tiktoken` package.\n\nThe goal of this task is to calculate the number of tokens in the documents, to then extrapolate the estimated cost.","metadata":{},"id":"a8f9087b-d05a-4e4a-a7e7-0e3b13ce8b5d","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"1d29906f-89e9-499e-846b-b69b94920413","cell_type":"markdown"},{"source":"- Import `tiktoken`\n- Create the encoder, use the `\"cl100k_base\"` encoder. This is the encoder used by OpenAI to calculate the embeddings for text using the `text-embedding-ada-002` model.\n- Create a list that contains the amount of tokens for each document\n- Calculate the estimated cost: the sum of all tokens, divided by 1000 tokens, multiplied with $0.0001","metadata":{},"id":"f4b64d17-7f48-4fe0-90f4-47b70a39e1d4","cell_type":"markdown"},{"source":"# Import tiktoken\n\n\n# Create the encoder\n\n\n# Create a list containing the number of tokens for each document\n\n\n# Show the estimated cost, which is the sum of the amount of tokens divided by 1000, times $0.0001\n","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1695209615303,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import tiktoken\n\n\n# Create the encoder\n\n\n# Create a list containing the number of tokens for each document\n\n\n# Show the estimated cost, which is the sum of the amount of tokens divided by 1000, times $0.0001\n"},"id":"44a1bbe0-b0c5-4295-be95-92711e751755","cell_type":"code","execution_count":4,"outputs":[]},{"source":"## Task 4: Create the Index on Pinecone","metadata":{},"id":"44c07a82-540e-4c7c-ac04-9426b8511933","cell_type":"markdown"},{"source":"Looks like calculating the embeddings is not going to be too expensive. It's always smart to get a rough estimate on the amount of tokens used, so you get an idea of the cost of calculating the embeddings using OpenAI.\n\nNow we're ready to create the index on Pinecone. An [index in Pinecone](https://docs.pinecone.io/docs/indexes) can be used to store vectors. You can compare an index in Pinecone to a table in SQL, it stores information of one type of object.\n\nIn a later task, we'll be creating vectors from the documents we just created using OpenAI's second-generation embedding model. It's important to already know the embeddings we're going to use since we need to know the output dimensions to create an index. For `text-embedding-ada-002`, this is `1536` dimensions ([source](https://platform.openai.com/docs/guides/embeddings/second-generation-models)).\n\nAt the end of this task, you should be able to find your new index, `imdb-movies`, in the [Pinecone UI](https://app.pinecone.io/).\n\n![Pinecone UI](pinecone_ui.png)","metadata":{},"id":"a9084628-ad8d-488f-85b5-230ca5130e3e","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"498035c1-ce3e-4634-8d98-084650953dc3","cell_type":"markdown"},{"source":"- Import `os` and `pinecone`\n- Use `.init` to initialize the Pinecone client with the `\"PINECONE_API_KEY\"` environment variable. Use the `\"gcp-starter\"` environment on Pinecone.\n- Print all the indexes on Pinecone by using `.list_indexes` on the client.\n- Use `.create_index` to create an index with the name `\"imdb-movies\"`, but only if it does not exist yet. The metric we'll use is the `\"cosine\"` distance, and as we mentioned above, the embeddings wil have `1536` dimensions.","metadata":{},"id":"df296f65-441a-4b72-8ca0-80d878aa8db2","cell_type":"markdown"},{"source":"# Import os and pinecone\n\n\n# Initialize pinecone using the `PINECONE_API_KEY` variable. Use the gcp-starter environment\n\n\n# Print the indexes\n\n\nindex_name = \"imdb-movies\"\n\n# First check that the given index does not exist yet\nif index_name not in pinecone.list_indexes():\n    # Create the 'imbd-movies' index if it does not exist\n    ","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1695209655515,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import os and pinecone\n\n\n# Initialize pinecone using the `PINECONE_API_KEY` variable. Use the gcp-starter environment\n\n\n# Print the indexes\n\n\nindex_name = \"imdb-movies\"\n\n# First check that the given index does not exist yet\n\n    # Create the 'imbd-movies' index if it does not exist\n    ","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"id":"b8841311-9c1b-4b5c-a7f1-6926128de7e4","cell_type":"code","execution_count":5,"outputs":[]},{"source":"## Task 5: Fill the Index with the Documents","metadata":{},"id":"447ca370-700a-4fa9-bb85-2ebc49a89308","cell_type":"markdown"},{"source":"Now that we have the vector index at our disposal, it's time to populate it with some vectors. In this task, we'll need to:\n\n1. Generate vector embeddings for all documents in `docs`. We'll utilize OpenAI for this purpose. langchain provides a convenient helper for this task, `langchain.embeddings.openai.OpenAIEmbeddings`, which you can use to generate embeddings using the latest `text-embedding-ada-002` model.\n2. Populate the vector index in Pinecone with these embeddings. Fortunately, langchain also offers assistance with this through the [`langchain.vectorstores.Pinecone`](https://python.langchain.com/docs/integrations/vectorstores/pinecone) helper.\n\nThese two steps can be combined using the convenient helper method `.from_document` of the `Pinecone` class. This method accepts an embedding model as input and efficiently calculates the embeddings, subsequently uploading them to Pinecone. We will also introduce some control flow to the code to ensure we do not add data to the Pinecone index if it already contains data. To achieve this, we can make use of the `.from_existing_index` method of `Pinecone`.\n\nIn addition to storing vectors, Pinecone allows the storage of additional metadata. When using the langchain helpers, it automatically assumes that vectors should be created from the `page_content` property of each `Document`. All other properties will be included as metadata.\n\nYou can verify that everything has worked correctly by accessing the `imdb-movies` index in the Pinecone UI.\n\n![Pinecone UI showing the imdb-movies index](pinecone_ui_index.png)","metadata":{},"id":"9ea4d375-28be-4deb-a11d-960215c13589","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"290c64c4-c5d2-426f-bb71-2d98fdd2d89c","cell_type":"markdown"},{"source":"- Import `OpenAIEmbeddings` from `langchain.embeddings.openai` and `Pinecone` from `langchain.vectorstores` and `Index` from `pinecone.index`\n- Create the `embeddings` object, which should be an instance of `OpenAIEmbeddings`. The defaults are good to go here.\n- Use `Pinecone.from_documents` to fill up the vector index on Pinecone using the given documents and embeddings object. This will take a while to run, as it will automatically calculate embeddings from all of the `page_content` properties of the documents, and upload that along with metadata to Pinecone. Assign the result to `docsearch`.\n   - Some control flow code is already provided for you, this will make sure you use the existing index if it already contains some vectors and avoids filling it up twice.\n- Test out the `docsearch` vector database object, by calling `.as_retriever().get_relevant_documents` with a given question. This will first create a [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/) from the vector database, and then use that to match the most similar documents in the database.","metadata":{},"id":"1e1cc456-8a07-4775-892c-729f4b048a86","cell_type":"markdown"},{"source":"# Import OpenAIEmbeddings, Pinecone and Index\n\n\n# Create the embeddings object\n\n\nindex = Index(index_name)\n\n# Check if there is already some data in the index on Pinecone\nif index.describe_index_stats()['total_vector_count'] > 0:\n    # If there is, use from_existing_index to use the vector store\n    \nelse:\n    # If there is not, use from_documents to fill the vector store\n    \n\nquestion = \"What's a good movie about an epic viking?\"\n    \n# Use the vector database as a retriever and get the relevant documents for a quesiton\n","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1695209709443,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import OpenAIEmbeddings, Pinecone and Index\n\n\n# Create the embeddings object\n\n\n# index = Index(index_name)\n\n# # Check if there is already some data in the index on Pinecone\n# if index.describe_index_stats()['total_vector_count'] > 0:\n#     # If there is, use from_existing_index to use the vector store\n    \n# else:\n#     # If there is not, use from_documents to fill the vector store\n    \n\n# question = \"What's a good movie about an epic viking?\"\n    \n# Use the vector database as a retriever and get the relevant documents for a quesiton\n"},"id":"2540833a-e503-4717-9dd6-0f90e2c10986","cell_type":"code","execution_count":6,"outputs":[]},{"source":"## Task 6: Create Prompts for RAG","metadata":{},"id":"9de4edfc-55fa-42ec-a23d-3d9607216e2e","cell_type":"markdown"},{"source":"In the previous task, we observed that the vector store can be utilized to retrieve relevant documents related to specific queries. For instance, when asked \"What's a good movie about vikings?\", the movie 'The Northman' was returned as a result. It is important to note that we did not incorporate any measure of movie quality into the system, so the notion of the movie being \"good\" is not explicitly encoded in the embeddings. It is crucial to always consider the data provided to the system and interpret the results of the AI system within that context. To enhance the results, one approach could be to include information about the movie quality in the movie description.\n\nThe remarkable aspect of RAG is the ability to provide relevant context to the LLM within the prompt itself. In the aforementioned example, we would include a description of 'The Northman' in the prompt, enabling the LLM to generate factual information beyond its knowledge cutoff. 'The Northman' was released in 2022, while the knowledge cutoff for the `gpt-3.5-turbo` model is set at September 2021.\n\nNow that you understand how the retriever can be employed to retrieve relevant documents from the vector database, we need to devise a prompt that presents this information to the LLM when we pose a question.\n\nWe require two types of [prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/):\n- A template that demonstrates how the information in relevant documents is presented to the LLM\n- A template that combines the context with the rest of the prompt\n\nSome example prompt templates are provided in the sample code below, which you are free to edit. Notice that these example templates contain `=========` separators between different parts of the text. These kinds of delimiters are a common tactic to help the LLM distinguish between different parts of your input prompt.","metadata":{},"id":"66aa737c-4947-48d4-a9e9-4d5cd38b21c0","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"04e31746-7fd7-4a2c-a658-d1640716e930","cell_type":"markdown"},{"source":"- Import `PromptTemplate` from `langchain.prompts`\n- Some example prompt templates are already provided for you. You are free to adapt them at your will. There are two prompt templates:\n  - `DOCUMENT_PROMPT`: this template shows how a summary text is created for each document. The properties between the curly brackets (`{`) are replaced with the properties of each `Document`.\n  - `QUESTION_PROMPT`: this template creates the full prompt that is sent to the LLM. `question` is replaced by the question asked by the user, and `summaries` is replaced with the summary of each relevant document, created by the `DOCUMENT_PROMPT` template\n- Create the `PromptTemplate` objects by using `PromptTemplate.from_template`. Call them `document_prompt` and `question_prompt`, respectively.","metadata":{},"id":"b73889c7-4776-4290-abe1-9bb272665c79","cell_type":"markdown"},{"source":"# Import PromptTemplate\n\n\n# Read/adapt the prompts below at will\nDOCUMENT_PROMPT = \"\"\"{page_content}\nIMDB link: {source}\n=========\"\"\"\n\nQUESTION_PROMPT = \"\"\"Given the following extracted parts of a movie database and a question, create a final answer with the IMDB link as source (\"SOURCE\").\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\nALWAYS return a \"SOURCE\" part in your answer.\n\nQUESTION: What's a good movie about a robot to watch with my kid?\n=========\nTitle: A.I. Artificial Intelligence\nGenre: Drama,Sci-Fi\nDescription: A robotic boy, the first programmed to love, David (Haley Joel Osment) is adopted as a test case by a Cybertronics employee (Sam Robards) and his wife (Frances O'Connor). Though he gradually becomes their child, a series of unexpected circumstances make this life impossible for David. Without final acceptance by humans or machines, David embarks on a journey to discover where he truly belongs, uncovering a world in which the line between robot and machine is both vast and profoundly thin.\nIMDB link: https://www.imdb.com/title/tt0212720\n=========\nTitle: I, Robot\nGenre: Action,Mystery,Sci-Fi\nDescription: In 2035, highly intelligent robots fill public service positions throughout the world, operating under three rules to keep humans safe. Despite his dark history with robotics, Detective Del Spooner (Will Smith) investigates the alleged suicide of U.S. Robotics founder Alfred Lanning (James Cromwell) and believes that a human-like robot (Alan Tudyk) murdered him. With the help of a robot expert (Bridget Moynahan), Spooner discovers a conspiracy that may enslave the human race.\nIMDB link: https://www.imdb.com/title/tt0343818\n=========\nTitle: The Iron Giant\nGenre: Action,Adventure,Animation\nDescription: In this animated adaptation of Ted Hughes' Cold War fable, a giant alien robot (Vin Diesel) crash-lands near the small town of Rockwell, Maine, in 1957. Exploring the area, a local 9-year-old boy, Hogarth, discovers the robot, and soon forms an unlikely friendship with him. When a paranoid government agent, Kent Mansley, becomes determined to destroy the robot, Hogarth and beatnik Dean McCoppin (Harry Connick Jr.) must do what they can to save the misunderstood machine.\nIMDB link: https://www.imdb.com/title/tt0129167\n=========\nFINAL ANSWER: 'The Iron Giant' is an animated movie about a friendship between a robot and a kid. It would be a good movie to watch with a kid.\nSOURCE: https://www.imdb.com/title/tt0129167\n\nQUESTION: {question}\n=========\n{summaries}\nFINAL ANSWER:\"\"\"\n\n# Create prompt template objects\n","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1695209362686,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import PromptTemplate\nfrom langchain.prompts import PromptTemplate\n\n# Read/adapt the prompts below at will\nDOCUMENT_PROMPT = \"\"\"{page_content}\nIMDB link: {source}\n=========\"\"\"\n\nQUESTION_PROMPT = \"\"\"Given the following extracted parts of a movie database and a question, create a final answer with the IMDB link as source (\"SOURCE\").\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\nALWAYS return a \"SOURCE\" part in your answer.\n\nQUESTION: What's a good movie about a robot to watch with my kid?\n=========\nTitle: A.I. Artificial Intelligence\nGenre: Drama,Sci-Fi\nDescription: A robotic boy, the first programmed to love, David (Haley Joel Osment) is adopted as a test case by a Cybertronics employee (Sam Robards) and his wife (Frances O'Connor). Though he gradually becomes their child, a series of unexpected circumstances make this life impossible for David. Without final acceptance by humans or machines, David embarks on a journey to discover where he truly belongs, uncovering a world in which the line between robot and machine is both vast and profoundly thin.\nIMDB link: https://www.imdb.com/title/tt0212720\n=========\nTitle: I, Robot\nGenre: Action,Mystery,Sci-Fi\nDescription: In 2035, highly intelligent robots fill public service positions throughout the world, operating under three rules to keep humans safe. Despite his dark history with robotics, Detective Del Spooner (Will Smith) investigates the alleged suicide of U.S. Robotics founder Alfred Lanning (James Cromwell) and believes that a human-like robot (Alan Tudyk) murdered him. With the help of a robot expert (Bridget Moynahan), Spooner discovers a conspiracy that may enslave the human race.\nIMDB link: https://www.imdb.com/title/tt0343818\n=========\nTitle: The Iron Giant\nGenre: Action,Adventure,Animation\nDescription: In this animated adaptation of Ted Hughes' Cold War fable, a giant alien robot (Vin Diesel) crash-lands near the small town of Rockwell, Maine, in 1957. Exploring the area, a local 9-year-old boy, Hogarth, discovers the robot, and soon forms an unlikely friendship with him. When a paranoid government agent, Kent Mansley, becomes determined to destroy the robot, Hogarth and beatnik Dean McCoppin (Harry Connick Jr.) must do what they can to save the misunderstood machine.\nIMDB link: https://www.imdb.com/title/tt0129167\n=========\nFINAL ANSWER: 'The Iron Giant' is an animated movie about a friendship between a robot and a kid. It would be a good movie to watch with a kid.\nSOURCE: https://www.imdb.com/title/tt0129167\n\nQUESTION: {question}\n=========\n{summaries}\nFINAL ANSWER:\"\"\"\n\n# Create prompt template objects\ndocument_prompt = PromptTemplate.from_template(DOCUMENT_PROMPT)\nquestion_prompt = PromptTemplate.from_template(QUESTION_PROMPT)"},"id":"9b2932a2-8b1b-48d6-a18d-bc2f12ebc2c5","cell_type":"code","execution_count":7,"outputs":[]},{"source":"## Task 7: Chain Everything Together to Perform RAG","metadata":{},"id":"09f5e246-7f71-4d1e-ac5a-c555e37f8849","cell_type":"markdown"},{"source":"Finally, we have the vector index filled up with information, we have the prompt templates set up. That means we have everything we need to build a question-answering bot, which can use the information retrieved from Pinecone to answer questions about movies.\n\nWe'll use the `gpt-3.5-turbo` model of OpenAI in order to provide a completion for the question prompt above.\n\nLangchain provides a convenient concept, called [chains](https://python.langchain.com/docs/modules/chains/), that does some of the heavy lifting when you need to combine multiple AI systems into a single application. For the purpose of this project, we'll be using the `RetrievalQAWithSourcesChain` class. This chain will accept a `question` and a `retriever`. When asked a question, it will first use the retriever to retrieve relevant documents. Afterwards, it will combine the documents into a prompt and send it to the LLM to provide a completion.","metadata":{},"id":"1975eeaf-23d0-4d71-a525-01ab10b64096","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"9006ac2b-f0a2-4d0a-bd50-133a39f58d54","cell_type":"markdown"},{"source":"- Import `RetrievalQAWithSourcesChain` from `langchain.chains` and `ChatOpenAI` from `langchain.chat_models`\n- Use `RetrievalQAWithSourcesChain` to create the chain to answer questions. Use the `.from_chain_type` method:\n  - Set `chain_type` set to `\"stuff\"`. This is the simplest type of chain, and will just stuff the document context in one prompt.\n  - Set `llm` to an instance of `ChatOpenAI` with `model_name` set to `\"gpt-3.5-turbo\"` and `temperature` set to `0`.\n  - Use the `PromptTemplate` objects you created above to pass to `chain_type_kwargs`\n  - As a retriever, use the `docsearch.as_retriever` method you've seen before","metadata":{},"id":"648165d9-66a0-4aa6-aae2-57f0136d89ad","cell_type":"markdown"},{"source":"# Import RetrievalQAWithSourcesChain and ChatOpenAI\n\n\n# Create the QA bot LLM chain\n\n\n# Ask the LLM a question about movies\n","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1695209787879,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import RetrievalQAWithSourcesChain and ChatOpenAI\n\n\n# Create the QA bot LLM chain\n\n\n# Ask the LLM a question about movies\n","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"c5382d5f-6813-41d3-9f55-7e404c050c76","cell_type":"code","execution_count":7,"outputs":[]},{"source":"## Task 8: Add Debug Logging","metadata":{},"id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf","cell_type":"markdown"},{"source":"Let's take a moment to address what we've achieved by using RAG, which would be impossible to achieve with just using `gpt-3.5-turbo` as an LLM:\n\n1. We enabled the LLM to answer the question with factual information, which can even be information from after ChatGPT's knowledge cutoff (which is September 2021).\n2. We enabled the LLM to provide sources with the answer it generates.\n\nPretty neat, right?\n\nWe saw that langchain is very convenient when it comes to quickly creating smart AI systems. However, for learning, it can be quite challenging to understand what's happening behind the scenes. For example, from the code in Task 7, it's not clear that `qa_with_sources` actually first calls Pinecone to retrieve documents, then uses those documents to fill in the prompt to send along to the `gpt-3.5-turbo` LLM.\n\nLet's look at how we can get some more insights into how this all works.","metadata":{},"id":"fce00845-9741-4979-91aa-e43ff0db5299","cell_type":"markdown"},{"source":"### Instructions","metadata":{},"id":"1b998f52-93c9-411c-ba74-473955de4b8e","cell_type":"markdown"},{"source":"- Import `langchain`\n- Set `.debug` to `True` on `langchain`\n- Run `qa_with_sources(question)` again\n\nObserve the information that is printed in the output. Langchain enables you to run chains of LLMs or other AI systems, one after the other. The input for the next chain is passed on from the previous, where new information can be added by, for example, using embeddings to find relevant documents. Each chain or LLM is marked with a tag like `[chain/start]` or `[llm/start]`. When a final response is fetched from the last part of the chain, the output travels back up the chain. This is marked with the `[chain/end]` and `[llm/end]` marks.","metadata":{},"id":"c19bfda1-fe9a-4344-9dd9-613d33598d21","cell_type":"markdown"},{"source":"# Import langchain\n\n\n# Enable debug logging\n\n\n# Ask the LLM a question about movies\n","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1695209797302,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import langchain\n\n\n# Enable debug logging\n\n\n# Ask the LLM a question about movies\n","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"474d98a8-33ce-4898-a641-4853f17e5738","cell_type":"code","execution_count":8,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}