{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Chatbots with the OpenAI API and Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we aim to explore the fascinating world of AI chatbots. We will be using LangChain, OpenAI, and Pinecone vector DB, to build a chatbot capable of learning from the external world using **R**etrieval **A**ugmented **G**eneration (RAG).\n",
    "\n",
    "We will be using a dataset sourced from the Llama 2 ArXiv paper and other related papers to help our chatbot answer questions about the latest and greatest in the world of GenAI.\n",
    "\n",
    "This project is designed for learners who have a basic understanding of the OpenAI API and Pinecone, as covered in our previous projects. It's a great opportunity for those interested in AI, machine learning, and NLP to get hands-on experience with building a chatbot with RAG.\n",
    "\n",
    "![rag](rag.png)\n",
    "\n",
    "By the end of this project, you will have a functioning chatbot and RAG pipeline that can hold a conversation and provide informative responses based on a knowledge base. This project is a stepping stone towards understanding and building more complex AI systems in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n",
    "\n",
    "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
    "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
    "- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n",
    "- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone vector DB where we will store our chatbot's knowledge base.\n",
    "- **tiktoken**: This is a library from OpenAI that allows you to count the number of tokens in a text string without making an API call.\n",
    "\n",
    "You can install these libraries using pip like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8483,
    "lastExecutedAt": 1698488761884,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install -qU \\\n    langchain==0.0.292 \\\n    openai==0.28.0 \\\n    datasets==2.10.1 \\\n    pinecone-client==2.2.4 \\\n    tiktoken==0.5.1",
    "outputsMetadata": {
     "0": {
      "height": 337,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/repl/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script pinecone is installed in '/home/repl/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langsmith is installed in '/home/repl/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script openai is installed in '/home/repl/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/home/repl/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gremlinpython 3.6.1 requires aiohttp<=3.8.1,>=3.8.0, but you have aiohttp 3.8.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -qU \\\n",
    "#     langchain==0.0.292 \\\n",
    "#     openai==0.28.0 \\\n",
    "#     datasets==2.10.1 \\\n",
    "#     pinecone-client==2.2.4 \\\n",
    "#     tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Building a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To get more familiar with the library let's first create a chatbot _without_ RAG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Initialize the chat model object.\n",
    "\n",
    "- Import OpenAI api key\n",
    "- From langchain's chat_models module, import `ChatOpenAI`.\n",
    "- Initialize a `ChatOpenAI` object with the `gpt-3.5-turbo` model. Assign to `chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenAI Key\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1576,
    "lastExecutedAt": 1698488763461,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.chat_models import ChatOpenAI\n\nchat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are chats structured?\n",
    "\n",
    "Chats with OpenAI's `gpt-3.5-turbo` and `gpt-4` chat models are typically structured (in plain text) like this:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "```python\n",
    "#openai\n",
    "messages =[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In LangChain there is a slightly different format. We use three _message_ objects like so:\n",
    "\n",
    "```python\n",
    "#langchain\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]\n",
    "```\n",
    "\n",
    "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Create a conversation.\n",
    "\n",
    "- From langchain's schema module, import the three message types: `SystemMessage`, `HumanMessage`, and `AIMessage`.\n",
    "- Create a conversation as a list of messages. Assign to `messages`.\n",
    "    1. A system message with content `\"You are a helpful assistant.\"`\n",
    "    2. A human message with content `\"Hi AI, how are you today?\"`\n",
    "    3. An AI message with content `\"I'm great thank you. How can I help you?\"`\n",
    "    4. A human message with content `\"I'd like to understand string theory.\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1698488763511,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.schema import (\n    SystemMessage,\n    HumanMessage,\n    AIMessage\n)\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hi AI, how are you today?\"),\n    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n    HumanMessage(content=\"I'd like to understand string theory.\")\n]"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like you to name 10 random pokemons.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " HumanMessage(content='Hi AI, how are you today?'),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
       " HumanMessage(content=\"I'd like you to name 10 random pokemons.\")]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the next response from the AI by passing these messages to the `ChatOpenAI` object. You can call `chat` as though it is a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Chat with GPT.\n",
    "\n",
    "- Pass the messages to the chat and get a response. Assign to `res`.\n",
    "- Print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5331,
    "lastExecutedAt": 1698488768843,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "res = chat(messages)\n\nres"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! Here are 10 random Pokémon names:\\n\\n1. Pikachu\\n2. Bulbasaur\\n3. Charizard\\n4. Jigglypuff\\n5. Snorlax\\n6. Eevee\\n7. Gyarados\\n8. Machamp\\n9. Lucario\\n10. Mewtwo\\n\\nI hope you like them!')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `AIMessage` object looks a bit like a dictionary. The most important element is `content`, which contains the chat text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Print only the contents of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1698488768900,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 377,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are 10 random Pokémon names:\n",
      "\n",
      "1. Pikachu\n",
      "2. Bulbasaur\n",
      "3. Charizard\n",
      "4. Jigglypuff\n",
      "5. Snorlax\n",
      "6. Eevee\n",
      "7. Gyarados\n",
      "8. Machamp\n",
      "9. Lucario\n",
      "10. Mewtwo\n",
      "\n",
      "I hope you like them!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "#display(Markdown(response.content))\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `response` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Continue the conversation with GPT.\n",
    "\n",
    "- Append the latest AI response to messages.\n",
    "- Create a new human message. Assign to `prompt`.\n",
    "    - Use the content `\"Why do physicists believe it can produce a 'unified theory'?\"`\n",
    "- Append the prompt to messages.\n",
    "- Send the messages to GPT. Assign to `res`.\n",
    "- Print the contents of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1698488768951,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "messages.append(res)",
    "outputsMetadata": {
     "0": {
      "height": 505,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " HumanMessage(content='Hi AI, how are you today?'),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
       " HumanMessage(content=\"I'd like you to name 10 random pokemons.\"),\n",
       " AIMessage(content='Sure! Here are 10 random Pokémon names:\\n\\n1. Pikachu\\n2. Bulbasaur\\n3. Charizard\\n4. Jigglypuff\\n5. Snorlax\\n6. Eevee\\n7. Gyarados\\n8. Machamp\\n9. Lucario\\n10. Mewtwo\\n\\nI hope you like them!')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append(response)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1698488769003,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "prompt = HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\")\n\nmessages.append(prompt)"
   },
   "outputs": [],
   "source": [
    "prompt = HumanMessage(content=\"In which generation was the number 9 in that list released?\")\n",
    "\n",
    "messages.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1698488769056,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "messages"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " HumanMessage(content='Hi AI, how are you today?'),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
       " HumanMessage(content=\"I'd like you to name 10 random pokemons.\"),\n",
       " AIMessage(content='Sure! Here are 10 random Pokémon names:\\n\\n1. Pikachu\\n2. Bulbasaur\\n3. Charizard\\n4. Jigglypuff\\n5. Snorlax\\n6. Eevee\\n7. Gyarados\\n8. Machamp\\n9. Lucario\\n10. Mewtwo\\n\\nI hope you like them!'),\n",
       " HumanMessage(content='In which generation was the number 9 in that list released?')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6628,
    "lastExecutedAt": 1698488775684,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "res = chat(messages)\n\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 397,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lucario was introduced in the fourth generation of Pokémon games, which includes Pokémon Diamond, Pearl, and Platinum.\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_turn(human_message):\n",
    "    chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")  # Initialize the chat model\n",
    "\n",
    "    # Initial chat history\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "        AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "        HumanMessage(content=human_message),\n",
    "    ]\n",
    "\n",
    "    # Now, generate the AI response with the updated chat history\n",
    "    response = chat(messages)  # The function or method to get the response from the chat model\n",
    "\n",
    "    # Append the AI's response to the chat history\n",
    "    messages.append(response)\n",
    "    \n",
    "    return response # Return the AI's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! Here are some famous football players:\\n\\n1. Cristiano Ronaldo\\n2. Lionel Messi\\n3. Neymar\\n4. Diego Maradona\\n5. Pele\\n6. Zinedine Zidane\\n7. Ronaldinho\\n8. David Beckham\\n9. Johan Cruyff\\n10. Franz Beckenbauer\\n11. Ronaldo (Brazilian)\\n12. Michel Platini\\n13. Thierry Henry\\n14. George Best\\n15. Roberto Carlos\\n\\nThese are just a few examples, and there are many more talented football players out there.')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_turn(\"list famouse football players\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our chatbot, but as mentioned—the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "![langchain-no-access-to-world](langchain-no-access-to-world.png)\n",
    "\n",
    "This means that GPT (or any other LLM) will perform badly on some types of question.\n",
    "\n",
    "* The chatbot doesn't know about recent events. How does it respond if you ask about the weather in your city today?\n",
    "* It can't answer questions about recent code or recent products. Try asking it `\"Can you tell me about the LLMChain in LangChain?\"` or `\"What was the latest course released on DataCamp?\"`\n",
    "* It can't answer questions about confidential corporate information that hasn't been released into the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Ask GPT about the new (and very popular) Llama 2 LLM.\n",
    "\n",
    "- Append the latest AI response to `messages`.\n",
    "- Create a new human message. Assign to `prompt`.\n",
    "    - Use the content `\"What is so special about Llama 2?\"`.\n",
    "- Append the prompt to `messages`.\n",
    "- Send the messages to GPT. Assign to `res`.\n",
    "- Print the contents of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1967,
    "lastExecutedAt": 1698488777699,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "prompt = HumanMessage(content=\"What is so special about Llama 2?\")\nmessages.append(prompt)\n\nres = chat(messages)\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 57,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m sorry, but I couldn\\'t find any specific information about \"Llama 2.\" It\\'s possible that you may be referring to something specific, such as a game, a book, or an event. Could you provide more context or clarify your question so I can assist you better?')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_hallucination_query = \"What is so special about Llama 2?\" \n",
    "new_turn(possible_hallucination_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidently wrong: hallucinations from LLMs\n",
    "\n",
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the information, but sometimes an LLM may respond like it _does_ know the answer—and this can be very hard to detect. See this example from the earliest version of GPT-4 in the OpenAI Playground:\n",
    "\n",
    "![llm-chain-hallucination](llm-chain-hallucination.png)\n",
    "\n",
    "OpenAI have since adjusted the behavior for this particular example as we can see below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Ask GPT about LangChain.\n",
    "\n",
    "- Append the latest AI response to `messages`.\n",
    "- Create a new human message. Assign to `prompt`.\n",
    "    - Use the content `\"Can you tell me about the LLMChain in LangChain?\"`.\n",
    "- Append the prompt to `messages`.\n",
    "- Send the messages to GPT. Assign to `res`.\n",
    "- Print the contents of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1962,
    "lastExecutedAt": 1698488779661,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "messages.append(res)\nprompt = HumanMessage(content=\"Can you tell me about the LLMChain in LangChain?\")\nmessages.append(prompt)\n\nres = chat(messages)\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The LLMChain is a blockchain technology developed by LangChain. It stands for Language Learning Management Chain and it is specifically designed for language learning and education purposes. The LLMChain aims to revolutionize the language learning industry by leveraging the power of blockchain technology.\\n\\nWith the LLMChain, LangChain aims to address some common challenges in language learning, such as lack of transparency, lack of trust, and ineffective verification systems. By utilizing blockchain, LangChain ensures that learning records are stored in a secure and immutable manner, allowing for transparent and trustworthy verification of language proficiency.\\n\\nThe LLMChain also incorporates smart contracts, which enable automated and secure transactions between learners, tutors, and institutions. This facilitates efficient and transparent payment systems, as well as streamlined administration processes.\\n\\nFurthermore, the LLMChain tokenizes language learning achievements, allowing learners to earn digital certificates and badges that can be stored on the blockchain. These digital credentials can be easily shared and verified, eliminating the need for traditional paper-based certificates.\\n\\nOverall, the LLMChain in LangChain aims to create a fair, transparent, and efficient ecosystem for language learning, enhancing the learning experience for both learners and educators.')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_hallucination_query = \"Can you tell me about the LLMChain in LangChain?\" \n",
    "new_turn(possible_hallucination_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of January 2021. we got a hallucination:\n",
    "\n",
    "```\n",
    "AIMessage(content='The LLMChain is a blockchain technology developed by LangChain. It stands for Language Learning Management Chain and it is specifically designed for language learning and education purposes. The LLMChain aims to revolutionize the language learning industry by leveraging the power of blockchain technology.\\n\\nWith the LLMChain, LangChain aims to address some common challenges in language learning, such as lack of transparency, lack of trust, and ineffective verification systems. By utilizing blockchain, LangChain ensures that learning records are stored in a secure and immutable manner, allowing for transparent and trustworthy verification of language proficiency.\\n\\nThe LLMChain also incorporates smart contracts, which enable automated and secure transactions between learners, tutors, and institutions. This facilitates efficient and transparent payment systems, as well as streamlined administration processes.\\n\\nFurthermore, the LLMChain tokenizes language learning achievements, allowing learners to earn digital certificates and badges that can be stored on the blockchain. These digital credentials can be easily shared and verified, eliminating the need for traditional paper-based certificates.\\n\\nOverall, the LLMChain in LangChain aims to create a fair, transparent, and efficient ecosystem for language learning, enhancing the learning experience for both learners and educators.')\n",
    "```\n",
    "\n",
    "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Create a string of knowledge about chains.\n",
    "\n",
    "- *Read the descriptions of LLMChains, Chains, and LangChain given in `llmchain_information`.*\n",
    "- Combine the list of description strings into a single string. Assign to `source_knowledge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 55,
    "lastExecutedAt": 1698488779716,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# A description of LLMChains, Chains, and LangChain \nllmchain_information = [\n    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n]\nlen(llmchain_information)"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A description of LLMChains, Chains, and LangChain \n",
    "llmchain_information = [\n",
    "    \"An LLMChain is used widely throughout LangChain, including in other chains and agents. It consists of a PromptTemplate and a language model (either an LLM or chat model).\",\n",
    "    \"The default implementation of LLMChain allows usage of async code and supports methods like apply, apply_and_parse, apredict, and arun to process inputs and generate outputs, often leveraging the LLM's generative capabilities for speed gains.\",\n",
    "    \"Chains in LangChain can be used for a variety of purposes, including retrieving documents, interacting with APIs, and conversing with documents to generate responses. Chains like ConversationalRetrievalChain and ReduceDocumentsChain are examples of how LLMs can be utilized to process information and answer questions.\",\n",
    "    \"LLMChains can be customized with specific prompt templates for various subjects like physics, math, history, and computer science. These chains can be routed using an LLMRouterChain, which decides which subchain to use based on the input. Destination chains are language model chains, an LLM chain, which are called by RouterChain.\",\n",
    "    \"LLMChains can also be used with models from the Hugging Face Hub, where you can specify the model and generate responses to questions. Multiple questions can be handled either by generating answers one by one or by combining all questions into a single prompt for more advanced LLMs.\"\n",
    "]\n",
    "\n",
    "len(llmchain_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1698488779772,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "source_knowledge = \"\\n\".join(llmchain_information)\nsource_knowledge"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"An LLMChain is used widely throughout LangChain, including in other chains and agents. It consists of a PromptTemplate and a language model (either an LLM or chat model).\\nThe default implementation of LLMChain allows usage of async code and supports methods like apply, apply_and_parse, apredict, and arun to process inputs and generate outputs, often leveraging the LLM's generative capabilities for speed gains.\\nChains in LangChain can be used for a variety of purposes, including retrieving documents, interacting with APIs, and conversing with documents to generate responses. Chains like ConversationalRetrievalChain and ReduceDocumentsChain are examples of how LLMs can be utilized to process information and answer questions.\\nLLMChains can be customized with specific prompt templates for various subjects like physics, math, history, and computer science. These chains can be routed using an LLMRouterChain, which decides which subchain to use based on the input. Destination chains are language model chains, an LLM chain, which are called by RouterChain.\\nLLMChains can also be used with models from the Hugging Face Hub, where you can specify the model and generate responses to questions. Multiple questions can be handled either by generating answers one by one or by combining all questions into a single prompt for more advanced LLMs.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_knowledge = \"\\n\".join(llmchain_information)\n",
    "source_knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Define a question. Assign to `query`.\n",
    "    - Use the text `\"Can you tell me about the LLMChain in LangChain?\"`\n",
    "- Create an augmented prompt containing the context and query. Assign to `augmented_prompt`.\n",
    "\n",
    "        augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "        Contexts:\n",
    "        {source_knowledge}\n",
    "\n",
    "        Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1698488779819,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "query = \"Can you tell me about the LLMChain in LangChain?\"\n\naugmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\nthe contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\nContexts:\n{source_knowledge}\n\nQuery: {query}\"\"\""
   },
   "outputs": [],
   "source": [
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\n",
    "the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {possible_hallucination_query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1698488779872,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(augmented_prompt)",
    "outputsMetadata": {
     "0": {
      "height": 317,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query. If some information is not provided within\n",
      "the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n",
      "\n",
      "Contexts:\n",
      "An LLMChain is used widely throughout LangChain, including in other chains and agents. It consists of a PromptTemplate and a language model (either an LLM or chat model).\n",
      "The default implementation of LLMChain allows usage of async code and supports methods like apply, apply_and_parse, apredict, and arun to process inputs and generate outputs, often leveraging the LLM's generative capabilities for speed gains.\n",
      "Chains in LangChain can be used for a variety of purposes, including retrieving documents, interacting with APIs, and conversing with documents to generate responses. Chains like ConversationalRetrievalChain and ReduceDocumentsChain are examples of how LLMs can be utilized to process information and answer questions.\n",
      "LLMChains can be customized with specific prompt templates for various subjects like physics, math, history, and computer science. These chains can be routed using an LLMRouterChain, which decides which subchain to use based on the input. Destination chains are language model chains, an LLM chain, which are called by RouterChain.\n",
      "LLMChains can also be used with models from the Hugging Face Hub, where you can specify the model and generate responses to questions. Multiple questions can be handled either by generating answers one by one or by combining all questions into a single prompt for more advanced LLMs.\n",
      "\n",
      "Query: Can you tell me about the LLMChain in LangChain?\n"
     ]
    }
   ],
   "source": [
    "print(augmented_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this into our chatbot as we did before.\n",
    "\n",
    "Don't append the previous AI message, since it wasn't a good answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Ask GPT about LangChain again, this time providing source knowledge.\n",
    "\n",
    "- Create a new human message. Assign to `prompt`.\n",
    "    - Use `augmented_prompt` as the content.\n",
    "- Append the prompt to `messages`.\n",
    "- Send the messages to GPT. Assign to `res`.\n",
    "- Print the contents of the response.\n",
    "\n",
    "You can also use the `new_turn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LLMChain is a widely used component in LangChain. It is composed of a PromptTemplate and a language model, which can be an LLM or a chat model. The default implementation of LLMChain supports async code and offers methods such as apply, apply_and_parse, apredict, and arun for input processing and generating outputs. LLMChains in LangChain serve various purposes, such as retrieving documents, interacting with APIs, and generating responses by conversing with documents. Examples of LLM-based chains in LangChain include ConversationalRetrievalChain and ReduceDocumentsChain. LLMChains can be customized with specific prompt templates for subjects like physics, math, history, and computer science. They can also be routed using an LLMRouterChain, which decides which subchain to use based on the input. Destination chains are language model chains called by the RouterChain. Additionally, LLMChains can utilize models from the Hugging Face Hub to generate responses to questions, either handling them individually or combining multiple questions into a single prompt for more advanced LLMs.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_turn(augmented_prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of this answer is phenomenal! This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem—how do we get this information in the first place?\n",
    "\n",
    "We learned in the previous code-alongs about Pinecone and vector databases. Well, they can help us here too. But first, we'll need a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library and [the `\"jamescalam/llama-2-arxiv-papers\"` dataset](https://huggingface.co/datasets/jamescalam/llama-2-arxiv-papers-chunked). This dataset contains a collection of ArXiv papers which will serve as the external knowledge base for our chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Load the ArXiv papers dataset.\n",
    "\n",
    "- From the *datasets* package, import `load_dataset`.\n",
    "- Load the train split of the `jamescalam/llama-2-arxiv-papers-chunked` dataset. Assign to `dataset`.\n",
    "- Print the dataset object to see the structure of the data.\n",
    "- *Look at the structure. Which fields should we keep?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1388,
    "lastExecutedAt": 1698488786201,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from datasets import load_dataset\n\ndata = load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\", split=\"train\")\ndata",
    "outputsMetadata": {
     "1": {
      "height": 77,
      "type": "stream"
     },
     "6": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e27f84f2724840b17a9b012ce14a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/409 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278f6f64e6434b1ea19b54ad19b8d970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a27687e5f34c0b950e83d77bf6118e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f51619b1a3b4dd5897c0625a0c28402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851b87d0eda94ca791989a8e4b2aa7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Print a record of dataset to get a feel for what they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 383,
    "lastExecutedAt": 1698488786584,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "data[0]"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1102.0183',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n",
       " 'id': '1102.0183',\n",
       " 'title': 'High-Performance Neural Networks for Visual Object Classification',\n",
       " 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n",
       " 'source': 'http://arxiv.org/pdf/1102.0183',\n",
       " 'authors': ['Dan C. Cireşan',\n",
       "  'Ueli Meier',\n",
       "  'Jonathan Masci',\n",
       "  'Luca M. Gambardella',\n",
       "  'Jürgen Schmidhuber'],\n",
       " 'categories': ['cs.AI', 'cs.NE'],\n",
       " 'comment': '12 pages, 2 figures, 5 tables',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '20110201',\n",
       " 'updated': '20110201',\n",
       " 'references': []}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0] #dict type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Summary\n",
    "\n",
    "The dataset we are using is sourced from the Llama 2 ArXiv papers. It is a collection of academic papers from ArXiv, a repository of electronic preprints approved for publication after moderation. Each entry in the dataset represents a \"chunk\" of text from these papers.\n",
    "\n",
    "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, they cannot answer our questions about Llama 2—at least not without this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Building the Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "The workflow for setting up a chatbot is much the same as for setting up semantic serach and retrieval augmented generation, as seen in previous code-alongs.\n",
    "\n",
    "- Initialize your connection to the Pinecone vector DB.\n",
    "- Create an index (remember to consider the dimensionality of `text-embedding-ada-002`).\n",
    "- Initialize OpenAI's `text-embedding-ada-002` model with LangChain.\n",
    "- Populate the index with records (in this case from the Llama 2 dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Initialize Pinecone, getting setup details from Workspace environment variables.\n",
    "\n",
    "- Import the os package.\n",
    "- Import the pinecone package.\n",
    "- Initialize pinecone, setting the API key and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 659,
    "lastExecutedAt": 1698488788386,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import os\nimport pinecone\n\npinecone.init(\n    api_key=os.environ[\"PINECONE_API_KEY\"],\n    environment=os.environ[\"PINECONE_ENVIRONMENT\"]\n)"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "    environment=os.environ[\"PINECONE_ENVIRONMENT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Create a vector index in the Pinecone database.\n",
    "\n",
    "- Import the time package.\n",
    "- Choose a name for the vector index. Assign to `index_name`.\n",
    "- Check if index_name is not in Pinecone's list of existing indexes.\n",
    "    -  Create an index named index_name, dimension 1536, cosine similarity as its metric.\n",
    "    -  While the index status is not ready, sleep for one second.\n",
    "- Connect to the index. Assign to `index`.\n",
    "- View the index stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 33401,
    "lastExecutedAt": 1698489011979,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import time\n\nindex_name = \"llama-2-rag\"\n\nif index_name not in pinecone.list_indexes():\n    pinecone.create_index(\n        index_name, dimension=1536, metric=\"cosine\"\n    )\n    while not pinecone.describe_index(index_name).status[\"ready\"]:\n        time.sleep(1)\n        \nindex = pinecone.Index(index_name)"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"llama-2-rag\"\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name, dimension=1536, metric=\"cosine\"\n",
    "    )\n",
    "    while not pinecone.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "        \n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 232,
    "lastExecutedAt": 1698489028339,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "index.describe_index_stats()"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model—we can access it via LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Create an embeddings model.\n",
    "\n",
    "- From langchain's embeddings module and openai submodule, import `OpenAIEmbeddings`.\n",
    "- Create an embedings model object for `text-embedding-ada-002`. Assign to `embed_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1698489100619,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.embeddings.openai import OpenAIEmbeddings\n\nembed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1272,
    "lastExecutedAt": 1698489196826,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "texts = [\n    \"this is a sentence\",\n    \"this is another sentence\"\n]\n\nres = embed_model.embed_documents(texts=texts)\nlen(res), len(res[0])"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"this is a sentence\",\n",
    "    \"this is another sentence\"\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts=texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Split the dataset into batches and add it to the vector index.\n",
    "\n",
    "- From *tqdm*, import `tqdm` (a progress bar).\n",
    "- Convert `dataset` to a pandas DataFrame to make it easier to iterate over the dataset.\n",
    "- Set the batch size to 100. Assign to `batch_size`.\n",
    "- Loop from 0 to the length of contexts by batch size, adding a progress bar.\n",
    "    - Find the end of the batch. Assign to `i_end`.\n",
    "    - Get the data records for the batch. Assign to `batch`.\n",
    "    - Generate unique ids for each chunk. Assign to `ids`.\n",
    "    - Get text to embed. Assign to `texts`.\n",
    "    - Use the embedding model to embed the text documents. Assign to embeds.\n",
    "    - Get text, source, and title metadata to store in Pinecone. Assign to metadata.\n",
    "    - Upsert to Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1698489533412,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "data[0][\"doi\"], data[1][\"doi\"], data[0][\"chunk-id\"], data[1][\"chunk-id\"]"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1102.0183', '1102.0183', '0', '1')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"doi\"], data[1][\"doi\"], data[0][\"chunk-id\"], data[1][\"chunk-id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1698489659921,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "data"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 74075,
    "lastExecutedAt": 1698489896718,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from tqdm import tqdm\n\ndata = data.to_pandas()\n\nbatch_size = 100\n\nfor i in tqdm(range(0, len(data), batch_size)):\n    i_end = min(i+batch_size, len(data))\n    batch = data.iloc[i:i_end]\n    ids = [f\"{x['doi']}-{x['chunk-id']}\" for _, x in batch.iterrows()]\n    texts = [x[\"chunk\"] for _, x in batch.iterrows()]\n    embeds = embed_model.embed_documents(texts)\n    metadata = [\n        {\"text\": x[\"chunk\"],\n         \"title\": x[\"title\"],\n         \"source\": x[\"source\"]} for _, x in  batch.iterrows()\n    ]\n    # [(id1, embed1, metadata1), (id2, embed2, metadata2), ...]\n    index.upsert(vectors=zip(ids, embeds, metadata))",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:14<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = data.to_pandas()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(i+batch_size, len(data))\n",
    "    batch = data.iloc[i:i_end]\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for _, x in batch.iterrows()]\n",
    "    texts = [x[\"chunk\"] for _, x in batch.iterrows()]\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    metadata = [\n",
    "        {\"text\": x[\"chunk\"],\n",
    "         \"title\": x[\"title\"],\n",
    "         \"source\": x[\"source\"]} for _, x in  batch.iterrows()\n",
    "    ]\n",
    "    # [(id1, embed1, metadata1), (id2, embed2, metadata2), ...]\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Check on updates to the vector index now that it contains the ArXiv dataset.\n",
    "\n",
    "- View the index stats again.\n",
    "- *What has changed since you last looked?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 75,
    "lastExecutedAt": 1698489901904,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "index.describe_index_stats()"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous task we built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "* Create a LangChain `vectorstore` object using our `index` and `embed_model`.\n",
    "* Try searching for relevant information about Llama 2.\n",
    "* Create a function (`augment_prompt`) that can take our query, retrieve information using the `vectorstore`, and merge them all into a single retrieval-augmented prompt.\n",
    "* Try asking the chatbot Llama 2 questions with and without RAG, comparing the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LangChain's RAG pipeline we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Initialize the vector store object.\n",
    "\n",
    "- From langchain's vectorstores module, import `Pinecone`.\n",
    "- State the metadata field that contains our text (namely `\"text\"`). Assign to `text_field`.\n",
    "- Initialize the vector store object. Assign to `vectorstore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1698489995687,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.vectorstores import Pinecone\n\ntext_field = \"text\"\n\nvectorstore = Pinecone(\n    index, embed_model.embed_query, text_field\n)"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Perform similarity search against a question.\n",
    "\n",
    "- Define a question. Assign to query.\n",
    "    - Use the text `\"What is so special about Llama 2?\"`.\n",
    "- Perform a similarity search for the query, returning 3 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 304,
    "lastExecutedAt": 1698490032321,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "query = \"What is so special about Llama 2?\"\n\nvectorstore.similarity_search(query, k=3)"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is so special about Llama 2?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Run the code to define a function to augment a prompt with knowledge base results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10,
    "lastExecutedAt": 1698490248062,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def augment_prompt(query: str):\n    results = vectorstore.similarity_search(query, k=3)\n    source_knowledge = \"\\n\".join([x.page_content for x in results])\n    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\nthe contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\nContexts:\n{source_knowledge}\n\nQuery: {query}\"\"\"\n    return augmented_prompt"
   },
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\n",
    "the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 302,
    "lastExecutedAt": 1698490260911,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(augment_prompt(query))",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query. If some information is not provided within\n",
      "the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n",
      "\n",
      "Contexts:\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom\u0003\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
      "preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n",
      "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\n",
      "Grave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\n",
      "arXiv:2302.13971 , 2023.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "and Illia Polosukhin. Attention is all you need, 2017.\n",
      "Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\n",
      "David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\n",
      "multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint\n",
      "\n",
      "Query: What is so special about Llama 2?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Ask GPT about LLama2, augmenting the prompt with source knowledge from the Pinecone vector index.\n",
    "\n",
    "- Create a new human message. Assign to `prompt`.\n",
    "    - Call `augment_prompt()` on the query and use this as the content.\n",
    "- Append the prompt to `messages`.\n",
    "- Send the messages to GPT. Assign to `res`.\n",
    "- Print the contents of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5775,
    "lastExecutedAt": 1698490333721,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "prompt = HumanMessage(content=augment_prompt(query))\n\nmessages.append(prompt)\nres = chat(messages)\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 251,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it is mentioned that Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging from 7 billion to 70 billion parameters. These LLMs, such as L/l.sc/a.sc/m.sc/a.sc/t.sc and L/l.sc/a.sc/m.sc/a.sc/t.sc-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. The models have been evaluated and found to outperform open-source chat models on most benchmarks tested, and they are considered as potential substitutes for closed-source models in terms of helpfulness and safety.\n",
      "\n",
      "Furthermore, the authors describe their approach to fine-tuning and safety in detail, highlighting that closed-source LLMs are heavily fine-tuned to align with human preferences, enhancing their usability and safety. The development and release of Llama 2 aims to provide an open and efficient foundation for language models, allowing for advancements in AI alignment research.\n",
      "\n",
      "However, it's important to note that the context provided is limited, and there may be additional information about Llama 2 that is not included.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(content=augment_prompt(query))\n",
    "\n",
    "messages.append(prompt)\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue with more Llama 2 questions. Let's try _without_ RAG first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Ask GPT about LLama 2.\n",
    "\n",
    "- Create a new human message. Assign to `prompt`.\n",
    "    - Use the context `\"What safety measures were used in the development of llama 2?\"`.\n",
    "- Send the messages plus the prompt to GPT. Assign to `res`.\n",
    "    - *Don't use `.append()` here, as we don't want to store the latest message in the conversation.*\n",
    "- Print the contents of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 4663,
    "lastExecutedAt": 1698490475739,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "prompt = HumanMessage(\n    content=\"What safety measures were used in the development of llama 2?\"\n)\n\nres = chat(messages + [prompt])\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 193,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the safety measures used in the development of Llama 2 are mentioned but not explicitly described. The text mentions that the fine-tuned LLMs in Llama 2, such as L/l.sc/a.sc/m.sc/a.sc/t.sc and L/l.sc/a.sc/m.sc/a.sc/t.sc-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases and appear to be on par with some closed-source models in terms of safety. It also mentions that closed-source models are heavily fine-tuned to align with human preferences, enhancing their usability and safety.\n",
      "\n",
      "However, the specific safety measures employed in the development of Llama 2 are not elaborated upon in the given information. It is possible that the paper or source from which this information is derived may provide more details on the safety measures utilized.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"What safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chatbot is able to respond about Llama 2 thanks to it's conversational history stored in `messages`. However, it doesn't know anything about the safety measures themselves as we have not provided it with that information via the RAG pipeline. Let's try again but with RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Ask GPT about LLama 2 again.\n",
    "\n",
    "- Do the same thing again, but this time augment the prompt using `augment_prompt()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2855,
    "lastExecutedAt": 1698490604812,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "prompt = HumanMessage(\n    content=augment_prompt(\"What safety measures were used in the development of llama 2?\")\n)\n\nres = chat(messages + [prompt])\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 95,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided contexts, the safety measures used in the development of Llama 2 include safety-specific data annotation and tuning, red-teaming, iterative evaluations, and a thorough approach to improving the safety of the fine-tuned large language models (LLMs). These measures were taken to increase the safety of the models and ensure responsible development of LLMs. The paper also mentions that a detailed description of their fine-tuning methodology and safety approach is provided, which can further shed light on the specific safety measures implemented.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\"What safety measures were used in the development of llama 2?\")\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a much better informed response that includes several items missing in the previous non-RAG response, such as \"red-teaming\", \"iterative evaluations\", and the intention of the researchers to share this research to help \"improve their safety, promoting responsible development in the field\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You built a chatbot that can answer questions about cutting edge large language models!\n",
    "\n",
    "In particular, you\n",
    "\n",
    "- learned how to have a conversation with GPT by appending messages.\n",
    "- saw how to provide context in a prompt to help GPT answer questions.\n",
    "- setup a Pinecone database and added data to a vector index.\n",
    "- retrieved text relevant to user questions.\n",
    "- combined it all to create a chatbot that answered questions that GPT could not answer by itself."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
