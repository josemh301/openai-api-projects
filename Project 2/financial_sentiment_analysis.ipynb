{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Sentiment Analysis Project\n",
    "\n",
    "## Prompt Engineering with GPT and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is framework that is extremely helpful for prompt engineering and the integration of generative AI capabilities in applications or data platforms. It has many capabilities, some of which will not be introduced until later modules, but we will start with a gentle introduction to some of the easy-to-understand concepts in the framework.\n",
    "\n",
    "You'll build an AI agent that uses Python and GPT to perform sentiment analysis on financial headlines.\n",
    "\n",
    "In more detail, you'll cover:\n",
    "- Getting set up with an OpenAI developer account and integration with Workspace.\n",
    "- Interacting with OpenAI models through the langchain framework.\n",
    "- Using prompt templates that write reusable, dynamic prompts.\n",
    "- Working with LLM chains.\n",
    "- Automatically parsing the output of an LLM to be used downstream.\n",
    "- Working with langchain agents and tools.\n",
    "- Using the OpenAI Moderation API to filter explicit content.\n",
    "\n",
    "For this project, we are using two small samples: `financial_headlines.txt` and `reddit_comments.txt`. These 5-6 line samples are kept short to keep evaluation easy, but keep in mind that this same code and prompt engineering techniques can scale to datasets of much larger size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install a few packages, one of which being the `langchain` package. This is currently being developed quickly, sometimes with breaking changes, so we fix the version.\n",
    "\n",
    "`langchain` depends on a recent version of `typing_extensions`, so we need to update that package, again fixing the version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Run the following code to install `openai`, `langchain`, `typing_extensions` and `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install openai.\n",
    "# !pip install openai==0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install langchain.\n",
    "# !pip install langchain==0.0.293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install typing-extensions.\n",
    "# !pip install typing-extensions==4.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas.\n",
    "# !pip install pandas==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should work with this one so far\n",
    "# !pip install langchain==0.0.339\n",
    "# !pip install openai==1.3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we need first need to load the openai and os packages to set the API key from the environment variables you just created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Import the `os` package.\n",
    "- Import the `openai` package.\n",
    "- Set `openai.api_key` to the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI imported successfully!\n",
      "Success getting the OpenAI key!\n"
     ]
    }
   ],
   "source": [
    "# Import the os package.\n",
    "# Import the openai package.\n",
    "# Set openai.api_key to the OPENAI_API_KEY environment variable.\n",
    "\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import openai\n",
    "    print(\"OpenAI imported successfully!\")\n",
    "    \n",
    "except:\n",
    "    print(\"The OpenAI import didn't work\")\n",
    "    \n",
    "try:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    print(\"Success getting the OpenAI key!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `langchain` package, let's start by importing it's `OpenAI` and `ChatOpenAI` class, which are used to interact with completion models and chat completion models respectively.\n",
    "\n",
    "Completion models, such as the GPT-1, GPT-2, GPT-3 and GPT-3.5, work as an advanced autocomplete model. Given a certain snippet of text as input, they will complete the text until a certain point. This could be either an end-of-sequence token (a natural way of stopping), the model reaching its maximum token limit for outputs and so on.\n",
    "\n",
    "Chat completion models, such as GPT-3.5-Turbo (the ChatGPT model) and GPT-4, are designed for conversational use. These models are typically more fine-tuned for conversations, keep a prompt/conversation history and allow access to a system message, which we can use as a meta prompt to define a role, a tone of voice, a scope, etc.\n",
    "\n",
    "Completion models and chat completion models tend to work with different classes and functions in the SDK. For that reason, we will start by importing both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Import `OpenAI` from `langchain.llms`.\n",
    "- Import `ChatOpenAI` from `langchain.chat_models`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "    \n",
    "Remember the syntax for Python imports: `from ... import ...`\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import OpenAI. ------> For completions models (one turn)\n",
    "# Import ChatOpenAI. --> For converational models (full conversation, like ChatGPT)\n",
    "try:\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    print(\"Langchain imported successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error with langchain imports:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the IPython.display package, import display and Markdown\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import the Financial News Headlines Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small sample of financial headlines is stored in `financial_headlines.txt`.\n",
    "\n",
    "Our first step is to read in the text file and store the headlines in a Python list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Import the text file to a Python list.\n",
    "\n",
    "- Open `financial_headlines.txt` for reading.\n",
    "- Read in the lines using the `.readlines()` method. Assign to `headlines`.\n",
    "- Print the sample headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "\n",
    "- A good way of opening (and automatically closing) a file is using: `with open(file_name, \"r\") as file:`.\n",
    "- We can then use the `.readlines()` method on the `file` variable.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Finnish Aktia Group 's operating profit rose to EUR 17.5 mn in the first quarter of 2010 from EUR 8.2 mn in the first quarter of 2009 .\\n\", 'Finnish measuring equipment maker Vaisala Oyj HEL : VAIAS said today that its net loss widened to EUR4 .8 m in the first half of 2010 from EUR2 .3 m in the corresponding period a year earlier .\\n', 'Finnish pharmaceuticals company Orion reports profit before taxes of EUR 70.0 mn in the third quarter of 2010 , up from EUR 54.9 mn in the corresponding period in 2009 .\\n', 'Tiimari , the Finnish retailer , reported to have geenrated quarterly revenues totalling EUR 1.3 mn in the 4th quarter 2009 , up from EUR 0.3 mn loss in 2008 .\\n', \"Finnish Metso Paper has been awarded a contract for the rebuild of Sabah Forest Industries ' ( SFI ) pulp mill in Sabah , Malaysia .\\n\", 'Finnish Outokumpu Technology has been awarded several new grinding technology contracts .']\n"
     ]
    }
   ],
   "source": [
    "# Open the text file and read its lines.\n",
    "with open(\"financial_headlines.txt\", \"r\", encoding=\"utf-8\") as financial_headlines_txt:\n",
    "    headlines = financial_headlines_txt.readlines()\n",
    "\n",
    "# Print all headlines.\n",
    "print(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headlines seem to a bit of whitespace preceding the punctuation, but this does not influence the performance of our large language model.\n",
    "You can also see that every headline ends with a new line (`\\n`).\n",
    "\n",
    "We can quickly strip the `\\n` from the end of each headline, as this might improve visibility later down the line, when printing these headlines in a dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's strip the `\\n` character from the end of every news headline.\n",
    "\n",
    "- Loop through `headlines` and use the `.strip()` method to remove the `\\n` character from each line.\n",
    "- Print the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "\n",
    "To quickly reassign the adjusted elements of our list, we can make use of Python list comprehensions.\n",
    "    \n",
    "For example: `new_list = [f(x) for x in list]`\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Finnish Aktia Group 's operating profit rose to EUR 17.5 mn in the first quarter of 2010 from EUR 8.2 mn in the first quarter of 2009 .\", 'Finnish measuring equipment maker Vaisala Oyj HEL : VAIAS said today that its net loss widened to EUR4 .8 m in the first half of 2010 from EUR2 .3 m in the corresponding period a year earlier .', 'Finnish pharmaceuticals company Orion reports profit before taxes of EUR 70.0 mn in the third quarter of 2010 , up from EUR 54.9 mn in the corresponding period in 2009 .', 'Tiimari , the Finnish retailer , reported to have geenrated quarterly revenues totalling EUR 1.3 mn in the 4th quarter 2009 , up from EUR 0.3 mn loss in 2008 .', \"Finnish Metso Paper has been awarded a contract for the rebuild of Sabah Forest Industries ' ( SFI ) pulp mill in Sabah , Malaysia .\", 'Finnish Outokumpu Technology has been awarded several new grinding technology contracts .']\n"
     ]
    }
   ],
   "source": [
    "# Strip the new line character from all headlines.\n",
    "headlines =[x.strip(\"\\n\") for x in headlines]\n",
    "\n",
    "# Print all headlines.\n",
    "print(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Setting up Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this code-along we are using the OpenAI API to programmatically make requests to a GPT-model. This allows us to automate calls to the model, as would be the case when implementing generative AI functionalities in an application or data transformation process.\n",
    "\n",
    "In general, when developing an application, we want our code to be modular, scalable and reusable. How do we do this with LLM prompts?\n",
    "\n",
    "This is where Prompt Templates come into play! It allows for dynamic prompts, with built-in verification tools on whether all inputs are given (this will ease the load on testing). They can easily be saved, versioned and integrated into the code base of an application.\n",
    "\n",
    "We will set up Prompt Templates (from the `langchain` package) to automatically determine financial sentiment from the headlines and extract relevant company names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of Prompt Templates\n",
    "\n",
    "A prompt template can have dynamic input, which can be added using `{ }`.\n",
    "\n",
    "Example: `\"Can you give me some suggestions for my trip to {city}?\"`\n",
    "\n",
    "We can then format the prompt template by filling in the `city` variable.\n",
    "\n",
    "Certain prompts that are often reused programmatically in application processes might be very lengthy and can be carefully designed to meet a specific need. For example, if we want the output of a sentiment analysis by the GPT-model to be limited to either positive, negative or neutral (without anything else in the answer), we need to explicitly tell the model within our prompt. In order to not accidentally forget this in any of the future prompts, it is best practice to design and save a prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Prompt Templates\n",
    "\n",
    "Prompt Templates in Langchain come in two formats:\n",
    "- **PromptTemplate**: this is used for completion models.\n",
    "- **ChatPromptTemplate**: this is used for chat completion models. On top of the normal input prompt, these can hold a system message (meta prompt) and a conversation history.\n",
    "\n",
    "Let's start by creating a PromptTemplate and ChatPromptTemplate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "Create a Prompt Template to analyze financial sentiment.\n",
    "- import `PromptTemplate` from `langchain.prompts`.\n",
    "- Create a `PromptTemplate` object by using its `.from_template()` method. Assign to `prompt_template`.\n",
    "- For the template argument, use:\n",
    "\n",
    "``\"Analyze the following financial headline for sentiment: {headline}\"``\n",
    "\n",
    "- Format the prompt using its `.format()` method. Let's use our first headline as input. Assign to `formatted_prompt`.\n",
    "- Print the formatted prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Code hints ></b></summary>\n",
    "<p>\n",
    "\n",
    "`prompt_template.format()` will have one argument called `headline` (as defined in our `PromptTemplate`), where we pass along the first element of our `headlines` list.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze the following financial headline for sentiment: Finnish Aktia Group 's operating profit rose to EUR 17.5 mn in the first quarter of 2010 from EUR 8.2 mn in the first quarter of 2009 .\n"
     ]
    }
   ],
   "source": [
    "# Import the PromptTemplate class.\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a dynamic template to analyze a single headline.\n",
    "\n",
    "prompt = \"Analyze the following {domain} headline for sentiment: {headline}\"\n",
    "prompt_template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "# Format the prompt template on the first headline of the dataset.\n",
    "# format() takes the same parameter as what is in between square brackets in the prompt template\n",
    "formatted_prompt = prompt_template.format(domain = \"financial\", headline = headlines[0])\n",
    "\n",
    "# Print the formatted template.\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a `ChatPromptTemplate`, which are compatible with conversational models like GPT-4 and GPT-3.5-Turbo. When using the ChatPromptTemplate, we have the ability to assign a **system message**, so let's make use of this.\n",
    "\n",
    "In terms of prompt engineering, what we write in the system can heavily influence the quality of the output. Some things we can do using the system message is:\n",
    "- Define a role: *\"You are a X\", \"Your role is to do X\", ...*\n",
    "- Define a tone of voice: *\"Respond in a formal manner\", \"Use customer-oriented language\", ...*\n",
    "- Define restrictions on output format: *\"The format of the output is X\", \"The output is strictly limited to X, Y, Z\", ...*\n",
    "- Define a scope: *\"Only answer questions on topic X\", \"If the user questions is not about X, answer with Y\", ...*\n",
    "\n",
    "You will notice some of these tricks applied to the following system message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "- Import `ChatPromptTemplate` from `langchain.prompts`.\n",
    "- Define a system message as follows and assign to `system_message`.\n",
    "\n",
    "```\n",
    "\"\"\"You are performing sentiment analysis on news headlines regarding financial analysis. \n",
    "This sentiment is to be used to advice financial analysts. \n",
    "The format of the output has to be consistent. \n",
    "The output is strictly limited to any of the following options: [positive, negative, neutral].\"\"\"\n",
    "```\n",
    "\n",
    "- Instantiate a new `ChatPromptTemplate` using its `.from_messages()` method. Assign to `chat_template`.\n",
    "    - This method will take a list of tuples as input. We need two tuples, one for the system message and one for the human message. To distinguish the two, the first element of the tuple is either `\"system\"` or `\"human\"`.\n",
    "    - The second element of the tuple is the actual message, as string. For the system message, you can use the `system_message`variable. For the human message, we can reuse the same message as before (including the input variable `{headlines}`).\n",
    "    \n",
    "- Format the template using its `.format_messages()` method. Let's use our first headline again. Assign to `formatted_chat_template`.\n",
    "- Print the formatted template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "\n",
    "The input for `ChatPromptTemplate.from_messages()` follows this structure:\n",
    "`[(\"system\", system_message), (\"human\", input_prompt)]`\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are performing sentiment analysis on news headlines regarding financial analysis. \n",
      "This sentiment is to be used to advice financial analysts. \n",
      "The format of the output has to be consistent. \n",
      "The output is strictly limited to any of the following options: positive, negative, neutral.\n",
      "Human: Analyze the following financial headline for sentiment: Finnish Aktia Group 's operating profit rose to EUR 17.5 mn in the first quarter of 2010 from EUR 8.2 mn in the first quarter of 2009 .\n"
     ]
    }
   ],
   "source": [
    "# Import the ChatPromptTemplate class.\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the system message.\n",
    "system_message = \"\"\"You are performing sentiment analysis on news headlines regarding financial analysis. \n",
    "This sentiment is to be used to advice financial analysts. \n",
    "The format of the output has to be consistent. \n",
    "The output is strictly limited to any of the following options: {options}.\n",
    "Avoid newlines (\\n) and other non-printable characters.\"\"\"\n",
    "\n",
    "options = \"positive, negative, neutral\"\n",
    "domain = \"financial\"\n",
    "\n",
    "# Initialize a new ChatPromptTemplate with a system message and human message.\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"human\", prompt)\n",
    "])\n",
    "\n",
    "# Format the ChatPromptTemplate.\n",
    "formatted_chat_template = chat_template.format(options=options,\n",
    "                                               domain=domain,\n",
    "                                               headline=headlines[0])\n",
    "\n",
    "# Print the formatted template.\n",
    "print(formatted_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Setting up LLM Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will briefly cover the concept of chains in langchain. LLM Chains are an easy way to combine a model with a prompt template. These chains can be created for both *completion models* and *chat completion models*.\n",
    "\n",
    "LLM Chains can be used to \"chain\" prompt flows, by using the output of a previous chain as input for the next.\n",
    "\n",
    "Let's set up a chain for a completion model first, using the templates that we've just built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "Create an LLM Chain for a completion model.\n",
    "- Import `LLMChain` from `langchain.chains`.\n",
    "- Instantiate a new `LLMChain`. Assign to `completion_chain`. We need to pass along two parameters for its init function:\n",
    "    - `llm`: Here we can easily pass along a completion model by creating one using `OpenAI()`.\n",
    "    - `prompt`: Here can we use the `prompt_template` we have created before.\n",
    "- Run the chain using its `.run()` method.\n",
    "    - Our prompt template has an input variable called `headline`. This becomes an input parameter for the `.run()` method. Let's pass along the first headline of our dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LLMChain class.\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create the LLMChain by combining a completion model and a prompt.\n",
    "completion_chain = LLMChain(llm = OpenAI(temperature=0, #default davinci model\n",
    "                                         top_p=0.2,\n",
    "                                         max_tokens=5),\n",
    "                            prompt = prompt_template)\n",
    "\n",
    "# Run the LLMChain.\n",
    "completion_model_output = completion_chain.run(domain=domain, headline=headlines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(completion_model_output)\n",
    "print(type(completion_model_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same, using a chat completion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Instantiate a new `LLMChain`. Assign to `chat_chain`. We need to pass along two parameters for its init function:\n",
    "    - `llm`: Here we can easily pass along a completion model by creating one using `ChatOpenAI()`.\n",
    "    - `prompt`: Here can we use the `chat_template` we have created before.\n",
    "- Run the chain using the `.run()` method of the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "\n",
    "`chat_chain.run()` has two input variables, since `chat_template` has two input variables. Make sure to pass along a value for both `headline` and `system_message`.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLMChain by combining a chat completion model and a prompt.\n",
    "chat_chain = LLMChain(llm=ChatOpenAI(temperature=0, #default 3.5-turbo\n",
    "                                     max_tokens=5),\n",
    "                     prompt=chat_template)\n",
    "\n",
    "# Run the LLMChain.\n",
    "chat_model_output = chat_chain.run(options=options,\n",
    "                                   domain=domain,\n",
    "                                   headline=headlines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(chat_model_output)\n",
    "print(type(chat_model_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Extracting Company Names with the Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parsing is a very useful feature in Langchain when integrating LLM outputs into your application. The output parser can automatically transform the output of the GPT-model to numerous data types, such as lists, datetimes, JSONs and so on.\n",
    "\n",
    "In this example, we will ask the GPT-model to extract the company name from every headline and instantly assign them to a Python list.\n",
    "\n",
    "As we want to combine sentiment with the company name later, we will limit the output to one name per headline.\n",
    "\n",
    "In order to format the output as a Python list, we can make use of the `CommaSeparatedListOutputParser` class in Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Create an output parser and a formatted prompt template to extract company names from multiple headlines.\n",
    "- Import `CommaSeparatedListOutputParser` from `langchain.output_parsers`.\n",
    "- Instantiate a new `CommaSeparatedListOutputParser` and assign to `output_parser`.\n",
    "- To retrieve the parsing instructions from the output parser, we can use its `.get_format_instructions()` method. Assign this to `format_instructions`.\n",
    "- Let's instantiate a new `PromptTemplate`. This time we won't use its `.from_template()` method. When calling `PromptTemplate()` with the output parser, we need to pass three arguments:\n",
    "    - `template`: here we can use the following string; \n",
    "```\n",
    "\"List all the company names from the following headlines, limited to one name per headline: {headlines}.\\n{format_instructions}\"\n",
    "```\n",
    "\n",
    "- `input_variables`: This is a list of strings containing the input variables that are required. In our case, this is only `\"headlines\"`.\n",
    "- `partial_variables`: Here we pass along a dictionary with the key being `\"format_instructions\"` and the value being the `format_instructions` variable we created earlier.\n",
    "- Format the prompt template using the entire `headlines` list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "\n",
    "We can create a new prompt template using `PromptTemplate(template= , input_variables= , partial_variables= )`\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CommaSeparatedListOutputParser class.\n",
    "\n",
    "\n",
    "# Instantiate the output parser.\n",
    "\n",
    "\n",
    "# Get the format instructions from the output parser.\n",
    "\n",
    "\n",
    "# Instantiate a new prompt template with the format instructions.\n",
    "\n",
    "\n",
    "# Format the prompt using all headlines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a template with format instructions ready, let's send it to a GPT-model and look at the output. We want to run these kinds of tasks with the temperature parameter of the large language model set to zero, as this maximizes precision. \n",
    "\n",
    "We tend to distinguish tasks that either require precision or creativity. When we are looking for correctness in the answer (e.g. when generating code) we aim for high precision (by lowering temperature) whereas when generating ideas or content, we might prefer more creativity (by increasing temperature). A simplified explanation of the *temperature* of a large language model is its randomness. When temperature is set to 0, we will get the exact same output, given the same inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Create a new Langchain model, send over the template and inspect the parsed output.\n",
    "- Instantiate a new `OpenAI()` model. Set the temperature to 0. Assign to `model`.\n",
    "- Run `model()` on the formatted template. Assign to `_output`. The underscore preceding our variable name indicates that this is just a temporary variable, that will likely be overwritten many times.\n",
    "- Use the `.parse()` method of the output parser on the output of the model. Assign to `company_names`.\n",
    "- Print the data type of `company_names`.\n",
    "- Print the company names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "\n",
    "- The temperature of the model can be set to 0 by using `OpenAI(temperature= )`.\n",
    "- We can get the data type of a variable by using `type(variable)`.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Langchain OpenAI Model object.\n",
    "\n",
    "\n",
    "# Run the model on the input.\n",
    "\n",
    "\n",
    "# Parse the output.\n",
    "\n",
    "\n",
    "# Print the data type the parsed output.\n",
    "\n",
    "\n",
    "# Print the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Working with Agents and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leveraging the agents and tools in LangChain is where the framework's value really starts to shine! But before we dive deeper into this concept, we need to understand MRKL prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are MRKL Prompts?\n",
    "\n",
    "MRKL stands for Modular Reasoning, Knowledge and Language prompts. It is a system composed of a set of modules, often accompanied by an agent that decides how to route prompts to the appropriate module (or tools).\n",
    "\n",
    "These kinds of prompts follow a specific format, which we can force the GPT-model to adhere to by using the system message. It will loop through this format (steps 1-5 below) using recursive requests to the GPT-model until we get our final answer. A commonly used format is the following:\n",
    "1. Question: the user question (in the first iteration) or follow-up question composed by the GPT-models (in later iterations)\n",
    "2. Thought: think about what to do as a next step\n",
    "3. Action: pick a tool from the list of tool names we have provided\n",
    "4. Action Input: the input for the chosen tool\n",
    "5. Observation: the output of the tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Tools and Agents?\n",
    "\n",
    "We can access (external) tools using the output of the GPT-model. Large language models output only text. In order to call a function (to access a tool) based on the text output of a large language model, we can use agents. They can parse the text output, pick the correct tool and define its input.\n",
    "\n",
    "The langchain framework has a wide variety of built-in tools, along with the ability to define additional custom tools. A very common use case for tools is accessing document stores or vector databases to ingest information from our own documents. This will be explored more in-depth in future modules.\n",
    "\n",
    "For now, to have a gentle introduction to tools, we have decided on one that does not require external set up (no API token that needs to be created or external database that needs to be set up). \n",
    "\n",
    "In this example, we will make use of the `PythonREPLTool`. This allows the GPT-model to run the Python code that it generates, and can be useful for carrying out an abundance of tasks.\n",
    "\n",
    "Note: as we introduce recursive prompts using agents, it is best practice to always define a maximum number of output tokens. This ensures our costs will not skyrocket if a prompt loop takes too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Before we continue with our financial analysis, let's create a quick example of how code can be ran using a Python agent. In this case, we will ask it to make a calculation (something that most large language models are not trained to do out-of-the-box).\n",
    "- Import the `create_python_agent` function from `langchain.agents.agent_toolkits`.\n",
    "- Import the `PythonREPLTool` class from `langchain.tools.python.tool`.\n",
    "- Create a Python agent by calling the `create_python_agent()` function. Assign to `agent_executor`. This function takes three arguments:\n",
    "    - `llm`: here we can create a new `OpenAI()` model. Let's set the `temperature` to 0 and `max_tokens` to 1000.\n",
    "    - `tool`: here we instantiate a new `PythonREPLTool()`.\n",
    "    - `verbose`: set this to True so that can we see the prompt loop.\n",
    "- Run the agent using its `.run()` method. As an example, you can ask it: `\"What is the square root of 250? Round the answer down to 4 decimals.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary classes from langchain.\n",
    "\n",
    "\n",
    "# Instantiate a Python agent, with the PythonREPLTool.\n",
    "\n",
    "\n",
    "# Ask the agent for the solution of a mathematical equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the output above. As we haven't assigned any other tool, the choice of tools for the model was quite limited. Hence under Action, it should list the `Python_REPL` tool.\n",
    "The Action Input will show the actual code that was generated by the GPT-model and executed by the Agent.\n",
    "\n",
    "Now let's try to use the same agent to help us in our financial news analysis.\n",
    "\n",
    "We want to structure our prompt in a clear way, explaining a step by step process. For example:\n",
    "- First, *Analyze the sentiment...*\n",
    "- Second, *Load this data into a pandas dataframe.*\n",
    "- Third, *Save this dataframe to a CSV under the name financial_analysis.csv*\n",
    "- ...\n",
    "\n",
    "Lastly, we pass along the headlines (input data) itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Ask the agent to extract the company name and sentiment from the headlines and save its output in a `.csv` file called `financial_analysis.csv`.\n",
    "- Run the agent on the following prompt:\n",
    "    \n",
    "    ``` \n",
    "    f\"\"\"For every of the following headlines, extract the company name and whether the financial sentiment is positive, neutral or negative. \n",
    "    Load this data into a pandas dataframe. \n",
    "    The dataframe will have three columns: the name of the company, whether the financial sentiment is positive or negative and the headline itself. \n",
    "    The dataframe can then be saved in the current working directory under the name financial_analysis.csv.\n",
    "    If a csv file already exists with the same name, it should be overwritten.\n",
    "\n",
    "    The headlines are the following:\n",
    "    {headlines}\"\"\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the output above. Do you see anything that could be improved? We will come back to this later in this notebook.\n",
    "\n",
    "For now, let's quickly load our `.csv` file in a dataframe to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Load the data in a dataframe for evaluation.\n",
    "- Import `pandas` under its usual alias: `pd`.\n",
    "- Load the `financial_analysis.csv` file into a dataframe. Assign to `df`.\n",
    "- Print the dataframe. As our dataframe only contains six rows, we can just print the entire dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "\n",
    "Use the `pd.read_csv(filename)` function to load the `.csv` file to dataframe.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the necessary import.\n",
    "\n",
    "\n",
    "# Load the CSV file into a dataframe.\n",
    "\n",
    "\n",
    "# Print the dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing the output above (looking at the company names and sentiment), you will probably notice some room for improvement. \n",
    "\n",
    "Company names and sentiment may not be extracted in a very powerful way. The reason for this is that without further instructions, the GPT-model will use the PythonREPLTool (Python code) to complete its task. Looking back at the output from our last call to the Python agent, we may find that it created rule sets on how to extract the company name or determine the sentiment. These hard-coded rules negate the power of large language models! We will improve on this in Task 7.\n",
    "\n",
    "Another problem that might arise is that the *sentiment* of a sentence can differ from *financial sentiment*. For example, an aggressive headline complaining about a large corporation making too much profit might result in negative sentiment, while from a financial analysis point of view the sentiment is positive. To steer the GPT-model to our desired outcome, we will now introduce few shot learning.\n",
    "\n",
    "For example, *Company X was awarded a new contract* might be categorized as a neutral sentence. The sentence itself is simply an objective statement or observation. Nothing is mentioned about whether we like or dislike that particular company because of this. From a financial perspective however, this is considered as something positive. To steer the GPT-model to our desired outcome, we will now introduce few shot learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Adding Few Shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few shot learning basically comes down to adding some examples into our prompt, in this case, what we consider to be positive or negative headlines. A shot refers to an example given to the model in the input prompt (or sometimes the system message).\n",
    "\n",
    "We distinguish three categories of contextual learning:\n",
    "- Few shot leaning (multiple examples)\n",
    "- Single shot learning (one example)\n",
    "- Zero shot leaning (no examples)\n",
    "\n",
    "Few shot learning might take more effort in terms of prompt building, but it will generally yield better results, as the model has a better understanding of our desired outcome.\n",
    "\n",
    "Let's look at an example of financial sentiment analysis without few shot learning first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Create a prompt template with output parsing to determine the financial sentiment of all headlines.\n",
    "- Create a new `PromptTemplate` called `sentiment_template`. Remember the three arguments `template`, `input_variables` and `partial_variables`. Assign to `sentiment_template`.\n",
    "    - We can reuse the `format_instructions` variable that we have loaded into memory before.\n",
    "    - As a template, use: \n",
    "```\n",
    "\"Get the financial sentiment of each of the following headlines. The output is strictly limited to any of the following options: ['Positive', 'Negative', 'Neutral']: {headlines}.\\n{format_instructions}\"\n",
    "```\n",
    "\n",
    "\n",
    "- Format the template on all headlines. Assign to `formatted_sentiment_template`.\n",
    "- Run the formatted template through our `model` and assign the result to our temporary variable `_output`.\n",
    "- Parse the output using the output parser. Assign the result to `sentiments`.\n",
    "- Print the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new prompt template with output parsing.\n",
    "\n",
    "\n",
    "# Format the prompt template.\n",
    "\n",
    "\n",
    "# Run the model on the formatted prompt template.\n",
    "\n",
    "\n",
    "# Parse the output.\n",
    "\n",
    "\n",
    "# Print the list of sentiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to evaluate the sentiments without seeing the associated headline. To make our lives easier, let's write a quick function to easily visualize and interpret the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Visualize and interpret the results of the sentiment analysis.\n",
    "- Write a function called `visualize_sentiments` to visualize both the sentiment and associated headline, for all headlines. \n",
    "    - The input for this function should be two lists: one containing all headlines and one containing all sentiments.\n",
    "    - As a best practice, start with using an `assert` that ensures that both lists are of equal length.\n",
    "    - There are many ways to create this: simply printing with f-strings, making a dictionary or Dataframe, get creative!\n",
    "- Call the `visualize_sentiments` function using `headlines` and `sentiments` as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "\n",
    "- We can assert that both input lists are of equal length by using `assert len(list1) == len(list2)`.\n",
    "- A very simplistic way of visualizing the sentiments per headline is using f-strings, such as `f\"{sentiments[i]}: {headlines[i]}\"` in a loop. \n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new function with two inputs\n",
    "\n",
    "    # Assert that both inputs are of equal length\n",
    "    \n",
    "\n",
    "    # Visualize the sentiments and their respective headlines\n",
    "    \n",
    "\n",
    "# Call the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we might see that the financial sentiment is not always correctly assigned, such as a contract being awarded not being recognized as a financially positive headline.\n",
    "To improve the performance, we will add some examples. Few shot learning can be done by either giving some observations (headlines in this case) accompanied by their ground truth (label) *or* by giving an abstract description of what is seen as positive, negative or neutral.\n",
    "\n",
    "In this case, we will opt for the later. Here is a prompt you can use for few shot learning:\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "If a company is doing financially better than before, the sentiment is positive. For example, when profits or revenue have increased since the last quarter or year, exceeding expectations, a contract is awarded or an acquisition is announced.\n",
    "If the company's profits are decreasing, losses are mounting up or overall performance is not meeting expectations, the sentiment is negative.\n",
    "If nothing positive or negative is mentioned from a financial perspective, the sentiment is neutral.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Create and run a prompt template using few shot learning.\n",
    "- Store the prompt above in a variable called `sentiment_examples`.\n",
    "- Create a `PromptTemplate` called `sentiment_template` like we did two cells above.\n",
    "    - In our template, we will add a new input variable called `few_shot_examples`. This can be placed in between the two sentences.\n",
    "    - Don't forget to add our new input variables to the list of `input_variables`.\n",
    "    - Reuse the same `format_instructions` as before.\n",
    "- Format the `sentiment_template`. Remember that you will need to pass both `headlines` and `sentiment_examples`.\n",
    "- Run the formatted template through our `model` and assign the result to our temporary variable `_output`.\n",
    "- Parse the output using the output parser. Assign the result to `sentiments`.\n",
    "- Visualize and interpret the results using your newly created `visualize_sentiments` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "\n",
    "The `template` we should use could look like this: \n",
    "\n",
    "```\n",
    "\"Get the financial sentiment of each of the following headlines. {few_shot_examples} The output is strictly limited to [`Positive`, `Negative`, `Neutral`]: {headlines}.\\n{format_instructions}\"\n",
    "```\n",
    "\n",
    "When formatting the template, we can pass along `sentiment_examples` to the `few_shot_examples` input variable.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the few shot examples in a variable.\n",
    "\n",
    "\n",
    "# Instantiate a new prompt template with the format instructions.\n",
    "\n",
    "\n",
    "# Format the template.\n",
    "\n",
    "\n",
    "# Run the model on the formatted template.\n",
    "\n",
    "\n",
    "# Parse the model output.\n",
    "\n",
    "\n",
    "# Visualize the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Combining Tools and Output Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed in Task 5, using tools is not a guaranteed success. We can improve the performance by clearly determining which tasks can be completed by the Python tool and which we use the GPT-model itself for.\n",
    "To maximize the powerful capabilities of the GPT-model, we prefer its use over hard-coded rule sets when it comes to company name extraction or financial sentiment analysis.\n",
    "However, other (cumbersome) tasks that do not require the ability to handle ambiguity, are often best left to the Python tool.\n",
    "\n",
    "Let's ask the model to use the existing lists that we got from our templates (`company_names` and `sentiments`), but use the Python tool to neatly place them in a Pandas dataframe and write them locally to a `.csv` file.\n",
    "\n",
    "Use the following prompt:\n",
    "\n",
    "```\n",
    "f\"\"\"Create a dataframe with two columns: company_name, sentiment and headline.\n",
    "                   To fill the dataframe, use the following lists respectively: {str(company_names)}, {str(sentiments)} and {str(headlines)}. \n",
    "                   The dataframe can then be saved in the current working directory under the name financial_analysis_with_parsing.csv.\n",
    "                   If a csv file already exists with the same name, it should be overwritten.\n",
    "                   \"\"\"\n",
    "```\n",
    "\n",
    "In the prompt above, we pass along lists that were generated by the GPT-model before (when it did not have access to the Python tool). Now we only want to give instructions on tasks that should be carried out using Python code, such as the creation of the dataframe, saving (and overwriting) it, ...\n",
    "\n",
    "Keep in mind that we can use this same way of working for much more complex tasks, that might encompass extensive coding requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Run the `agent_executor` on the prompt above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent to create a file with the headlines, company names and sentiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at our working directory, we will see a new file pop up, called `financial_analysis_with_parsing.csv`.\n",
    "\n",
    "Let's analyze it and compare against the output from Task 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Load and display the new file.\n",
    "- Load `financial_analysis_with_parsing.csv` into a dataframe called `df`.\n",
    "- Print the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a dataframe.\n",
    "\n",
    "\n",
    "# Print the dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Using the OpenAI Moderation API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI API platform also sports a Moderation API, in addition to their model and embeddings APIs. The Moderation API can check whether the prompt contains explicit content and can flag various categories like hate, violence, sexually explicit content and so on. When we are building an application targeting large user bases, it becomes crucial to leverage the Moderation API and filter our input prompts to avoid the complications associated with unethical LLM usage.\n",
    "\n",
    "To test the Moderation API, we have a small sample of five comments picked from the `r/WallStreetBets` subreddit, stored in the `reddit_comments.txt` file.\n",
    "\n",
    "Let's start by reading the text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content warning\n",
    "\n",
    "In order to trigger the moderation API, the comments were specifically chosen to be offensive. If you are sensitive to awful content, you may wish to avoid printing and reading the text.\n",
    "\n",
    "Naturally, neither the project instructor nor DataCamp agrees with the ideas expressed within this text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Read the text file and store its lines in a variable called `comments`.\n",
    "- Open `reddit_comments.txt` as read.\n",
    "- Use the `.readlines()` method to store its contents in a list called `comments`.\n",
    "- Optionally print the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "\n",
    "Here we can use the same `with open(filename, \"r\") as file:` structure as in Task 1.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 175,
    "lastExecutedAt": 1696556993785,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load the lines of the text file.\n\n\n# Optionally print the comments.\n# comments"
   },
   "outputs": [],
   "source": [
    "# Load the lines of the text file.\n",
    "\n",
    "\n",
    "# Optionally print the comments.\n",
    "# comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Analyze a comment using the Moderation API.\n",
    "- Pick a comment from the dataset (using and index between 0 - 4) and store this in a variable called `comment`.\n",
    "- Use the API by calling `openai.Moderation.create()`. For the `input` argument, we will pass along the `comment`. Assign to `moderation_output`.\n",
    "- Print the comment and moderation output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a comment.\n",
    "\n",
    "\n",
    "# Send the comment to the Moderation API.\n",
    "\n",
    "\n",
    "# Optionally print the comment.\n",
    "# print(comment)\n",
    "\n",
    "# Print the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyze the output above to determine whether the comment has been deemed explicit or not. The `\"flagged\"` boolean will show us if any (at least one) category has been flagged, and underneath we can see which categories have been flagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this module! You should be able to get started with basic LangChain projects yourself now. \n",
    "\n",
    "You've learned:\n",
    "- Important prompt engineering tricks and optimizations\n",
    "- Setting up prompt templates\n",
    "- Using LLMChains\n",
    "- Using LangChain output parsing to generate Python objects to be used downstream\n",
    "- Using LangChain Agents and Tools to add additional functionalities to generative AI projects\n",
    "- Leveraging the Moderation API to act as a filter of user input\n",
    "\n",
    "We wish you the best of luck in the following modules!"
   ]
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
